<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Want595</title>
  
  <subtitle>温特兔</subtitle>
  <link href="https://want595.github.io/atom.xml" rel="self"/>
  
  <link href="https://want595.github.io/"/>
  <updated>2024-07-25T02:35:45.348Z</updated>
  <id>https://want595.github.io/</id>
  
  <author>
    <name>junwei</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据行业应用-情感分析</title>
    <link href="https://want595.github.io/2024/07/25/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    <id>https://want595.github.io/2024/07/25/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/</id>
    <published>2024-07-25T02:28:47.000Z</published>
    <updated>2024-07-25T02:35:45.348Z</updated>
    
    <content type="html"><![CDATA[<h2 id="大数据行业应用-情感分析"><a href="#大数据行业应用-情感分析" class="headerlink" title="大数据行业应用-情感分析"></a>大数据行业应用-情感分析</h2><h2 id="1、数据准备"><a href="#1、数据准备" class="headerlink" title="1、数据准备"></a>1、数据准备</h2><p>1.1在master节点打开一个终端，进入虚拟环境zkbc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master ~]$ source activate zkbc</span><br></pre></td></tr></table></figure><p>1.2在zkpk家目录下创建实验文件夹sparkmllib，并进入实验文件夹sparkmllib</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(zkbc)[zkpk@master ~]$ mkdir sparkmllib</span><br><span class="line">(zkbc)[zkpk@master ~]$ cd sparkmllib</span><br></pre></td></tr></table></figure><p>1.3拷贝训练集和测试集数据到实验文件夹中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(zkbc)[zkpk@master sparkmllib]$ cp /home/zkpk/experiment/tweetstest.json ./</span><br><span class="line">(zkbc)[zkpk@master sparkmllib]$ cp /home/zkpk/experiment/hillarytest.json ./</span><br><span class="line">(zkbc)[zkpk@master sparkmllib]$ cp /home/zkpk/experiment/donaldtest.json ./</span><br></pre></td></tr></table></figure><p>1.4拷贝文件向量化模型文件夹word2vecModel到实验文件夹中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(zkbc)[zkpk@master sparkmllib]$ cp -r /home/zkpk/experiment/word2vecModel ./</span><br></pre></td></tr></table></figure><p>1.5拷贝basemap绘制美国地图所需的shapefile到实验文件夹中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(zkbc)[zkpk@master sparkmllib]$ cp  /home/zkpk/experiment/st99_d00.shp ./</span><br><span class="line">(zkbc)[zkpk@master sparkmllib]$ cp /home/zkpk/experiment/st99_d00.dbf ./</span><br><span class="line">(zkbc)[zkpk@master sparkmllib]$ cp /home/zkpk/experiment/st99_d00.shx ./</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725101351601.png" alt="image-20240725101351601"></p><h2 id="2、推特文本转换成向量"><a href="#2、推特文本转换成向量" class="headerlink" title="2、推特文本转换成向量"></a>2、推特文本转换成向量</h2><p>2.1利用vim编辑器创建python文件sparktest.py</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(zkbc)[zkpk@master sparkmllib]$ vim sparktest.py</span><br></pre></td></tr></table></figure><p>2.2在sparktest.py中键入代码</p><p>2.2.1定义编码格式utf8</p><p>2.2.2导入python基本库、Spark依赖库、basemap可视化库等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SQLContext</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.tree <span class="keyword">import</span> RandomForest</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.basemap <span class="keyword">import</span> Basemap</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.basemap <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> rgb2hex</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Polygon</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102008284.png" alt="image-20240725102008284"></p><p>2.2.3Spark MLlib中提供的机器学习模型处理的是向量形式的数据，因此我们需将文本转换为向量形式，为了节省时间，这里我们利用Spark提供的Word2Vec功能结合其提供的text8文件中的一部分单词进行了word2vec模型的预训练，并将模型保存至word2vecModel/data文件夹中，因此本次实验中将推特数据转换为向量时直接调用此模型即可</p><p>2.2.4定义分词文本转换为向量的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">doc2vec</span>(<span class="params">document</span>):</span><br><span class="line">    doc_vec = np.zeros(<span class="number">100</span>)</span><br><span class="line">    tot_words = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> document:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            vec = np.array(lookup_bd.value.get(word)) + <span class="number">1</span></span><br><span class="line">            <span class="comment"># print(vec)</span></span><br><span class="line">            <span class="keyword">if</span> vec <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                doc_vec += vec</span><br><span class="line">                tot_words += <span class="number">1</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    vec = doc_vec / <span class="built_in">float</span>(tot_words)</span><br><span class="line">    <span class="keyword">return</span> vec</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102024014.png" alt="image-20240725102024014"></p><p>2.2.4.1本函数接受一个分词后的文本参数</p><p>2.2.4.2先定义一个100维的向量doc_vec</p><p>2.2.4.3遍历查找文本中的词在预训练模型word2vec中是否存在</p><p>2.2.4.4如果该特征词在预先训练好的模型中，则添加到向量中，否则继续下一循环</p><p>2.2.4.5最后获得词向量/匹配词总数</p><p>2.2.5定义推特文本中的停用词、标点符号、url等</p><p>2.2.5.1定义Python正则re模块去除特殊字符，如匹配特殊字符%S</p><p>2.2.5.2定义使用re模块的 escape函数去除字符串中的标点符号</p><p>2.2.5.3定义停用词组stopwords</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">remove_spl_char_regex = re.<span class="built_in">compile</span>(<span class="string">'[%s]'</span> % re.escape(string.punctuation))  <span class="comment"># 正则去除特殊字符</span></span><br><span class="line">stopwords = [<span class="string">u'rt'</span>, <span class="string">u're'</span>, <span class="string">u'i'</span>, <span class="string">u'me'</span>, <span class="string">u'my'</span>, <span class="string">u'myself'</span>, <span class="string">u'we'</span>, <span class="string">u'our'</span>, <span class="string">u'ours'</span>, <span class="string">u'ourselves'</span>, <span class="string">u'you'</span>, <span class="string">u'your'</span>,</span><br><span class="line">             <span class="string">u'yours'</span>, <span class="string">u'yourself'</span>, <span class="string">u'yourselves'</span>, <span class="string">u'he'</span>, <span class="string">u'him'</span>, <span class="string">u'his'</span>, <span class="string">u'himself'</span>, <span class="string">u'she'</span>, <span class="string">u'her'</span>, <span class="string">u'hers'</span>,</span><br><span class="line">             <span class="string">u'herself'</span>, <span class="string">u'it'</span>, <span class="string">u'its'</span>, <span class="string">u'itself'</span>, <span class="string">u'they'</span>, <span class="string">u'them'</span>, <span class="string">u'their'</span>, <span class="string">u'theirs'</span>, <span class="string">u'themselves'</span>, <span class="string">u'what'</span>,</span><br><span class="line">             <span class="string">u'which'</span>, <span class="string">u'who'</span>, <span class="string">u'whom'</span>, <span class="string">u'this'</span>, <span class="string">u'that'</span>, <span class="string">u'these'</span>, <span class="string">u'those'</span>, <span class="string">u'am'</span>, <span class="string">u'is'</span>, <span class="string">u'are'</span>, <span class="string">u'was'</span>, <span class="string">u'were'</span>,</span><br><span class="line">             <span class="string">u'be'</span>, <span class="string">u'been'</span>, <span class="string">u'being'</span>, <span class="string">u'have'</span>, <span class="string">u'has'</span>, <span class="string">u'had'</span>, <span class="string">u'having'</span>, <span class="string">u'do'</span>, <span class="string">u'does'</span>, <span class="string">u'did'</span>, <span class="string">u'doing'</span>, <span class="string">u'a'</span>,</span><br><span class="line">             <span class="string">u'an'</span>, <span class="string">u'the'</span>, <span class="string">u'and'</span>, <span class="string">u'but'</span>, <span class="string">u'if'</span>, <span class="string">u'or'</span>, <span class="string">u'because'</span>, <span class="string">u'as'</span>, <span class="string">u'until'</span>, <span class="string">u'while'</span>, <span class="string">u'of'</span>, <span class="string">u'at'</span>, <span class="string">u'by'</span>,</span><br><span class="line">             <span class="string">u'for'</span>, <span class="string">u'with'</span>, <span class="string">u'about'</span>, <span class="string">u'against'</span>, <span class="string">u'between'</span>, <span class="string">u'into'</span>, <span class="string">u'through'</span>, <span class="string">u'during'</span>, <span class="string">u'before'</span>, <span class="string">u'after'</span>,</span><br><span class="line">             <span class="string">u'above'</span>, <span class="string">u'below'</span>, <span class="string">u'to'</span>, <span class="string">u'from'</span>, <span class="string">u'up'</span>, <span class="string">u'down'</span>, <span class="string">u'in'</span>, <span class="string">u'out'</span>, <span class="string">u'on'</span>, <span class="string">u'off'</span>, <span class="string">u'over'</span>, <span class="string">u'under'</span>,</span><br><span class="line">             <span class="string">u'again'</span>, <span class="string">u'further'</span>, <span class="string">u'then'</span>, <span class="string">u'once'</span>, <span class="string">u'here'</span>, <span class="string">u'there'</span>, <span class="string">u'when'</span>, <span class="string">u'where'</span>, <span class="string">u'why'</span>, <span class="string">u'how'</span>, <span class="string">u'all'</span>,</span><br><span class="line">             <span class="string">u'any'</span>, <span class="string">u'both'</span>, <span class="string">u'each'</span>, <span class="string">u'few'</span>, <span class="string">u'more'</span>, <span class="string">u'most'</span>, <span class="string">u'other'</span>, <span class="string">u'some'</span>, <span class="string">u'such'</span>, <span class="string">u'no'</span>, <span class="string">u'nor'</span>, <span class="string">u'not'</span>,</span><br><span class="line">             <span class="string">u'only'</span>, <span class="string">u'own'</span>, <span class="string">u'same'</span>, <span class="string">u'so'</span>, <span class="string">u'than'</span>, <span class="string">u'too'</span>, <span class="string">u'very'</span>, <span class="string">u's'</span>, <span class="string">u't'</span>, <span class="string">u'can'</span>, <span class="string">u'will'</span>, <span class="string">u'just'</span>, <span class="string">u'don'</span>,</span><br><span class="line">             <span class="string">u'should'</span>, <span class="string">u'now'</span>]</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102100915.png" alt="image-20240725102100915"></p><h2 id="3、数据分词"><a href="#3、数据分词" class="headerlink" title="3、数据分词"></a>3、数据分词</h2><p>3.1定义Segmentation函数对推特文本进行分词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Segmentation</span>(<span class="params">text</span>):</span><br><span class="line">    seg_after_data = []</span><br><span class="line">    text = text.encode(<span class="string">'ascii'</span>, <span class="string">'ignore'</span>)  <span class="comment"># 内容解码</span></span><br><span class="line">    text = re.sub(<span class="string">'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'</span>, <span class="string">''</span>, text)  <span class="comment"># 将url替换为空字符串</span></span><br><span class="line">    text = remove_spl_char_regex.sub(<span class="string">" "</span>, text)  <span class="comment"># 去除特殊字符</span></span><br><span class="line">    text = text.lower()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> text.split():</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords \</span><br><span class="line">                <span class="keyword">and</span> word <span class="keyword">not</span> <span class="keyword">in</span> string.punctuation \</span><br><span class="line">                <span class="keyword">and</span> <span class="built_in">len</span>(word) &gt; <span class="number">1</span> \</span><br><span class="line">                <span class="keyword">and</span> word != <span class="string">'``'</span>:</span><br><span class="line">            seg_after_data.append(word)</span><br><span class="line">    <span class="keyword">return</span> seg_after_data</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102137059.png" alt="image-20240725102137059"></p><p>3.1.1本函数接受原始数据文本，针对文本数据进行分词处理</p><p>3.1.2推特分词为英文分词，只需将每个单词分开即可（与中文分词不同）</p><p>3.1.3使用Python正则re模块将url替换为空格</p><p>3.1.4调用去除特殊字符方法remove_spl_char_regex去除文件中的特殊字符</p><p>3.1.5判断文本中的词是否不是停用词组中的词，且是否不包含标点符号，且是否词的长度大于1，且是否该词不等于空字符串，如果以上都是则则追加到seg_after_data中不做任何操作，如果有一条不满足则不做任何操作，继续下个词的判断</p><h2 id="4、构建basemap可视化图表"><a href="#4、构建basemap可视化图表" class="headerlink" title="4、构建basemap可视化图表"></a>4、构建basemap可视化图表</h2><p>4.1定义可视化函数res_visulization</p><p>4.1.1参数perd_result为sparkmllib情感分析的结果</p><p>4.1.2创建两个python字典</p><p>4.1.3popdensity_ori字典用于保存原始数据的不同州的情感属性</p><p>4.1.4popdensity字典用于保存随机森林分析的不同州的情感属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">res_visulization</span>(<span class="params">pred_result</span>):</span><br><span class="line"></span><br><span class="line">    popdensity_ori = {<span class="string">'New Jersey'</span>:  <span class="number">0.</span>, <span class="string">'Rhode Island'</span>: <span class="number">0.</span>, <span class="string">'Massachusetts'</span>: <span class="number">0.</span>, <span class="string">'Connecticut'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Maryland'</span>: <span class="number">0.</span>,<span class="string">'New York'</span>: <span class="number">0.</span>, <span class="string">'Delaware'</span>: <span class="number">0.</span>, <span class="string">'Florida'</span>: <span class="number">0.</span>, <span class="string">'Ohio'</span>: <span class="number">0.</span>, <span class="string">'Pennsylvania'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Illinois'</span>: <span class="number">0.</span>, <span class="string">'California'</span>: <span class="number">0.</span>, <span class="string">'Hawaii'</span>: <span class="number">0.</span>, <span class="string">'Virginia'</span>: <span class="number">0.</span>, <span class="string">'Michigan'</span>:    <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Indiana'</span>: <span class="number">0.</span>, <span class="string">'North Carolina'</span>: <span class="number">0.</span>, <span class="string">'Georgia'</span>: <span class="number">0.</span>, <span class="string">'Tennessee'</span>: <span class="number">0.</span>, <span class="string">'New Hampshire'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'South Carolina'</span>: <span class="number">0.</span>, <span class="string">'Louisiana'</span>: <span class="number">0.</span>, <span class="string">'Kentucky'</span>: <span class="number">0.</span>, <span class="string">'Wisconsin'</span>: <span class="number">0.</span>, <span class="string">'Washington'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Alabama'</span>:  <span class="number">0.</span>, <span class="string">'Missouri'</span>: <span class="number">0.</span>, <span class="string">'Texas'</span>: <span class="number">0.</span>, <span class="string">'West Virginia'</span>: <span class="number">0.</span>, <span class="string">'Vermont'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Minnesota'</span>:  <span class="number">0.</span>, <span class="string">'Mississippi'</span>: <span class="number">0.</span>, <span class="string">'Iowa'</span>: <span class="number">0.</span>, <span class="string">'Arkansas'</span>: <span class="number">0.</span>, <span class="string">'Oklahoma'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Arizona'</span>: <span class="number">0.</span>, <span class="string">'Colorado'</span>: <span class="number">0.</span>, <span class="string">'Maine'</span>: <span class="number">0.</span>, <span class="string">'Oregon'</span>: <span class="number">0.</span>, <span class="string">'Kansas'</span>: <span class="number">0.</span>, <span class="string">'Utah'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Nebraska'</span>: <span class="number">0.</span>, <span class="string">'Nevada'</span>: <span class="number">0.</span>, <span class="string">'Idaho'</span>: <span class="number">0.</span>, <span class="string">'New Mexico'</span>:  <span class="number">0.</span>, <span class="string">'South Dakota'</span>:<span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'North Dakota'</span>: <span class="number">0.</span>, <span class="string">'Montana'</span>: <span class="number">0.</span>, <span class="string">'Wyoming'</span>: <span class="number">0.</span>, <span class="string">'Alaska'</span>: <span class="number">0.</span>}</span><br><span class="line">    popdensity  = {<span class="string">'New Jersey'</span>:  <span class="number">0.</span>, <span class="string">'Rhode Island'</span>: <span class="number">0.</span>, <span class="string">'Massachusetts'</span>: <span class="number">0.</span>, <span class="string">'Connecticut'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Maryland'</span>: <span class="number">0.</span>,<span class="string">'New York'</span>: <span class="number">0.</span>, <span class="string">'Delaware'</span>: <span class="number">0.</span>, <span class="string">'Florida'</span>: <span class="number">0.</span>, <span class="string">'Ohio'</span>: <span class="number">0.</span>, <span class="string">'Pennsylvania'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Illinois'</span>: <span class="number">0.</span>, <span class="string">'California'</span>: <span class="number">0.</span>, <span class="string">'Hawaii'</span>: <span class="number">0.</span>, <span class="string">'Virginia'</span>: <span class="number">0.</span>, <span class="string">'Michigan'</span>:    <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Indiana'</span>: <span class="number">0.</span>, <span class="string">'North Carolina'</span>: <span class="number">0.</span>, <span class="string">'Georgia'</span>: <span class="number">0.</span>, <span class="string">'Tennessee'</span>: <span class="number">0.</span>, <span class="string">'New Hampshire'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'South Carolina'</span>: <span class="number">0.</span>, <span class="string">'Louisiana'</span>: <span class="number">0.</span>, <span class="string">'Kentucky'</span>: <span class="number">0.</span>, <span class="string">'Wisconsin'</span>: <span class="number">0.</span>, <span class="string">'Washington'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Alabama'</span>:  <span class="number">0.</span>, <span class="string">'Missouri'</span>: <span class="number">0.</span>, <span class="string">'Texas'</span>: <span class="number">0.</span>, <span class="string">'West Virginia'</span>: <span class="number">0.</span>, <span class="string">'Vermont'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Minnesota'</span>:  <span class="number">0.</span>, <span class="string">'Mississippi'</span>: <span class="number">0.</span>, <span class="string">'Iowa'</span>: <span class="number">0.</span>, <span class="string">'Arkansas'</span>: <span class="number">0.</span>, <span class="string">'Oklahoma'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Arizona'</span>: <span class="number">0.</span>, <span class="string">'Colorado'</span>: <span class="number">0.</span>, <span class="string">'Maine'</span>: <span class="number">0.</span>, <span class="string">'Oregon'</span>: <span class="number">0.</span>, <span class="string">'Kansas'</span>: <span class="number">0.</span>, <span class="string">'Utah'</span>: <span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'Nebraska'</span>: <span class="number">0.</span>, <span class="string">'Nevada'</span>: <span class="number">0.</span>, <span class="string">'Idaho'</span>: <span class="number">0.</span>, <span class="string">'New Mexico'</span>:  <span class="number">0.</span>, <span class="string">'South Dakota'</span>:<span class="number">0.</span>,</span><br><span class="line">                      <span class="string">'North Dakota'</span>: <span class="number">0.</span>, <span class="string">'Montana'</span>: <span class="number">0.</span>, <span class="string">'Wyoming'</span>: <span class="number">0.</span>, <span class="string">'Alaska'</span>: <span class="number">0.</span>}</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102210711.png" alt="image-20240725102210711"></p><p>4.1.5获取rawTst_data中的result字段中的积极性polarity并根据user_location字段保存在popdensity_ori中（rawTst_data为测试集hillarytest.json，后面的代码中会定义），获取随机森林预测的结果的积极性并根据user_location字段保存在popdensity中（pred_result是本函数参数）</p><p>4.1.6在终端打印输出字段数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">idx = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> rawTst_data[<span class="string">'results'</span>]:</span><br><span class="line">    user_location = obj[<span class="string">'user_location'</span>]</span><br><span class="line">    popdensity_ori[user_location] += (obj[<span class="string">'polarity'</span>] - <span class="number">1</span>)</span><br><span class="line">    popdensity[user_location] += (pred_result[idx] - <span class="number">1</span>)</span><br><span class="line">    idx += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'popdensity_ori'</span>)</span><br><span class="line"><span class="built_in">print</span>(popdensity_ori)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"---------------------------------------------------------"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'popdensity'</span>)</span><br><span class="line"><span class="built_in">print</span>(popdensity)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"---------------------------------------------------------"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102224859.png" alt="image-20240725102224859"></p><p>4.1.7在可视化结果中利用basemap中matplotlib的pyplot生成画布figure并定义尺寸，为画布创建两个图实例ax1和ax3，并设置坐标属性（add_axes中的参数分别代表left：绘图区左侧边缘线与Figure画布左侧边缘线的距离；bottom：绘图区底部边线与Figure画布底部边缘线的距离；width：绘图区的宽度；height：绘图区的高度）。创建图m1应用图实例ax1，用于展示原始数据情感分析的结果，读取配置文件st99_d00来获取美国兰伯特正形图下的48个州并画出州边界，定义各个州的缩写以及各个州的经度和纬度，这里我们只给出了部分州的缩写，美国较小的城市没有具体列出，示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取美国兰伯特正形图下的48个州</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">ax1 = fig.add_axes([<span class="number">0.05</span>, <span class="number">0.20</span>, <span class="number">0.40</span>, <span class="number">0.75</span>])</span><br><span class="line">ax3 = fig.add_axes([<span class="number">0.50</span>, <span class="number">0.20</span>, <span class="number">0.40</span>, <span class="number">0.75</span>])</span><br><span class="line">m1 = Basemap(llcrnrlon=-<span class="number">119</span>,llcrnrlat=<span class="number">22</span>,urcrnrlon=-<span class="number">64</span>,urcrnrlat=<span class="number">49</span>,</span><br><span class="line">            projection=<span class="string">'lcc'</span>,lat_1=<span class="number">33</span>,lat_2=<span class="number">45</span>,lon_0=-<span class="number">95</span>, ax = ax1)</span><br><span class="line"><span class="comment"># 绘画州边界</span></span><br><span class="line">shp_info = m1.readshapefile(<span class="string">'st99_d00'</span>,<span class="string">'states'</span>,drawbounds=<span class="literal">True</span>)</span><br><span class="line">cities = [<span class="string">'WA'</span>, <span class="string">'OR'</span>, <span class="string">'CA'</span>, <span class="string">'NV'</span>, <span class="string">'MT'</span>,<span class="string">'ID'</span>,<span class="string">'WY'</span>,<span class="string">'UT'</span>,</span><br><span class="line">      <span class="string">'CO'</span>, <span class="string">'AZ'</span>, <span class="string">'NM'</span>, <span class="string">'ND'</span>, <span class="string">'SD'</span>, <span class="string">'NE'</span>, <span class="string">'KS'</span>,</span><br><span class="line">      <span class="string">'OK'</span>, <span class="string">'TX'</span>, <span class="string">'MN'</span>, <span class="string">'IA'</span>, <span class="string">'MO'</span>, <span class="string">'AR'</span>, <span class="string">'LA'</span>,</span><br><span class="line">      <span class="string">'WI'</span>, <span class="string">'IL'</span>, <span class="string">'MI'</span>, <span class="string">'IN'</span>, <span class="string">'OH'</span>, <span class="string">'KY'</span>, <span class="string">'TN'</span>,</span><br><span class="line">      <span class="string">'MS'</span>,<span class="string">'AL'</span>, <span class="string">'PA'</span>, <span class="string">'WV'</span>, <span class="string">'GA'</span>, <span class="string">'ME'</span>, <span class="string">'VT'</span>,</span><br><span class="line">      <span class="string">'NY'</span>, <span class="string">'VA'</span>, <span class="string">'NC'</span>, <span class="string">'SC'</span>, <span class="string">'FL'</span>, <span class="string">'AL'</span>]</span><br><span class="line"><span class="comment"># 纬度</span></span><br><span class="line">lat = [<span class="number">47.40</span>, <span class="number">44.57</span>, <span class="number">36.12</span>, <span class="number">38.31</span>, <span class="number">46.92</span>, <span class="number">44.24</span>,</span><br><span class="line">       <span class="number">42.75</span>, <span class="number">40.15</span>, <span class="number">39.06</span>, <span class="number">33.73</span>, <span class="number">34.84</span>, <span class="number">47.53</span>,</span><br><span class="line">       <span class="number">44.30</span>, <span class="number">41.125</span>, <span class="number">38.526</span>, <span class="number">35.565</span>, <span class="number">31.05</span>,</span><br><span class="line">       <span class="number">45.69</span>, <span class="number">42.01</span>, <span class="number">38.46</span>, <span class="number">34.97</span>, <span class="number">31.17</span>, <span class="number">44.27</span>,</span><br><span class="line">       <span class="number">40.35</span>, <span class="number">43.33</span>, <span class="number">39.85</span>, <span class="number">40.39</span>, <span class="number">37.67</span>, <span class="number">35.75</span>,</span><br><span class="line">       <span class="number">32.74</span>, <span class="number">61.37</span>, <span class="number">40.59</span>, <span class="number">38.49</span>, <span class="number">33.04</span>, <span class="number">44.69</span>,</span><br><span class="line">       <span class="number">44.045</span>, <span class="number">42.165</span>, <span class="number">37.77</span>, <span class="number">35.63</span>, <span class="number">33.86</span>, <span class="number">27.77</span>,</span><br><span class="line">       <span class="number">32.81</span>]</span><br><span class="line"><span class="comment"># 经度</span></span><br><span class="line">lon = [-<span class="number">121.49</span>, -<span class="number">122.07</span>, -<span class="number">119.68</span>, -<span class="number">117.05</span>, -<span class="number">110.45</span>,</span><br><span class="line">       -<span class="number">114.48</span>, -<span class="number">107.30</span>, -<span class="number">111.86</span>, -<span class="number">105.31</span>, -<span class="number">111.43</span>,</span><br><span class="line">       -<span class="number">106.25</span>, -<span class="number">99.93</span>, -<span class="number">99.44</span>, -<span class="number">98.27</span>, -<span class="number">96.726</span>,</span><br><span class="line">       -<span class="number">96.93</span>, -<span class="number">97.56</span>, -<span class="number">93.90</span>, -<span class="number">93.21</span>, -<span class="number">92.29</span>,</span><br><span class="line">       -<span class="number">92.37</span>, -<span class="number">91.87</span>, -<span class="number">89.62</span>, -<span class="number">88.99</span>, -<span class="number">84.54</span>,</span><br><span class="line">       -<span class="number">86.26</span>, -<span class="number">82.76</span>, -<span class="number">84.67</span>, -<span class="number">86.70</span>, -<span class="number">89.68</span>,</span><br><span class="line">       -<span class="number">152.40</span>, -<span class="number">77.21</span>, -<span class="number">80.95</span>, -<span class="number">83.64</span>, -<span class="number">69.38</span>,</span><br><span class="line">       -<span class="number">72.71</span>, -<span class="number">74.95</span>, -<span class="number">78.17</span>, -<span class="number">79.81</span>, -<span class="number">80.945</span>,</span><br><span class="line">       -<span class="number">81.67</span>, -<span class="number">86.79</span>]</span><br><span class="line"><span class="built_in">print</span>(shp_info)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102250276.png" alt="image-20240725102250276"></p><p>4.1.8定义颜色集color和color2，分别用于保存原始数据的情感分析颜色和spark mllib分析预测的情感分析颜色，使用cm.GMT_polar渐变色用以展示不同的情感；分别获取基于测试数据和基于模型分析的48个大州情感属性的最大和最小值</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">根据人口密度选择每个州的颜色</span></span><br><span class="line">colors={}</span><br><span class="line">colors2 = {}</span><br><span class="line">statenames=[]</span><br><span class="line">cmap = cm.GMT_polar</span><br><span class="line"></span><br><span class="line">inverse = [(value, key) for key, value in popdensity_ori.items()]</span><br><span class="line">vmin = min(inverse)[0]</span><br><span class="line">vmax = max(inverse)[0]  # 设置范围</span><br><span class="line">inverse = [(value, key) for key, value in popdensity.items()]</span><br><span class="line">vmin_pred = min(inverse)[0]</span><br><span class="line">inverse = [(value, key) for key, value in popdensity.items()]</span><br><span class="line">vmin_pred = min(inverse)[0]</span><br><span class="line">vmax_pred = max(inverse)[0]</span><br><span class="line">print('vmax:')</span><br><span class="line">print(vmax)</span><br><span class="line">print(m1.states_info[0].keys())</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102309923.png" alt="image-20240725102309923"></p><p>4.1.9循环获取美国48个州的名字（跳过哥伦毕亚特区DC和波多黎各自治邦），并赋值给statname变量，根据州名获取不同情感属性集中的情感等级，判断并确定颜色，不同州的颜色随情感越积极越红，越消极越蓝，无感状态时为无色，蓝色为小于cmap的0.5，无色为cmap的0.5，红色为大于cmap的0.5</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> shapedict <span class="keyword">in</span> m1.states_info:</span><br><span class="line">    statename = shapedict[<span class="string">'NAME'</span>]</span><br><span class="line">    <span class="comment"># 跳过特区和波多黎各</span></span><br><span class="line">    <span class="keyword">if</span> statename <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'District of Columbia'</span>,<span class="string">'Puerto Rico'</span>]:</span><br><span class="line">        pop = popdensity_ori[statename]</span><br><span class="line">        pop_pred = popdensity[statename]</span><br><span class="line">        <span class="comment"># 定义颜色表的值在0到1之间</span></span><br><span class="line">        <span class="comment"># 以开平方根展开的颜色反转颜色范围</span></span><br><span class="line">        <span class="keyword">if</span> pop == <span class="number">0</span>:</span><br><span class="line">           colors[statename] = cmap(<span class="number">0.5</span>)[:<span class="number">3</span>]</span><br><span class="line">        <span class="keyword">elif</span> pop &lt; <span class="number">0</span>:</span><br><span class="line">           colors[statename] = cmap(<span class="number">1.0</span> - np.sqrt((pop - vmin)/(<span class="number">0</span>-vmin)))[:<span class="number">3</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">           colors[statename] = cmap(<span class="number">0.5</span> - np.sqrt((pop - <span class="number">0</span>)/(vmax-<span class="number">0</span>)))[:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pop_pred == <span class="number">0</span>:</span><br><span class="line">            colors2[statename] = cmap(<span class="number">0.5</span>)[:<span class="number">3</span>]</span><br><span class="line">        <span class="keyword">elif</span> pop_pred &lt; <span class="number">0</span>:</span><br><span class="line">            colors2[statename] = cmap(<span class="number">1.0</span> - np.sqrt((pop_pred - vmin_pred) / (<span class="number">0</span> - vmin_pred)))[:<span class="number">3</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            colors2[statename] = cmap(<span class="number">0.5</span> - np.sqrt((pop_pred - <span class="number">0</span>) / (vmax_pred - <span class="number">0</span>)))[:<span class="number">3</span>]</span><br><span class="line">    statenames.append(statename)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102330893.png" alt="image-20240725102330893"></p><p>4.1.10根据州名遍历所有的州，并根据大州名获取当前轴实例ploy，填充颜色</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">遍历大州名称，为每一个州绘制颜色</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">ax = plt.gca() <span class="comment"># 获取当前轴实例</span></span></span><br><span class="line">for nshape,seg in enumerate(m1.states):</span><br><span class="line">    # 跳过特区和波多黎各</span><br><span class="line">    if statenames[nshape] not in ['District of Columbia','Puerto Rico']:</span><br><span class="line">        color = rgb2hex(colors[statenames[nshape]])</span><br><span class="line">        #print(statenames[nshape])</span><br><span class="line">        poly = Polygon(seg,facecolor=color,edgecolor=color)</span><br><span class="line">        ax1.add_patch(poly)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102352832.png" alt="image-20240725102352832"></p><p>4.1.11为图m1绘制经纬线lon、lat，并在地图上添加不同州的的缩写，设置图形标题，定义图m2应用图实例ax3，根据大州名为每一个州确定颜色，并根据大州名获取当前轴实例ploy，为图m2绘制经纬线lon、lat，设置x轴y轴坐标，设置图形标题，用于展示随机森林预测的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">m1.drawparallels(np.arange(<span class="number">25</span>,<span class="number">65</span>,<span class="number">20</span>),labels=[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">m1.drawmeridians(np.arange(-<span class="number">120</span>,-<span class="number">40</span>,<span class="number">20</span>),labels=[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">x, y = m1(lon, lat)</span><br><span class="line"><span class="keyword">for</span> city, xc, yc <span class="keyword">in</span> <span class="built_in">zip</span>(cities, x, y):</span><br><span class="line">    ax1.text(xc - <span class="number">60000</span>, yc - <span class="number">50000</span>, city)</span><br><span class="line">ax1.set_title(<span class="string">'Twitter-based sentiment analysis about Hillary '</span>)</span><br><span class="line"></span><br><span class="line">m2 = Basemap(llcrnrlon=-<span class="number">119</span>,llcrnrlat=<span class="number">22</span>,urcrnrlon=-<span class="number">64</span>,urcrnrlat=<span class="number">49</span>,</span><br><span class="line">            projection=<span class="string">'lcc'</span>,lat_1=<span class="number">33</span>,lat_2=<span class="number">45</span>,lon_0=-<span class="number">95</span>, ax = ax3)</span><br><span class="line">m2.readshapefile(<span class="string">'st99_d00'</span>, <span class="string">'states'</span>, drawbounds=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> nshape, seg <span class="keyword">in</span> <span class="built_in">enumerate</span>(m2.states):</span><br><span class="line">    <span class="comment"># 跳过特区和波多黎各</span></span><br><span class="line">    <span class="keyword">if</span> statenames[nshape] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'District of Columbia'</span>, <span class="string">'Puerto Rico'</span>]:</span><br><span class="line">        color = rgb2hex(colors2[statenames[nshape]])</span><br><span class="line">        <span class="comment"># print(statenames[nshape])</span></span><br><span class="line">        poly = Polygon(seg, facecolor=color, edgecolor=color)</span><br><span class="line">        ax3.add_patch(poly)</span><br><span class="line"></span><br><span class="line">ax3.set_title(<span class="string">'Random Forest prediction'</span>)</span><br><span class="line">m2.drawparallels(np.arange(<span class="number">25</span>,<span class="number">65</span>,<span class="number">20</span>),labels=[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">m2.drawmeridians(np.arange(-<span class="number">120</span>,-<span class="number">40</span>,<span class="number">20</span>),labels=[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">x, y = m2(lon, lat)</span><br><span class="line"><span class="keyword">for</span> city, xc, yc <span class="keyword">in</span> <span class="built_in">zip</span>(cities, x, y):</span><br><span class="line">    ax3.text(xc - <span class="number">60000</span>, yc - <span class="number">50000</span>, city)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102412777.png" alt="image-20240725102412777"></p><p>4.2构建颜色对比渐变条目</p><p>4.2.1定义图实例ax2，设置坐标属性（add_axes中的参数分别代表left：绘图区左侧边缘线与Figure画布左侧边缘线的距离；bottom：绘图区底部边线与Figure画布底部边缘线的距离；width：绘图区的宽度；height：绘图区的高度），添加渐变色条colorbar，设置图形x轴的三个情感极性，方便对应颜色查看情感属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ax2 = fig.add_axes([<span class="number">0.05</span>, <span class="number">0.10</span>, <span class="number">0.9</span>, <span class="number">0.05</span>])</span><br><span class="line">norm = mpl.colors.Normalize(vmin=-<span class="number">1</span>, vmax=<span class="number">1</span>)</span><br><span class="line">cb1 = mpl.colorbar.ColorbarBase(ax2, cmap=cmap,</span><br><span class="line">                                norm=norm,</span><br><span class="line">                                orientation=<span class="string">'horizontal'</span>,</span><br><span class="line">                                ticks=[-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">cb1.ax.set_xticklabels([<span class="string">'negative'</span>, <span class="string">'natural'</span>, <span class="string">'positive'</span>])</span><br><span class="line">cb1.set_label(<span class="string">'Sentiment'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102429876.png" alt="image-20240725102429876"></p><p>4.2.2创建sparkContext并设置其名称和日志级别问ERROR（即除正常日志外只显示ERROR日志）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setAppName(<span class="string">"zkpk_test"</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102500845.png" alt="image-20240725102500845"></p><p>4.2.3利用sparkContext创建sqlContext，再利用sqlContext读入预先训练好的文本向量化模型word2vecModel下的data文件夹中，并广播到所有节点(此处的地址应为本地文件系统file，因为如果有hdfs文件系统可能会查找hdfs文件系统)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">lookup=sqlContext.read.parquet(<span class="string">"file:/home/zkpk/sparkmllib/word2vecModel/data"</span>).alias(<span class="string">"lookup"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"------------------------------------------------------"</span>)</span><br><span class="line">lookup.printSchema()</span><br><span class="line">lookup_bd = sc.broadcast(lookup.rdd.collectAsMap())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"------------------------------------------------------"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102513794.png" alt="image-20240725102513794"></p><h2 id="5、模型训练"><a href="#5、模型训练" class="headerlink" title="5、模型训练"></a>5、模型训练</h2><p>5.1情感分析相关的函数定义好后，我们便可从json文件中读入数据，创建RDD对象，利用spark mllib的分类器进行情感分析</p><p>5.1.1读入tweetstest.json作为分类器训练数据集</p><p>5.1.2解析json数据并赋值给rawTrn_data</p><p>5.1.3获取rawTrn_data中的results字段，在获取results字段中的text字段，赋值给Segmentation_text</p><p>5.1.4调用doc2vec方法将Segmentation_text文本转换成词向量</p><p>5.15利用数据的情感极性和词向量生成向量标签LabeledPoint，追加到trn_data中</p><p>5.1.6调用sparkContext对象sc将trn_data序列化成RDD数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'tweetstest.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    rawTrn_data = json.load(f)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">trn_data = []</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> rawTrn_data[<span class="string">'results'</span>]:</span><br><span class="line">    Segmentation_text = Segmentation(obj[<span class="string">'text'</span>])</span><br><span class="line">    tweet_text = doc2vec(Segmentation_text)</span><br><span class="line">    trn_data.append(LabeledPoint(obj[<span class="string">'polarity'</span>], tweet_text))</span><br><span class="line"></span><br><span class="line">trnData = sc.parallelize(trn_data)</span><br><span class="line"><span class="comment">#print(trnData)</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102527370.png" alt="image-20240725102527370"></p><p>5.2读入hillarytest.json作为分类器测试数据集（此处的预测测集为hillarytest.json，也就是预测人们对希拉里的情感极性分布预测，如果想预测唐纳德则将数据文件换成donaldtest.json即可）</p><p>5.2.1解析json数据并赋值给rawTst_data</p><p>5.2.2获取rawTst_data中的results字段，在获取results字段中的text字段，赋值给Segmentation_text</p><p>5.2.3调用doc2vec方法将Segmentation_text文本转换成词向量</p><p>5.2.4利用数据的情感极性和词向量生成向量标签LabeledPoint，追加到tst_data中</p><p>5.2.5调用sparkContext对象sc将tst_data序列化成RDD数据</p><p>5.2.6调用随机森林中的类RandomForest中的训练函数trainClassifier，传入模型训练数据trnData，开始训练模型model</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'hillarytest.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    rawTst_data = json.load(f)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">tst_data = []</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> rawTst_data[<span class="string">'results'</span>]:</span><br><span class="line">    Segmentation_text = Segmentation(obj[<span class="string">'text'</span>])</span><br><span class="line">    tweet_text = doc2vec(Segmentation_text)</span><br><span class="line">    tst_data.append(LabeledPoint(obj[<span class="string">'polarity'</span>], tweet_text))</span><br><span class="line"></span><br><span class="line">tst_dataRDD = sc.parallelize(tst_data)</span><br><span class="line"></span><br><span class="line">model = RandomForest.trainClassifier(trnData, numClasses=<span class="number">3</span>, categoricalFeaturesInfo={},numTrees=<span class="number">3</span>, featureSubsetStrategy=<span class="string">"auto"</span>,impurity=<span class="string">'gini'</span>, maxDepth=<span class="number">4</span>, maxBins=<span class="number">32</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102551203.png" alt="image-20240725102551203"></p><h2 id="6、模型预测"><a href="#6、模型预测" class="headerlink" title="6、模型预测"></a>6、模型预测</h2><p>6.1利用训练好的模型model进行模型性能测试，调用模型的predict函数，传入测试集数据tst_dataRDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictions = model.predict(tst_dataRDD.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.features))</span><br><span class="line">labelsAndPredictions = tst_dataRDD.<span class="built_in">map</span>(<span class="keyword">lambda</span> lp: lp.label).<span class="built_in">zip</span>(predictions)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102606287.png" alt="image-20240725102606287"></p><p>6.2计算分类错误率，并打印输出训练好的随机森林的分类决策模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">testErr=labelsAndPredictions.<span class="built_in">filter</span>(<span class="keyword">lambda</span>(v,p):v!=p).count()/<span class="built_in">float</span>(tst_dataRDD.count())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Test Error = '</span> + <span class="built_in">str</span>(testErr))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Learned classification tree model:'</span>)</span><br><span class="line"><span class="built_in">print</span>(model.toDebugString())</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102622813.png" alt="image-20240725102622813"></p><p>6.3调用数据可视化函数res_visulization，对训练好的模型进行展示，输出完毕，调用sparkContext对象的stop方法关闭连接</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">res_visulization(predictions.collect())</span><br><span class="line"><span class="comment"># 关闭spark进程</span></span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102639933.png" alt="image-20240725102639933"></p><h2 id="7、程序运行"><a href="#7、程序运行" class="headerlink" title="7、程序运行"></a>7、程序运行</h2><p>7.1编辑代码完成，使用：wq保存退出vim编辑器</p><p>7.2利用spark-submit提交程序运行，运行模式为local</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(zkbc)[zkpk@master sparkmllib]$ spark-submit --master=local sparktest.py</span><br></pre></td></tr></table></figure><p>7.3等待程序运行，查看界面弹出结果</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725102739034.png" alt="image-20240725102739034"></p><p>7.4通过可视化结果，我们可以直观的感受到美国曾经的候选总统希拉里在美国各个州的受欢迎程度。我们也可以看到随机森林预测的结果大致准确，但是误差还是不小的，这与模型参数和数据量都有很大的关系，通过设置模型参数和增大数据量都能提升模型准确性，这里就不在赘述了。感兴趣的同学可以参考本实验针对从网络上获取的微博或者推特数据进行其他领域的情感分析，例如：房价、物价、生活幸福度等等</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;大数据行业应用-情感分析&quot;&gt;&lt;a href=&quot;#大数据行业应用-情感分析&quot; class=&quot;headerlink&quot; title=&quot;大数据行业应用-情感分析&quot;&gt;&lt;/a&gt;大数据行业应用-情感分析&lt;/h2&gt;&lt;h2 id=&quot;1、数据准备&quot;&gt;&lt;a href=&quot;#1、数据准备&quot;</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据行业应用-交通轨迹</title>
    <link href="https://want595.github.io/2024/07/25/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8-%E4%BA%A4%E9%80%9A%E8%BD%A8%E8%BF%B9/"/>
    <id>https://want595.github.io/2024/07/25/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8-%E4%BA%A4%E9%80%9A%E8%BD%A8%E8%BF%B9/</id>
    <published>2024-07-25T02:03:29.000Z</published>
    <updated>2024-07-25T02:43:44.755Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据行业应用-交通轨迹"><a href="#大数据行业应用-交通轨迹" class="headerlink" title="大数据行业应用-交通轨迹"></a>大数据行业应用-交通轨迹</h1><h2 id="1、数据准备"><a href="#1、数据准备" class="headerlink" title="1、数据准备"></a>1、数据准备</h2><p>1.1在master节点创建实验文件夹，拷贝数据文件</p><p>1.1.1在zkpk家目录下创建实验文件夹taxifx，并进入实验文件夹taxifx</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master ~]$ mkdir taxifx</span><br><span class="line">[zkpk@master ~]$ cd taxifx</span><br></pre></td></tr></table></figure><p>1.1.2拷贝实验数据到实验文件夹中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master taxifx]$ cp ~/experiment/taxi.csv ./</span><br></pre></td></tr></table></figure><p>1.1.3CSV 格式是数据分析工作中常见的一种数据格式。CSV意为逗号分隔值（Comma-Separated Values），其文件以纯文本形式存储表格数据（数字和文本）。每行只有一条记录，每条记录被逗号分隔符分隔为字段，并且每条记录都有同样的字段序列。</p><h2 id="2、解析csv数据"><a href="#2、解析csv数据" class="headerlink" title="2、解析csv数据"></a>2、解析csv数据</h2><p>2.1使用由 DataBricks 公司提供的第三方 Spark CSV 解析库来读取</p><p>2.1.1加载csv第三方插件，启动spark-shell（加载csv解析插件），出现如下结果即为csv解析包下载成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master taxifx]$ cd</span><br><span class="line">[zkpk@master ~]$ spark-shell --packages com.databricks:spark-csv_2.11:1.1.0</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725093623457.png" alt="image-20240725093623457"></p><p>2.1.2导入数据</p><p>2.1.2.1导入实验所需的spark包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark._</span><br><span class="line">import org.apache.spark.sql._</span><br><span class="line">import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line">import org.apache.spark.ml.feature.VectorAssembler</span><br><span class="line">import org.apache.spark.ml.clustering.KMeans</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725093659464.png" alt="image-20240725093659464"></p><p>2.1.2.2利用StructType定义字段格式，这里应该和csv数据集中的字段一一对应，StructField中的三个参数为字段名称，数据类型，和是否不允许为空</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val fieldSchema = StructType(Array(</span><br><span class="line">StructField("TID",StringType,true),</span><br><span class="line">StructField("Lat",DoubleType,true),</span><br><span class="line">StructField("Lon",DoubleType,true),</span><br><span class="line">StructField("Time",StringType,true)</span><br><span class="line">))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725093814327.png" alt="image-20240725093814327"></p><p>2.1.2.3利用sc创建sqlContext，用于执行SparkSQL，利用sqlContext对象的read接口，加载格式format为com.databricks.spark.csv的数据文件，并赋值给DataFrame对象taxidf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlContext = new SQLContext(sc)</span><br><span class="line">val taxidf = sqlContext.read.format("com.databricks.spark.csv").option("header","false").schema(fieldSchema).load("/home/zkpk/taxifx/taxi.csv")</span><br></pre></td></tr></table></figure><p>2.1.2.4利用taxidf对象的printSchema方法打印输出该对象的字段格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taxidf.printSchema()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725093902958.png" alt="image-20240725093902958"></p><p>2.1.2.5利用taxidf对象的show方法打印输出前20条数据（默认为输出前20条数据）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taxidf.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725093935425.png" alt="image-20240725093935425"></p><h2 id="3、构建特征向量"><a href="#3、构建特征向量" class="headerlink" title="3、构建特征向量"></a>3、构建特征向量</h2><p>3.1转换数据字段中的经纬度，定义特征数组</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val columns = Array("Lat","Lon")</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725093957660.png" alt="image-20240725093957660"></p><p>3.2创建向量装配器VetorAssembler，并设置相关属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val va = new VectorAssembler().setInputCols(columns).setOutputCol("features")</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094106538.png" alt="image-20240725094106538"></p><p>3.3利用向量装配器的transform方法对导入的数据taxidf进行转化，并赋值给taxidf2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val taxidf2 = va.transform(taxidf)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094131947.png" alt="image-20240725094131947"></p><p>3.4利用taxidf2对象的show方法打印前20条数据查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taxidf2.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094204105.png" alt="image-20240725094204105"></p><h2 id="4、聚类模型训练"><a href="#4、聚类模型训练" class="headerlink" title="4、聚类模型训练"></a>4、聚类模型训练</h2><p>4.1进行K-Means聚类模型训练</p><p>4.1.1K-Means是迭代算法，所以将数据缓存到内存中加快计算速度</p><p>4.1.1.1Spark命令行方法，没有具体参数的话，可以加上括号，也可以不加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taxidf2.cache</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094229275.png" alt="image-20240725094229275"></p><p>4.1.2将数据集划分比例分别作为训练集和测试集</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val trainTestRatio = Array(0.7,0.3)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094247811.png" alt="image-20240725094247811"></p><p>4.1.3对数据集进行随机划分，randomSplit 的第二个参数为随机数的种子</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val Array(trainingData,testData) = taxidf2.randomSplit(trainTestRatio,2333)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094307507.png" alt="image-20240725094307507"></p><p>4.1.4设置K-Means模型参数，创建模型</p><p>4.1.4.1setK()：是一个 “Parameter setter”，用于设置聚类的簇数量</p><p>4.1.4.2setFeaturesCol()：设置数据集中的特征列所在的字段名称</p><p>4.1.4.3setPredictionCol：设置生成预测值时使用的字段名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val km = new KMeans().setK(10).setFeaturesCol("features").setPredictionCol("prediction")</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094402227.png" alt="image-20240725094402227"></p><p>4.1.5利用fit方法将Kmeans对象对指定数据的特征进行匹配适应，训练模型</p><p>4.1.5.1fit()：将 KMeans 对象对指定数据的特征进行匹配适应，训练模型</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val kmModel = km.fit(taxidf2)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094439602.png" alt="image-20240725094439602"></p><p>4.1.6获取Kmeans模型的聚类中心，可以看到之前设定数量为 10 的聚类结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val Result = kmModel.clusterCenters</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094453583.png" alt="image-20240725094453583"></p><p>4.1.7将结果转换为RDD类型，进行经纬度互换，调用RDD对象的saveAsTextFile方法保存结果到本地</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val kmRDD1 = sc.parallelize(Result)</span><br><span class="line">val kmRDD2 = kmRDD1.map(x=&gt;(x(1),x(0)))</span><br><span class="line">kmRDD2.saveAsTextFile("/home/zkpk/taxifx/kmResult")</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094519408.png" alt="image-20240725094519408"></p><h2 id="5、聚类模型测试"><a href="#5、聚类模型测试" class="headerlink" title="5、聚类模型测试"></a>5、聚类模型测试</h2><p>5.1.对测试集进行聚类分析</p><p>5.1.1调用Kmeans模型的transform方法对测试数据进行聚类，结果赋值给predictions对象</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val predictions = kmModel.transform(testData)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094546041.png" alt="image-20240725094546041"></p><p>5.1.2调用predictions对象的show方法，输出预测结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094607402.png" alt="image-20240725094607402"></p><h2 id="6、分析预测结果"><a href="#6、分析预测结果" class="headerlink" title="6、分析预测结果"></a>6、分析预测结果</h2><p>6.1预测结果为DataFrame，我们先将其注册为临时表perdictions，然后使用SQL查询功能</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions.registerTempTable("predictions")</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094633607.png" alt="image-20240725094633607"></p><p>6.2利用SQL查询”每天哪个时段的出租车最繁忙”和“每天哪个时段的出租车最空闲”需求</p><p>6.2.1提取出 Time 字段的前 2 位作为一天之中的小时数</p><p>6.2.2基于小时数进行不同预测类型的数量进行统计</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val tmpQuery = predictions.select(substring($"Time",0,2).alias("hour"),$"prediction").groupBy("hour","prediction")</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094646524.png" alt="image-20240725094646524"></p><p>6.2.3利用聚焦函数agg的count实现，并以desc降序输出结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val predictCountdesc = tmpQuery.agg(count("prediction").alias("count")).orderBy(desc("count"))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094708375.png" alt="image-20240725094708375"></p><p>6.2.4可以发现某一区域在前20条数据中出现次数较高，此区域即为出租车最繁忙区域，如4号区域，可以看到在下午14-15点这一时间段，4号区域的载客量为5552最多最繁忙，，因此我们在对表predictions进行where查询，找出4号区域的经纬度记录下来，稍后我们看一下这个位置具体在哪里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictCountdesc.show()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val busyarea = predictions.select($"Lat",$"Lon").where("prediction=4")</span><br><span class="line">busyarea.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094732152.png" alt="image-20240725094732152"></p><p>6.2.4.1可以看出繁忙的4区域的经纬度基本在[104.072959,30.658476]左右</p><p>6.2.5再次利用聚焦函数agg的count实现，并以asc降序输出结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val predictCountasc = tmpQuery.agg(count("prediction").alias("count")).orderBy(asc("count"))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094801703.png" alt="image-20240725094801703"></p><p>6.2.6可以发现某一区域在前20条数据中出现次数较高，此区域即为出租车最空闲区域，例如9号区域，可以发现在早上8点到10点时间段，9号区域的载客量为最少最空闲，因此我们在对表predictions进行查询，找出9号区域的经纬度记录下来，稍后我们看一下这个位置具体在哪里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictCountasc.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094826807.png" alt="image-20240725094826807"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val notbusyarea = predictions.select($"Lat",$"Lon").where("prediction=9")</span><br><span class="line">notbusyarea.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094843285.png" alt="image-20240725094843285"></p><p>6.2.6.1可以看到不繁忙区域9的经纬度在[104.571579,30.368929]左右</p><p>6.3利用SQL查询“每天哪个区域的出租车最繁忙”和“每天哪个区域的出租车最空闲”需求</p><p>6.3.1这里的结果可以对聚类区域进行分组统计操作，即可得到每个区域的出租车载客次数总计</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val busyZones = predictions.groupBy("prediction").count()</span><br><span class="line">busyZones.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094915815.png" alt="image-20240725094915815"></p><p>6.3.1.1可以看到每天区域4最繁忙，载客次数为68366</p><p>6.3.1.2可以看到每天区域9最空闲，载客次数为141</p><p>6.3.2调用busyZones对象的write.csv(“路径”)方法，保存结果到本地</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">busyZones.write.csv("/home/zkpk/taxifx/busyZones")</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094938848.png" alt="image-20240725094938848"></p><p>6.3.3利用:quit退出spark shell</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/image-20240725094951684.png" alt="image-20240725094951684"></p><h2 id="7、数据可视化（可选）"><a href="#7、数据可视化（可选）" class="headerlink" title="7、数据可视化（可选）"></a>7、数据可视化（可选）</h2><p>7.1本实验利用百度地图的相关API实现出租车数据地图可视化</p><p>7.2注册百度地图开放平台和创建自己的应用</p><p>7.2.1现在浏览器中打开百度地图开放平台（<a href="http://lbsyun.baidu.com/">http://lbsyun.baidu.com</a>）,使用百度账户登录（若没有账户请自行创建）</p><p>7.2.2登录完成后，在导航栏输入百度地图开放平台控制台页面（<a href="http://lbsyun.baidu.com/apiconsole/key）">http://lbsyun.baidu.com/apiconsole/key）</a></p><p>7.2.3进入如下界面</p><p>7.2.4在API控制台的“查看应用”界面，点击“创建应用”按钮</p><p>7.2.5填写创建应用表单</p><p>7.2.5.1应用名称可自定义</p><p>7.2.5.2应用类型选择浏览器端</p><p>7.2.5.3白名单为*号</p><p>7.2.5.4点击“提交”按钮</p><p>7.2.6应用创建成功后，可在应用列表中看到相应应用信息</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/b96a38a1d0cdc1c16b60fa0f418e00d4.png" alt="img"></p><p>7.2.7记住上图的应用AK，稍后我们会用此AK去调用百度地图的相关服务</p><p>7.2.8在测试数据使用聚类模型进行数据分析中，我们将结果保存在/home/zkpk/taxifx/kmResult目录中</p><p>7.2.9数据处理</p><p>7.2.9.1利用cat重定向将分散在各个文件中的结果合并在单个文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master ~]$ cd taxifx/kmResult</span><br><span class="line">[zkpk@master kmResult]$ cat part-* &gt;&gt; kmresult.csv</span><br></pre></td></tr></table></figure><p>7.2.9.2利用sed命令来指定字符的删除操作，正则表达式s/^.//匹配了行首的第一个字符（左括号），正则表达式s/.$//匹配了行末的最后一个字符（右括号）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master kmResult]$ sed 's/^.//' kmresult.csv &gt;&gt; test1.csv</span><br><span class="line">[zkpk@master kmResult]$ sed 's/.$//' test1.csv &gt;&gt; test2.csv</span><br></pre></td></tr></table></figure><p>7.2.9.3将所有的换行符替换为分隔符“|”， 正则表达式 :a;N;<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="78.909ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 34877.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="21" d="M78 661Q78 682 96 699T138 716T180 700T199 661Q199 654 179 432T158 206Q156 198 139 198Q121 198 119 206Q118 209 98 431T78 661ZM79 61Q79 89 97 105T141 121Q164 119 181 104T198 61Q198 31 181 16T139 1Q114 1 97 16T79 61Z"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(707,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(1236,0)"><path data-c="3B" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 85 94 103T137 121Q202 121 202 8Q202 -44 183 -94T144 -169T118 -194Q115 -194 106 -186T95 -174Q94 -171 107 -155T137 -107T160 -38Q161 -32 162 -22T165 -4T165 4Q165 5 161 4T142 0Q110 0 94 18T78 60Z"></path></g><g data-mml-node="mi" transform="translate(1680.7,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2149.7,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mtext" fill="red" stroke="red" transform="translate(2649.7,0)"><path data-c="5C" d="M56 731Q56 740 62 745T75 750Q85 750 92 740Q96 733 270 255T444 -231Q444 -239 438 -244T424 -250Q414 -250 407 -240Q404 -236 230 242T56 731Z"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(500,0)"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3705.7,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mo" transform="translate(4205.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4483.7,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(4983.7,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(5460.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">中</text></g><g data-mml-node="mi" transform="translate(6460.7,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">：</text></g><g data-mml-node="mi" transform="translate(7460.7,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(7989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">用</text></g><g data-mml-node="mi" transform="translate(8989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">于</text></g><g data-mml-node="mi" transform="translate(9989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">创</text></g><g data-mml-node="mi" transform="translate(10989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">建</text></g><g data-mml-node="mi" transform="translate(11989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">一</text></g><g data-mml-node="mi" transform="translate(12989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">个</text></g><g data-mml-node="mi" transform="translate(13989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">标</text></g><g data-mml-node="mi" transform="translate(14989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">记</text></g><g data-mml-node="mi" transform="translate(15989.7,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(16989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">通</text></g><g data-mml-node="mi" transform="translate(17989.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">过</text></g><g data-mml-node="mi" transform="translate(18989.7,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mi" transform="translate(19877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">追</text></g><g data-mml-node="mi" transform="translate(20877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">加</text></g><g data-mml-node="mi" transform="translate(21877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">当</text></g><g data-mml-node="mi" transform="translate(22877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">前</text></g><g data-mml-node="mi" transform="translate(23877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">行</text></g><g data-mml-node="mi" transform="translate(24877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">和</text></g><g data-mml-node="mi" transform="translate(25877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">下</text></g><g data-mml-node="mi" transform="translate(26877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">一</text></g><g data-mml-node="mi" transform="translate(27877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">行</text></g><g data-mml-node="mi" transform="translate(28877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">到</text></g><g data-mml-node="mi" transform="translate(29877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">模</text></g><g data-mml-node="mi" transform="translate(30877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">式</text></g><g data-mml-node="mi" transform="translate(31877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">区</text></g><g data-mml-node="mi" transform="translate(32877.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">域</text></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(33877.7,0)"><g data-mml-node="mo"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">。</text></g></g></g></g></svg></mjx-container>!ba<br>的意思是如果处于最后一行前，则跳转到之前的标记处。最后定义好的置换操作，把模式区域（即整个文件）的每一个换行符<br>\n 换成一个分隔符“|”</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master kmResult]$ sed <span class="string">':a;N;$!ba;s/\n/|/g'</span> test2.csv &gt;&gt; test3.txt</span><br></pre></td></tr></table></figure><p>7.2.9.4用cat打开test3.txt查看并复制所有参数作为请求后面的参数</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/4874ff31fe41b338f9fbcdb89ecc20b5.png" alt="img"></p><p>7.2.10设置请求百度地图API的相关参数</p><p>7.2.10.1应用授权码AK（此处的ak码为你创建应用时的ak码）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ak= FNlqH4eBZmMCyCELAz4VWsNcZH******</span><br></pre></td></tr></table></figure><p>7.2.10.2设置图片的宽高度</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">height=650</span><br><span class="line">width=1000</span><br></pre></td></tr></table></figure><p>7.2.10.3设置地图中心点的经纬度</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">center=104.072959,30.658476</span><br></pre></td></tr></table></figure><p>7.2.10.4设置经纬度列表（直接粘贴刚才赋值的test3.txt的内容即可）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">markers=104.06722420765115,30.599701217390912|103.97678653966989,30.568331441513585|103.87500910854187,30.72471987264872|103.62871623398165,30.92740290503437|104.06526902861037,30.65233797319958|104.0788635677185,30.699548133578695|104.11641860369171,30.641546653863603|104.01607522993989,30.66518876584617|104.2584157733787,30.673234293660432|104.49812695974578,30.50180488983051</span><br></pre></td></tr></table></figure><p>7.2.10.5设置地图缩放级别</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zoom=11</span><br></pre></td></tr></table></figure><p>7.2.11我们将上述各项参数结合起来拼接成API请求即可向百度地图开放平台提交请求（下面的URL只是一个例子，ak请替换成自己创建的）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://api.map.baidu.com/staticimage/v2?width=1000&amp;height=650&amp;center=104.072959,30.658476&amp;markers=104.06722420765115,30.599701217390912|103.97678653966989,30.568331441513585|103.87500910854187,30.72471987264872|103.62871623398165,30.92740290503437|104.06526902861037,30.65233797319958|104.0788635677185,30.699548133578695|104.11641860369171,30.641546653863603|104.01607522993989,30.66518876584617|104.2584157733787,30.673234293660432|104.49812695974578,30.50180488983051&amp;zoom=11&amp;ak=FNlqH4eBZmMCyCELAz4VWsNcZH******</span><br></pre></td></tr></table></figure><p>7.2.12在浏览器打开输入URL回车，即可得到聚类结果的10个簇中心在地图上的位置，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/68c3cf884bc7c05b2d7a43eb1fd66cdd.png" alt="img"></p><p>7.2.13针对每天哪个时段的出租车最繁忙和最空闲这一需求，前面我们已经在解决这一问题时保存了在某一时段最繁忙和最空闲的两个区域的经纬度，这里我们将URL的markers参数修改为4号区域的经纬度，并输入在浏览器中，即可知道出租车最繁忙的4号区域地理位置，如下图</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/7151581119e3315c372b534837e4c558.png" alt="img"></p><p>7.2.14这里我们将URL中的markers参数修改为9号区域的经纬度，并输入在浏览器中，即可知道出租车最空闲的9号区域的地理位置，如下图</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/83058b430384ef4d6df1d24739a80e8e.png" alt="img"></p><p>7.2.15我们还可以通过查询百度地图经纬度，修改center参数和缩放级别达到查看具体地址的效果，这里就不在赘述了</p><p>7.3通过绘制柱状图比较不同的数据</p><p>7.3.1创建项目：这里我们需要使用提供好的D3.js和其他的javascript脚本文件来构建效果网页</p><p>7.3.2创建一个新的文件夹Visualization，并在此文件夹下创建名为data和js的两个文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master kmResult]$ cd </span><br><span class="line">[zkpk@master ~]$ cd taxifx</span><br><span class="line">[zkpk@master taxifx]$ mkdir Visualization</span><br><span class="line">[zkpk@master taxifx]$ cd Visualization</span><br><span class="line">[zkpk@master Visualization]$ mkdir data</span><br><span class="line">[zkpk@master Visualization]$ mkdir js</span><br></pre></td></tr></table></figure><p>7.3.3解压并拷贝所有的js文件到此js目录中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master Visualization]$ cd ~/tgz/</span><br><span class="line">[zkpk@master tgz]$ tar -zxvf js.tar.gz</span><br><span class="line">[zkpk@master tgz]$ cp *.js ~/taxifx/Visualization/js/</span><br></pre></td></tr></table></figure><p>7.3.4将数据聚类分析的结果busyZones合并成单个文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master tgz]$ cd</span><br><span class="line">[zkpk@master ~]$ cd taxifx/busyZones</span><br><span class="line">[zkpk@master busyZones]$ cat part-* &gt;&gt; data.csv</span><br></pre></td></tr></table></figure><p>7.3.5在csv文件的首行加上字段名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master busyZones]$ sed -i "1i zone,numOfServices" data.csv</span><br></pre></td></tr></table></figure><p>7.3.6将数据文件data.csv拷贝到data文件夹下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master busyZones]$ cp data.csv ~/taxifx/Visualization/data/</span><br></pre></td></tr></table></figure><p>7.3.7在Visualization目录下创建编辑index.html文件（如果对html语言不太熟悉，我们也提供了写好的html在/home/zkpk/tgz目录下，直接拷贝到指定位置使用亦可）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zkpk@master busyZones]$ cd</span><br><span class="line">[zkpk@master ~]$ cd taxifx/Visualization</span><br><span class="line">[zkpk@master Visualization]$ vim index.html</span><br></pre></td></tr></table></figure><p>7.3.8在html文件中插入基本的HTML元素</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/f4d8824ff68ffb444a13ff0ea3e77085.png" alt="img"></p><p>7.3.9因为我们会用到d3.js的相关API来绘图，所以在body中引用相关js文件（js文件已经拷贝到js文件夹下了）</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/81052364a30d7de338a9cd3aced01f70.png" alt="img"></p><p>7.3.10在body中填充画布的JavaScript代码（以下代码是包含在一个script标签中）</p><p>7.3.10.1script标签开始，设置画布大小和边距</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/18fe95e4a2e1ccbb9462c67c01b814ff.png" alt="img"></p><p>7.3.10.2设置X方向的范围和序数</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/9755ef3d1a9688b9747fd2eedfebbfa2.png" alt="img"></p><p>7.3.10.3设置Y方向的范围</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/07b1758a1315d4f642ca23b2e3f07696.png" alt="img"></p><p>7.3.10.4设置X轴的大小和位置</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/9ba56aeb81d7b99ca135870eef9654c2.png" alt="img"></p><p>7.3.10.5设置Y轴的大小和位置</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/ace84e4033327ebf432bef7dac63517c.png" alt="img"></p><p>7.3.10.6设置鼠标指针指向矩形时的提示框属性和内容</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/a558a40f9f94e9a842ca0d90413307c8.png" alt="img"></p><p>7.3.10.7设置画布，在body标签内动态添加svg标签，同时设置画布的宽高和边距</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/470877ff5f44e0a62c0501fec700f720.png" alt="img"></p><p>7.3.10.8调用显示框组件</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/9b3a1be1d4c53e32adcebeebb832cd1e.png" alt="img"></p><p>7.3.10.9读取data目录下的data.csv文件</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/4a0d61adf7fbf98f7909abc508c3b70e.png" alt="img"></p><p>7.3.10.10设置x、y方向上要显示的数据</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/167b42b01ee7b847cf6d6d8d0ca901ce.png" alt="img"></p><p>7.3.10.11在画布上绘制x轴</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/b34675fbf68a41269370f962a29476cb.png" alt="img"></p><p>7.3.10.12在画布上绘制y轴，并设置标签和属性</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/d86b408d1d516bc9db8bc830872e4d2d.png" alt="img"></p><p>7.3.10.13在画布上绘制矩形满招每项数据占总量的比例来绘制高度，同时为每个矩形设置事件监听器，在鼠标进入和移出当前元素时触发</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/5fcec150790019a0697809ccc5cf9fb5.png" alt="img"></p><p>7.3.10.14定义用于计算数据总量的函数，script标签结束</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/329a97705b23ff0fe506ab260c6f0763.png" alt="img"></p><p>7.3.11绘制图形结束后，我们需要在head标签中的title标签下方添加style标签，在style标签中添加以下内容，用于美化图形</p><p>7.3.11.1style标签开始，设置坐标轴的颜色和边缘属性</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/2f732b242e37bcc83127031d3c933ea5.png" alt="img"></p><p>7.3.11.2设置矩形的填充颜色</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/56d1145569cb6fa3d4333bf4cb576859.png" alt="img"></p><p>7.3.11.3设置矩形被选中时的填充颜色</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/01f7a18b0a2194e476d7cdbaaff712ea.png" alt="img"></p><p>7.3.11.4设置Tip提示框大小、间距、颜色等属性</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/1abe4973f96b7a524e787c402b311db4.png" alt="img"></p><p>7.3.11.5在Tip提示框所在的圆角矩形下方添加一个小的三角形并设置它的大小和颜色</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/8a8783f77f7f713a365a43dbebab420a.png" alt="img"></p><p>7.3.11.6设置三角形的位置，style标签结束</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/88f28262bd585875bba374080d3d8f1d.png" alt="img"></p><p>7.3.12页面编辑完成之后利用:wq保存并退出，然后在浏览器中打开文件URL，查看可视化结果</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/25/a6e1f8d071590e09d277a19da19e7869.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;大数据行业应用-交通轨迹&quot;&gt;&lt;a href=&quot;#大数据行业应用-交通轨迹&quot; class=&quot;headerlink&quot; title=&quot;大数据行业应用-交通轨迹&quot;&gt;&lt;/a&gt;大数据行业应用-交通轨迹&lt;/h1&gt;&lt;h2 id=&quot;1、数据准备&quot;&gt;&lt;a href=&quot;#1、数据准备&quot;</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>配置Markdown的图床（Typora+PicGo+GitHub）</title>
    <link href="https://want595.github.io/2024/07/24/%E9%85%8D%E7%BD%AEMarkdown%E7%9A%84%E5%9B%BE%E5%BA%8A%EF%BC%88Typora-PicGo-GitHub%EF%BC%89/"/>
    <id>https://want595.github.io/2024/07/24/%E9%85%8D%E7%BD%AEMarkdown%E7%9A%84%E5%9B%BE%E5%BA%8A%EF%BC%88Typora-PicGo-GitHub%EF%BC%89/</id>
    <published>2024-07-24T07:38:03.000Z</published>
    <updated>2024-07-24T08:24:18.872Z</updated>
    
    <content type="html"><![CDATA[<h1 id="配置Markdown的图床（Typora-PicGo-GitHub）"><a href="#配置Markdown的图床（Typora-PicGo-GitHub）" class="headerlink" title="配置Markdown的图床（Typora+PicGo+GitHub）"></a>配置Markdown的图床（Typora+PicGo+GitHub）</h1><h2 id="零、安装软件"><a href="#零、安装软件" class="headerlink" title="零、安装软件"></a>零、安装软件</h2><ul><li>Typora</li><li>PicGo：<a href="https://github.com/Molunerfinn/PicGo/releases">https://github.com/Molunerfinn/PicGo/releases</a></li></ul><h2 id="一、配置GitHub"><a href="#一、配置GitHub" class="headerlink" title="一、配置GitHub"></a>一、配置GitHub</h2><p>声明：在写该文章前，我刚刚配置过一个<strong>MarkdownImage</strong>仓库，因为无法创建相同的仓库，所以这里用Image仓库代替。</p><ol><li>新建一个仓库，存储图片</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724154041086.png" alt="image-20240724154041086"></p><p>输入仓库名称，并设置为公开</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724154130497.png" alt="image-20240724154130497"></p><ol><li>创建密匙</li></ol><p>单击头像，选择Settings</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724154205204.png" alt="image-20240724154205204"></p><p>进入开发者设置</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724154231966.png" alt="image-20240724154231966"></p><p>创建一个新的密匙</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724154254099.png" alt="image-20240724154254099"></p><p>设置密匙名称，到期时间，并勾选repo，然后创建密匙</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724154643621.png" alt="image-20240724154643621"></p><p>复制密匙，记得将其保存（如果忘记保存密匙，后面需要重新创建）</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724154725703.png" alt="image-20240724154725703"></p><h2 id="二、配置PicGo"><a href="#二、配置PicGo" class="headerlink" title="二、配置PicGo"></a>二、配置PicGo</h2><p>声明：从现在开始，我的GitHub仓库就是MarkdownImage。</p><ol><li>安装插件</li></ol><ul><li>github-plus：用于GitHub和PicGo同步删除图片</li><li>rename-file：用于给图片重命名</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724160231571.png" alt="image-20240724160231571"></p><ol><li>配置githubPlus</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724160408997.png" alt="image-20240724160408997"></p><p>编辑GitHub信息</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724160444597.png" alt="image-20240724160444597"></p><ul><li>图床配置名随意</li><li>仓库名repo为<code>账户名/仓库名</code>，例如<code>Want595/Image</code></li><li>分支名branch，设为main即可</li><li>token就是刚才创建的密匙，复制过来即可</li><li>存储路径path为图片在该仓库的存放路径，可以不设置</li><li>自定义域名customUrl的格式：<code>https://cdn.jsdelivr.net/gh/用户名/仓库名@main</code></li></ul><p><strong>最后别忘了设为默认图床</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724160608755.png" alt="image-20240724160608755"></p><ol><li>配置rename-file</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724160656289.png" alt="image-20240724160656289"></p><p>命名规则：</p><ul><li>{y} 年，4位</li><li>{m} 月，2位</li><li>{d} 日期，2位</li><li>{h} 小时，2位</li><li>{i} 分钟，2位</li><li>{s} 秒，2位</li><li>{ms} 毫秒，3位(<strong>v1.0.4</strong>)</li><li>{timestamp} 时间戳(秒)，10位(<strong>v1.0.4</strong>)</li><li>{hash}，文件的md5值，32位</li><li>{origin}，文件原名（会去掉后缀）</li><li>{rand:<count>}, 随机数，<count>表示个数，默认为6个，示例：{rand：32}、{rand}</count></count></li><li>{localFolder:<count>}, <count>表示层级 ，默认为1，示例：{localFolder:6}、{localFolder}</count></count></li></ul><p>具体见：<a href="https://github.com/liuwave/picgo-plugin-rename-file#readme">https://github.com/liuwave/picgo-plugin-rename-file#readme</a></p><h2 id="三、配置Typora"><a href="#三、配置Typora" class="headerlink" title="三、配置Typora"></a>三、配置Typora</h2><ol><li>配置 偏好设置-&gt;图像</li></ol><ul><li>上传服务选PicGo(app)</li><li>PicGo路径为安装路径下的PicGo.exe文件</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724160914670.png" alt="image-20240724160914670"></p><ol><li>单击“验证图片上传选项“</li></ol><p>出现以下信息说明配置成功</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724161106103.png" alt="image-20240724161106103"></p><h2 id="四、上传图片"><a href="#四、上传图片" class="headerlink" title="四、上传图片"></a>四、上传图片</h2><p>在Typora中添加图片后，需要右击图片，选择”上传图片“，即可将图片上传至MarkdownImage图床中（上传后图片的路径会更新）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;配置Markdown的图床（Typora-PicGo-GitHub）&quot;&gt;&lt;a href=&quot;#配置Markdown的图床（Typora-PicGo-GitHub）&quot; class=&quot;headerlink&quot; title=&quot;配置Markdown的图床（Typora+Pic</summary>
      
    
    
    
    <category term="【笔记】Markdown" scheme="https://want595.github.io/categories/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91Markdown/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据行业应用-微博热点发现</title>
    <link href="https://want595.github.io/2024/07/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8-%E5%BE%AE%E5%8D%9A%E7%83%AD%E7%82%B9%E5%8F%91%E7%8E%B0/"/>
    <id>https://want595.github.io/2024/07/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8-%E5%BE%AE%E5%8D%9A%E7%83%AD%E7%82%B9%E5%8F%91%E7%8E%B0/</id>
    <published>2024-07-24T05:12:44.000Z</published>
    <updated>2024-07-25T02:11:03.541Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据行业应用-微博热点发现"><a href="#大数据行业应用-微博热点发现" class="headerlink" title="大数据行业应用-微博热点发现"></a>大数据行业应用-微博热点发现</h1><h2 id="1、实验描述"><a href="#1、实验描述" class="headerlink" title="1、实验描述"></a>1、实验描述</h2><p>社交网络在社会热点话题等方面起着重要作用。如何高效迅速的处理海量的微博数据、快速非人工的发现当前的热点话题具有一定的挑战。和博客长文本相比很大的差异之处，微博形式主要以短文本为主。微博的语义相对而言更不易总结，很难快速发现隐含的热点话题。热点发现的研究涉及到计算语言学、人工智能、统计学、信息科学等多种学科，是自然语言处理应用领域的研究热点之一。该项目针对微博热点话题词进行整理排序，方便用户查看当前热点词条信息。首先对微博数据集的话题词提取，通过分词、过滤、聚类等技术排序出热点话题。</p><ul><li>实验时长：120分钟</li><li>主要步骤：<ul><li>对微博数据获取与整合</li><li>对采集到的数据预处理</li><li>对数据进行中文分词</li><li>选取特征词进行文本建模</li><li>微博热点话题词抽取</li></ul></li></ul><h2 id="2、实验环境"><a href="#2、实验环境" class="headerlink" title="2、实验环境"></a>2、实验环境</h2><ul><li>anaconda 版本：anaconda3</li><li>python 版本：python 3.6</li><li>导入HanLP包、gensim包</li></ul><h2 id="3、相关技能"><a href="#3、相关技能" class="headerlink" title="3、相关技能"></a>3、相关技能</h2><ul><li>python3编程</li><li>自然语言处理基本方法</li></ul><h2 id="4、相关知识点"><a href="#4、相关知识点" class="headerlink" title="4、相关知识点"></a>4、相关知识点</h2><ul><li>获取到的微博数据采集是json数据结构</li><li>解析数据存储到Sql Server</li><li>对问句进行分词，并去除停用词（HanLP）</li><li>根据问题文本构建SVM向量空间模型</li><li>余弦相识度计算</li><li>FAQ匹配数据库构建</li><li>TF-IDF关键词提取</li><li>Kmeans算法对微博信息文本进行文本聚类</li></ul><h2 id="5、实验步骤"><a href="#5、实验步骤" class="headerlink" title="5、实验步骤"></a>5、实验步骤</h2><p>6.1 数据准备，将API和网络爬虫结合采集到的数据解析后存储到Sql Server，数据中的微博相关信息，进行降噪处理，去除干扰信息之后对微博的短文本数据进行分词处理。最后将处理完成的数据整理存入excel文件“new_weibotext.xlsx”中，截取的部分数据如图所示。此文件已存放在工作目录中。</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724092935140.png" alt="image-20240724092935140"></p><p>6.2 右击桌面空白处，单击”打开终端“，如图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724092955404.png" alt="image-20240724092955404"></p><p>6.3 在命令提示符下输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cd</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">jupyter-notebook --ip=0.0.0.0</span></span><br></pre></td></tr></table></figure><p>如图所示，回车，进入Jupyter-Notebook。</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724093043758.png" alt="image-20240724093043758"></p><p>6.4 在打开的Jupyter Notebook中，单击”数据与程序代码”目录，进入后点击对应的章节目录，如图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724093201102.png" alt="image-20240724093201102"></p><p>6.5 单击“新建”，选择“Python3”，新建Python3文件，如图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724093224348.png" alt="image-20240724093224348"></p><p>6.6 在默认的第一个Cell里导入所用第三方库，分别是数据分析包pandas，科学计算基础包numpy，中文分词库jieba，记录日志模块logging，快速文本分类算法fasttext，多进程管理包multiprocessing中的cpu_count函数，matplotlib绘图模块，kMeans聚类算法，以及数据存储包pickle，os模块，渲染制作图云的wordcloud模块，输入代码以下代码。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import jieba</span><br><span class="line">import re</span><br><span class="line">import logging</span><br><span class="line">import fasttext</span><br><span class="line">from multiprocessing import cpu_count</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">import pickle</span><br><span class="line">import os</span><br><span class="line">from wordcloud import WordCloud</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724093709679.png" alt="image-20240724093709679"></p><p>6.7 使用jieba.load_userdict(‘./data/uers_words.txt’)来载入自定义词典文档，如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jieba.load_userdict('./data/uers_words.txt')</span><br></pre></td></tr></table></figure><p>6.8 编译正则表达式模式，返回一个对象patten，如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">patten = re.compile('[\u4e00-\u9fa5]+')</span><br></pre></td></tr></table></figure><p>6.9 定义data_clean函数，将图2所示的“stop_words.txt”中的数据去掉空格添加到stopword中，读取excel表“new_weibotext1.xlsx”，将文件“Text”赋给f，建立字典word_list，随后打开文件“cut_line.text”，之后逐行遍历传给f的文档，进入外层判断语句，运行程序。代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def data_clean():</span><br><span class="line">    stopword = [word.strip for word in open('./data/stop_words.txt','r',encoding='utf-8')]</span><br><span class="line">    file = pd.read_excel('./data/new_weibotext1.xlsx', engine='openpyxl')</span><br><span class="line">   # print(file.head(10))</span><br><span class="line">    f = file['Text']</span><br><span class="line">    word_dict = {}</span><br><span class="line">    with open('./data/cut_line.txt','w',encoding='utf-8') as fl:</span><br><span class="line">        for line in f:</span><br><span class="line">            words = patten.findall(str(line).strip())</span><br><span class="line">            word_list = [word for word in jieba.lcut(''.join(words))]</span><br><span class="line">            if word_list:</span><br><span class="line">                for word in word_list:</span><br><span class="line">                    if word_dict.get(word,0):</span><br><span class="line">                        word_dict[word] += 1</span><br><span class="line">                    else:</span><br><span class="line">                        word_dict[word] = 1</span><br><span class="line">                fl.write(' '.join(word_list)+'\n')</span><br><span class="line"></span><br><span class="line">    sort_word_count = sorted(word_dict.items(),key=lambda x:x[-1],reverse=True)</span><br><span class="line">    print(sort_word_count[:10])</span><br><span class="line">data_clean()</span><br></pre></td></tr></table></figure><p>6.10 若运行结果正确，会以字典的形式输出出现次数最多的前10个汉字跟它的出现次数。</p><p>同时按下Ctrl+Enter键，该Cell中的程序运行结果如图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724104120730.png" alt="image-20240724104120730"></p><p>6.11 在第四个Cell中，定义Word2Vec类，在类里面定义train_w2v方法，方法里面使用fasttext.train_unsupervised函数，为了学习词向量(向量表示)，并且在学习开始时输出’Start training word vectors……’，保存好模型后print输出’Word vector train done!!!’。代码清单如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Word2Vec:</span><br><span class="line">    def train_w2v(self,file):</span><br><span class="line">        print('Start training word vectors......')</span><br><span class="line">        w2vModel = fasttext.train_unsupervised(file, model='skipgram', epoch=100, lr=0.01,</span><br><span class="line">                                               dim=100, ws=5, word_ngrams=2,</span><br><span class="line">                                               bucket=2000000)</span><br><span class="line">        w2vModel.save_model('./model/word2vec_model_100.bin')</span><br><span class="line">        print('Word vector train done!!!')</span><br></pre></td></tr></table></figure><p>6.12 定义load_model方法，内置加载模型函数，定义sentences_embedding私有方法，定义形状矩阵；代码清单如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def load_model(self,model_path):</span><br><span class="line">    w2v_model = fasttext.load_model(model_path)</span><br><span class="line">    return w2v_model</span><br><span class="line">def sentences_embedding(self,line,model):</span><br><span class="line">    vector = np.zeros(100)</span><br><span class="line">    #print(line)</span><br><span class="line">    for word in line.split(' '):</span><br><span class="line">        #print(word)</span><br><span class="line">        try:</span><br><span class="line">            vector += model[word]</span><br><span class="line">        except:</span><br><span class="line">            pass</span><br><span class="line">    vector = vector / len(line.split(' '))</span><br><span class="line">    return list(vector)</span><br></pre></td></tr></table></figure><p>6.13 定义file_embedding私有方法，循环逐一判断文件是否去除空格，并在循环判断语句中调用sentences_embedding方法。代码清单如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def file_embedding(self,file,model):</span><br><span class="line">    out_vector = []</span><br><span class="line">    for line in file:</span><br><span class="line">        #print(line)</span><br><span class="line">        if line.strip():</span><br><span class="line">            vector = self.sentences_embedding(line.strip(),model)</span><br><span class="line">            out_vector.append(vector)</span><br><span class="line">        else:</span><br><span class="line">            pass</span><br><span class="line">    return out_vector</span><br></pre></td></tr></table></figure><p>6.14 在第四个Cell中，定义Similar类，在类里定义sentencssim私有方法跟vectorCosine私有方法，方法里进行数学运算。代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Similar:</span><br><span class="line">    def sentencs_sim(self,text1,text2):</span><br><span class="line">        # vector1 = vector1 / len(word_list1)</span><br><span class="line">        # vector2 = vector2 / len(word_list2)</span><br><span class="line">        return self.vectorCosine(text1,text2)</span><br><span class="line">    def vectorCosine(self,vector1,vector2):</span><br><span class="line">        cos1 = np.sum(vector1 * vector2)</span><br><span class="line">        cos21 = np.sqrt(sum(vector1 ** 2))</span><br><span class="line">        cos22 = np.sqrt(sum(vector2 ** 2))</span><br><span class="line">        similarity = cos1 / float(cos21 * cos22)</span><br><span class="line">        return similarity</span><br></pre></td></tr></table></figure><p>6.15 在第五个Cell中，定义KMEANA类，在类里定义train私有方法，输入数据，用predict()方法进行预测、使用Kmeans算法的文本聚类。代码清单如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class KMEANA:</span><br><span class="line">    def train(self,data,n_clusters):</span><br><span class="line">        km = KMeans(n_clusters=n_clusters,n_init=n_clusters,tol=0.0001,init='k-means++')   # 初始化</span><br><span class="line">        km.fit(data)                   # 拟合</span><br><span class="line">        km_pred = km.predict(data)     # 预测</span><br><span class="line">        centers = km.cluster_centers_  # 质心</span><br></pre></td></tr></table></figure><p>6.16 将倒排索引永久存储在“./model/kmeans_model.pkl”文件中，程序代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with open('./model/kmeans_model.pkl','wb') as f:</span><br><span class="line">    pickle.dump(km,f)</span><br></pre></td></tr></table></figure><p>6.17 再用figure模块绘图，fig.add_subplot函数画子图，设置主题名称跟横纵坐标的名称。代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(111)</span><br><span class="line">ax1.set_title('Scatter Plot')</span><br><span class="line">plt.xlabel('X')</span><br><span class="line">plt.ylabel('Y')</span><br></pre></td></tr></table></figure><p>6.18 通过scatter函数定义散点图中点的大小，颜色，标记。最后以图片的格式显示绘图结果、保存图片。代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(np.array(data)[:, 1], np.array(data)[:, 0], c=km_pred,marker='.')</span><br><span class="line">plt.scatter(centers[:, 1], centers[:, 0], c="r",marker='p',linewidths=5)</span><br><span class="line">plt.show()</span><br><span class="line">fig.savefig('./data/plot.png')</span><br><span class="line">ax1.set_title('centers scatter Plot')</span><br></pre></td></tr></table></figure><p>6.19 用同样的方法绘制出第二张图片。程序代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plt.xlabel('X')</span><br><span class="line">plt.ylabel('Y')</span><br><span class="line">cValue = ['r', 'y', 'g', 'b', 'r', 'y', 'g', 'b', 'r', 'y', 'g', 'b','r', 'y', 'g', 'b','r', 'y', 'g', 'b']</span><br><span class="line">plt.scatter(centers[:, 1], centers[:, 0], c=cValue, marker='p', linewidths=5)</span><br><span class="line">plt.show()</span><br><span class="line">result = {}</span><br><span class="line">for text_idx, label_idx in enumerate(km_pred):</span><br><span class="line">    if label_idx not in result:</span><br><span class="line">        result[label_idx] = [text_idx]</span><br><span class="line">    else:</span><br><span class="line">        result[label_idx].append(text_idx)</span><br><span class="line">return result,list(centers)</span><br></pre></td></tr></table></figure><p>同时按下Ctrl+Enter键，该Cell中的程序运行结果如图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724105008276.png" alt="image-20240724105008276"></p><p>6.20 在下一个cell里，创建MODEL类，定义私有方法，用来判断文件是否存在。代码清单如下显示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class MODEL:</span><br><span class="line">    def __init__(self,path,model_path):</span><br><span class="line">        self.file = self.read_file(path)</span><br><span class="line">        # print(list(self.file))</span><br><span class="line">        if not os.path.exists('./model/word2vec_model_100.bin'):</span><br><span class="line">            w2v.train_w2v(path)</span><br><span class="line">        self.w2v_model = w2v.load_model(model_path)</span><br></pre></td></tr></table></figure><p>6.21 以只读方式打开文件并去除文档中的空格。代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def read_file(self,path):</span><br><span class="line">    file = open(path,'r',encoding='utf-8')</span><br><span class="line">    return [line.strip() for line in file if line.strip()]</span><br><span class="line">def all_line(self):</span><br><span class="line">    return [line.strip() for line in self.file if line.strip()]</span><br></pre></td></tr></table></figure><p>6.22 定义形状矩阵，统计已经训练过的文章后的余弦相似度。统计每个类别下，和聚类中心最近的topk个文本id集合，对列表进行排序。代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def sim_one_centers(self,ponits,center_vec):</span><br><span class="line">    '''</span><br><span class="line">    统计每个类别下，和聚类中心距离最近的topk个文本id集合</span><br><span class="line">    :param ponits: 给类别下的所有的文本id</span><br><span class="line">    :param center_vec: 聚类中心向量</span><br><span class="line">    :return: 与聚类中心距离最近的topk个文本id</span><br><span class="line">    '''</span><br><span class="line">    sentence_sim_dict = {}</span><br><span class="line">    print('该类别的集合：{}'.format(ponits))</span><br><span class="line">    # print(type(center_vec))</span><br><span class="line">    for i in ponits:</span><br><span class="line">        line = self.file[i]</span><br><span class="line">        line_vec = w2v.sentences_embedding(line,self.w2v_model)</span><br><span class="line"></span><br><span class="line">        # print(np.array(line_vec).shape)</span><br><span class="line">        # print(center_vec.shape)</span><br><span class="line">        cos = sim.vectorCosine(center_vec,np.array(line_vec))</span><br><span class="line">        sentence_sim_dict[i] = cos</span><br><span class="line"></span><br><span class="line">    top_line = sorted(sentence_sim_dict.items(),key=lambda x:x[1])[:int(len(ponits)*0.5)]</span><br><span class="line">   # print('top_line:{},数量为：{}'.format(top_line,len(top_line)))</span><br><span class="line">    # print(lines[top_line[0][0]])</span><br><span class="line">    return [item[0] for item in top_line]</span><br></pre></td></tr></table></figure><p>6.23 统计几个聚类中心的距离，返回频次最高的topk个中心。代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def center_sim(self):</span><br><span class="line">    '''</span><br><span class="line">    统计几个聚类中心的之间的距离，返回频次最高的topk个中心</span><br><span class="line">    :return: 返回频次最高的topk个中心</span><br><span class="line">    '''</span><br><span class="line">    i= 0</span><br><span class="line">    sim_dict = {}</span><br><span class="line">    for vec1 in self.centers:</span><br><span class="line">        i += 1</span><br><span class="line">        if i &lt;= len(self.centers):</span><br><span class="line">            j = i</span><br><span class="line">            for vec2 in self.centers[i:]:</span><br><span class="line">                cos = sim.vectorCosine(vec1,vec2)</span><br><span class="line">                # print(cos)</span><br><span class="line">                sim_dict[cos] = (i-1,j)</span><br><span class="line">                j += 1</span><br><span class="line">    sort_sim = sorted(sim_dict.items(),key=lambda x:x[0],reverse=True)</span><br><span class="line">    #print(sort_sim[:10])</span><br><span class="line">    num_dict = {}</span><br><span class="line">    for item in sort_sim[:10]:</span><br><span class="line">        center_nums = item[-1]</span><br><span class="line">        for i in center_nums:</span><br><span class="line">            # print(i)</span><br><span class="line">            if i in num_dict.keys():</span><br><span class="line">                num_dict[i] += 1</span><br><span class="line">            else:</span><br><span class="line">                num_dict[i] = 1</span><br><span class="line">    top_center = sorted(num_dict.items(),key=lambda x:x[-1],reverse=True)[:2]</span><br><span class="line">    #print('top_center:{}'.format(top_center))</span><br><span class="line">    return [num[0] for num in top_center]</span><br></pre></td></tr></table></figure><p>6.24 把带权重的关键词渲染成词云，显示颜色，大小等，generate函数循环输出该类别的集合。代码清单如下显示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def get_key_word(self,top_line_dict):</span><br><span class="line">    #print(top_line_dict.values())</span><br><span class="line">    for lines in top_line_dict.values():</span><br><span class="line">        line_list = []</span><br><span class="line">        for id in lines:</span><br><span class="line">            line_list.append(self.file[id])</span><br><span class="line">        # print(line_list)</span><br><span class="line">        self.get_cloud_word(line_list)</span><br><span class="line">def get_cloud_word(self,word_list):</span><br><span class="line">    my_wordcloud = WordCloud(font_path='./data/simkai.ttf').generate(' '.join(word_list))</span><br><span class="line">    plt.imshow(my_wordcloud)</span><br><span class="line">    plt.axis("off")</span><br><span class="line">    plt.show()</span><br><span class="line">def main(self,n_clusters):</span><br><span class="line">    train_data = w2v.file_embedding(self.file,self.w2v_model)</span><br><span class="line">    self.result,self.centers = Kmeans.train(train_data,n_clusters=n_clusters)</span><br><span class="line">    top_center_index = self.center_sim()</span><br><span class="line">   # print('结果：{}'.format(self.result))</span><br><span class="line">    top_line_dict = {}</span><br><span class="line">    for i in top_center_index:</span><br><span class="line">        center_id = list(self.result.keys())[i]</span><br><span class="line">        # print(center_id)</span><br><span class="line">        line_ids = self.result.get(center_id)</span><br><span class="line">        top_line_id = self.sim_one_centers(line_ids,self.centers[i])</span><br><span class="line">        top_line_dict[i] = top_line_id</span><br><span class="line">    self.get_key_word(top_line_dict)</span><br></pre></td></tr></table></figure><h2 id="6、运行效果"><a href="#6、运行效果" class="headerlink" title="6、运行效果"></a>6、运行效果</h2><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724105215410.png" alt="image-20240724105215410"></p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240724105238226.png" alt="image-20240724105238226"></p><h2 id="7、总结"><a href="#7、总结" class="headerlink" title="7、总结"></a>7、总结</h2><p>完成本实验可掌握自然语言处理的基本方法，并使用python编程实现关于微博中社会热点话题的发现。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;大数据行业应用-微博热点发现&quot;&gt;&lt;a href=&quot;#大数据行业应用-微博热点发现&quot; class=&quot;headerlink&quot; title=&quot;大数据行业应用-微博热点发现&quot;&gt;&lt;/a&gt;大数据行业应用-微博热点发现&lt;/h1&gt;&lt;h2 id=&quot;1、实验描述&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    <category term="项目实训" scheme="https://want595.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%AE%AD/"/>
    
    
  </entry>
  
  <entry>
    <title>本地PyCharm连接虚拟机Centos9</title>
    <link href="https://want595.github.io/2024/07/23/%E6%9C%AC%E5%9C%B0PyCharm%E8%BF%9E%E6%8E%A5%E8%99%9A%E6%8B%9F%E6%9C%BACentos9/"/>
    <id>https://want595.github.io/2024/07/23/%E6%9C%AC%E5%9C%B0PyCharm%E8%BF%9E%E6%8E%A5%E8%99%9A%E6%8B%9F%E6%9C%BACentos9/</id>
    <published>2024-07-23T03:02:24.000Z</published>
    <updated>2024-07-24T07:26:46.210Z</updated>
    
    <content type="html"><![CDATA[<h1 id="本地PyCharm连接虚拟机（Centos9）"><a href="#本地PyCharm连接虚拟机（Centos9）" class="headerlink" title="本地PyCharm连接虚拟机（Centos9）"></a>本地PyCharm连接虚拟机（Centos9）</h1><p>注意：创建项目后再连接，更换项目后需要重新连接</p><h2 id="零、资源准备"><a href="#零、资源准备" class="headerlink" title="零、资源准备"></a>零、资源准备</h2><ul><li><strong>PyCharm专业版</strong></li><li>虚拟机（Centos9）</li></ul><h2 id="一、远程连接虚拟机"><a href="#一、远程连接虚拟机" class="headerlink" title="一、远程连接虚拟机"></a>一、远程连接虚拟机</h2><ol><li>打开PyCharm（专业版）的Configuration</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105156843.png" alt="image-20240723105156843"></p><ol><li>单击＋号，选择SFTP，输入虚拟机的ip</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105233539.png" alt="image-20240723105233539"></p><p>注：虚拟机的ip可以用ifconfig获取</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723102657235.png" alt="image-20240723102657235"></p><ol><li>单击SSH configuration的“…”，配置虚拟机信息并测试连接是否成功（确保虚拟机启动）</li></ol><ul><li>输入虚拟机的ip</li><li>输入虚拟机的用户名</li><li>单击Test Connection测试</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105300963.png" alt="image-20240723105300963"></p><ol><li>单击Mappings，配置基本信息</li></ol><ul><li>Local path表示本地项目</li><li>Deployment path表示运行.py文件后，在虚拟机创建的文件路径</li><li>Web path可以不配置</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105321299.png" alt="image-20240723105321299"></p><ol><li>配置完成后，可以单击Browse Remote Host查看虚拟机</li></ol><ul><li>单击Browse Remote Host</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105440480.png" alt="image-20240723105440480"></p><ul><li>查看虚拟机</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105451975.png" alt="image-20240723105451975"></p><h2 id="二、运行Python脚本"><a href="#二、运行Python脚本" class="headerlink" title="二、运行Python脚本"></a>二、运行Python脚本</h2><ol><li>单击File-&gt;settings，打开设置，选择Python Interpreter，单击Add Interpreter，选择On SSH</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105530745.png" alt="image-20240723105530745"></p><ol><li>配置虚拟机</li></ol><ul><li>输入虚拟机账号</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105613837.png" alt="image-20240723105613837"></p><ul><li>输入密码</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105625091.png" alt="image-20240723105625091"></p><ul><li>等待配置</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105631097.png" alt="image-20240723105631097"></p><ul><li>配置解释器（路径可自己设置，Sync folders表示虚拟机的文件地址）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105715918.png" alt="image-20240723105715918"></p><ul><li>配置成功</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105751115.png" alt="image-20240723105751115"></p><ol><li>测试脚本</li></ol><p>创建main.py脚本并运行</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240723105851172.png" alt="image-20240723105851172"></p><ul><li>/usr/bin/python3：python位置</li><li>/opt/data/python/main.py：python文件位置</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;本地PyCharm连接虚拟机（Centos9）&quot;&gt;&lt;a href=&quot;#本地PyCharm连接虚拟机（Centos9）&quot; class=&quot;headerlink&quot; title=&quot;本地PyCharm连接虚拟机（Centos9）&quot;&gt;&lt;/a&gt;本地PyCharm连接虚拟机（Ce</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>完全分布式高可用集群（Flink）</title>
    <link href="https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Flink%EF%BC%89/"/>
    <id>https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Flink%EF%BC%89/</id>
    <published>2024-07-22T07:10:14.000Z</published>
    <updated>2024-07-24T07:08:17.544Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h1><h2 id="零、前提条件"><a href="#零、前提条件" class="headerlink" title="零、前提条件"></a>零、前提条件</h2><p>Flink集群规划</p><div class="table-container"><table><thead><tr><th>hadoop1</th><th>hadoop2</th><th>hadoop3</th></tr></thead><tbody><tr><td>JobManager</td><td>TaskManager</td><td>TaskManager</td></tr></tbody></table></div><ol><li>上传安装包（在hadoop1操作）</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722130840508.png" alt="image-20240722130840508"></p><ol><li>解压安装包</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /software/flink-1.19.1-bin-scala_2.12.tgz -C /opt</span><br><span class="line">ln -s /opt/flink-1.19.1 /opt/flink</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722131007541.png" alt="image-20240722131007541"></p><ol><li>配置环境变量，添加如下内容</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export FLINK_HOME=/opt/flink</span><br><span class="line">export PATH=$PATH:$FLINK_HOME/bin</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722131055667.png" alt="image-20240722131055667"></p><p>让环境变量生效</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>验证版本号</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink -v</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722131123536.png" alt="image-20240722131123536"></p><p>看到如上版本号字样，说明环境变量配置成功。</p><h2 id="一、配置Flink"><a href="#一、配置Flink" class="headerlink" title="一、配置Flink"></a>一、配置Flink</h2><p>进入flink配置目录<code>/opt/flink/conf</code>，查看配置文件</p><ol><li>配置<code>flink-conf.yaml</code></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim flink-conf.yaml</span><br></pre></td></tr></table></figure><p>找到相关配置项并修改，如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: hadoop1</span><br><span class="line">jobmanager.bind-host: 0.0.0.0</span><br><span class="line">taskmanager.bind-host: 0.0.0.0</span><br><span class="line">taskmanager.host: hadoop1</span><br><span class="line">rest.address: hadoop1</span><br><span class="line">rest.port: 8082</span><br><span class="line">rest.bind-address: 0.0.0.0</span><br></pre></td></tr></table></figure><p>（下图以rest为例）</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722142152267.png" alt="image-20240722142152267"></p><ol><li>配置workers</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim workers</span><br></pre></td></tr></table></figure><p>把原有内容删除，添加内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722131938038.png" alt="image-20240722131938038"></p><ol><li>配置masters</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim masters </span><br></pre></td></tr></table></figure><p>修改后内容如下：（由于8081端口被Spark占用了，改成8082）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop1:8082</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722132109968.png" alt="image-20240722132109968"></p><ol><li>分发flink安装目录</li></ol><p>确保hadoop2、hadoop3机器已开启的情况下，执行如下分发命令（注意分发的是flink-1.19.1，不是flink）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/flink-1.19.1 root@hadoop2:/opt/</span><br><span class="line">scp /etc/profile root@hadoop2:/etc/</span><br><span class="line">scp -r /opt/flink-1.19.1 root@hadoop3:/opt/</span><br><span class="line">scp /etc/profile root@hadoop3:/etc/</span><br></pre></td></tr></table></figure><p>让环境变量生效</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop2 "source /etc/profile"</span><br><span class="line">ssh hadoop3 "source /etc/profile"</span><br></pre></td></tr></table></figure><ol><li>修改hadoop2和hadoop3的配置</li></ol><p>进入hadoop2机器flink的配置目录<code>/opt/flink-1.19.1/conf</code>，配置<code>config.yaml</code>文件</p><p>将taskmanager.host的值修改为hadoop2</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskmanager.host: hadoop2</span><br></pre></td></tr></table></figure><p>hadoop3机器同理修改为hadoop3</p><ol><li>启动flink集群</li></ol><p>1）在hadoop1机器，执行如下命令启动集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-cluster.sh</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722140517180.png" alt="image-20240722140517180"></p><p>2）查看进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/xshell/xjps.sh</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722142748823.png" alt="image-20240722142748823"></p><p>hadoop1有StandaloneSessionClusterEntrypoint进程</p><p>hadoop2有TaskManagerRunner进程</p><p>hadoop3有TaskManagerRunner进程</p><p>看到如上进程，说明flink集群配置成功。</p><p>网页查看（<a href="http://hadoop1:8082）">http://hadoop1:8082）</a></p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722142824463.png" alt="image-20240722142824463"></p><p>3）关闭flink集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-cluster.sh </span><br></pre></td></tr></table></figure><h2 id="二、测试Flink"><a href="#二、测试Flink" class="headerlink" title="二、测试Flink"></a>二、测试Flink</h2><ol><li>运行flink提供的WordCount案例程序</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink run /opt/flink/examples/streaming/WordCount.jar</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240722143245860.png" alt="image-20240722143245860"></p><ol><li><p>查看输出的WordCount结果的末尾10行数据</p><p>（运行时需要超过<strong>100G</strong>的内存，本人的内存不足，无法运行出结果）</p></li><li><p>Web UI查看运行结果</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Flink&quot;&gt;&lt;a href=&quot;#Flink&quot; class=&quot;headerlink&quot; title=&quot;Flink&quot;&gt;&lt;/a&gt;Flink&lt;/h1&gt;&lt;h2 id=&quot;零、前提条件&quot;&gt;&lt;a href=&quot;#零、前提条件&quot; class=&quot;headerlink&quot; title=&quot;零</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>完全分布式高可用集群（Scala+Spark）</title>
    <link href="https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Scala-Spark%EF%BC%89/"/>
    <id>https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Scala-Spark%EF%BC%89/</id>
    <published>2024-07-22T07:09:39.000Z</published>
    <updated>2024-07-23T01:19:09.095Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><h2 id="一、前置准备（Scala）"><a href="#一、前置准备（Scala）" class="headerlink" title="一、前置准备（Scala）"></a>一、前置准备（Scala）</h2><p>以下步骤都在hadoop1机器执行，然后复制到hadoop2和hadoop3机器中</p><ol><li><p>上传scala包</p><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722102358308.png" alt="image-20240722102358308"></p></li><li><p>解压scala包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf /software/scala-2.13.14.tgz -C /opt</span><br><span class="line">ln -s /opt/scala-2.13.14 /opt/scala</span><br></pre></td></tr></table></figure></li></ol><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722102504582.png" alt="image-20240722102504582"></p><ol><li>配置<code>/etc/profile</code>的scala环境变量，重新加载配置文件，运行scala（记得source让环境变量生效）</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SCALA_HOME=/opt/scala</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722102612842.png" alt="image-20240722102612842"></p><ol><li>检查是否安装成功</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722102655402.png" alt="image-20240722102655402"></p><ol><li>分发到hadoop2和hadoop3</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/scala root@hadoop2:/opt/</span><br><span class="line">scp /etc/profile root@hadoop2:/etc/</span><br><span class="line">scp -r /opt/scala root@hadoop3:/opt/</span><br><span class="line">scp /etc/profile root@hadoop3:/etc/</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722120705329.png" alt="image-20240722120705329"></p><h2 id="二、完全分布式部署（Spark）"><a href="#二、完全分布式部署（Spark）" class="headerlink" title="二、完全分布式部署（Spark）"></a>二、完全分布式部署（Spark）</h2><ol><li>Spark部署方式</li></ol><ul><li><p>Local模式：单机模式</p></li><li><p>集群模式：</p></li><li><p>Standalone模式：使用Spark自带的简单集群管理器</p></li><li><p>YARN模式：使用YARN作为集群管理器</p></li><li><p>Mesos模式：使用Mesos作为集群管理器</p></li><li><p>Kubernetes模式：实验阶段</p></li></ul><ol><li>安装spark</li></ol><p>1）Spark官网的Download界面<a href="https://spark.apache.org/downloads.html可选版本较少。">https://spark.apache.org/downloads.html可选版本较少。</a></p><p>2）可以在下载页面的下方进入它的release archives：<a href="https://archive.apache.org/dist/spark/">https://archive.apache.org/dist/spark/</a> 选择想要的版本。</p><p>由于我们之前配置过Hadoop了，所以选择without-hadoop的版本</p><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722103729587.png" alt="image-20240722103729587"></p><ol><li>安装Spark</li></ol><p>约定虚拟机主机名为hadoop1，用户名为root, 安装路径为/opt，如果不是，请自行修改相应配置文件。</p><p>1）上传安装包</p><p>将下载好的安装包上传至虚拟机（当然你也可以直接在虚拟机中下载，省得上传文件）</p><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722104232927.png" alt="image-20240722104232927"></p><p>2）解压并创建软链接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf /software/spark-3.5.1-bin-without-hadoop.tgz -C /opt</span><br><span class="line">ln -s /opt/spark-3.5.1-bin-without-hadoop  /opt/spark</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722104350802.png" alt="image-20240722104350802"></p><p>3）查看Spark的目录结构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /opt/spark</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722104427100.png" alt="image-20240722104427100"></p><ul><li><p>bin: 可执行脚本，比如常用的spark-shell, pyspark等。</p></li><li><p>data：示例程序使用数据</p></li><li><p>jars：依赖的jar包</p></li><li><p>R：R API包</p></li><li><p>yarn：整合yarn相关内容</p></li><li><p>conf：配置文件目录</p></li><li><p>examples：示例程序</p></li><li><p>kubernetes：K8S相关内容</p></li><li><p>licenses：许可文件</p></li><li><p>python：python API包</p></li><li><p>sbin：管理使用的脚本，比如: start-all.sh、start-master.sh等。</p></li></ul><p>4）配置环境变量（/etc/profile）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/opt/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722104644135.png" alt="image-20240722104644135"></p><p>运行下面的命令使配置生效</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>5）配置Hadoop信息</p><p>因为我们下载的是“without-hadoop” 版本，所以要配置SPARK_DIST_CLASSPATH 变量才能正常运行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/spark/conf</span><br><span class="line">cp spark-env.sh.template  spark-env.sh</span><br><span class="line">vi spark-env.sh</span><br></pre></td></tr></table></figure><p>添加配置如下信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_DIST_CLASSPATH=$(/opt/hadoop/bin/hadoop classpath)</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722104849976.png" alt="image-20240722104849976"></p><p>6）测试安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722105824151.png" alt="image-20240722105824151"></p><p>运行代码进行测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var r = sc.parallelize(Array(1,2,3,4))</span><br><span class="line">r.map(_*10).collect()</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722110106854.png" alt="image-20240722110106854"></p><h3 id="安装Anaconda（可选）"><a href="#安装Anaconda（可选）" class="headerlink" title="安装Anaconda（可选）"></a>安装Anaconda（可选）</h3><ol><li>安装python（centos9自带python3.9.18）</li></ol><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722112459698.png" alt="image-20240722112459698"></p><ol><li>安装Anaconda</li></ol><p>1）查看python3.9对应的Anaconda版本</p><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722114603637.png" alt="image-20240722114603637"></p><p>2）下载</p><p>可以在网页<a href="https://repo.anaconda.com/archive/（或网址https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/）上选择相应版本进行下载，这里将下载[Anaconda3-2022.10-Linux-x86_64.sh](https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh),其python对应版本为3.9。">https://repo.anaconda.com/archive/（或网址https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/）上选择相应版本进行下载，这里将下载[Anaconda3-2022.10-Linux-x86_64.sh](https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh),其python对应版本为3.9。</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /software</span><br><span class="line">wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722115039721.png" alt="image-20240722115039721"></p><p>3）执行安装命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-2022.10-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>然后一路回车，等出现接受许可界面，输入yes接收许可。</p><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722115227946.png" alt="image-20240722115227946"></p><p>接着可以配置anacoda的安装目录，这里配置的安装目录是：<code>/opt/anaconda3</code></p><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722115502263.png" alt="image-20240722115502263"></p><p>然后配置是否自动配置PATH路径，这里输入的yes，让其自动配置</p><p>最后提示是否安装VSCode，这里输入的no，拒绝安装。</p><h3 id="伪分布式"><a href="#伪分布式" class="headerlink" title="伪分布式"></a>伪分布式</h3><ol><li>配置Spark伪Standalone模式</li></ol><p>这里配置Standalone模式，为了方便初学者，这里只配置一台从节点，所以将其称为”伪Standalone模式”</p><p>1）打开配置文件目录<code>/opt/spark/conf/spark-env.sh</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /opt/spark/conf/spark-env.sh</span><br></pre></td></tr></table></figure><p>添加如下配置信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_MASTER_HOST=hadoop1           #设置主节点地址</span><br><span class="line">export SPARK_MASTER_WEBUI_PORT=8081   #设置网页端口</span><br><span class="line">export SPARK_WORKER_MEMORY=2g              #设置节点内存大小，此处为4G。</span><br><span class="line">export SPARK_WORKER_CORES=2                #设置节点参与计算的核心数</span><br><span class="line">export SPARK_WORKER_INSTANCES=1            #设置节点实例数</span><br><span class="line">export JAVA_HOME=/opt/jdk</span><br><span class="line">export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop</span><br><span class="line">SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://hadoop1:9000/sparklog/ -Dspark.history.fs.cleaner.enabled=true/"</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722125142795.png" alt="image-20240722125142795"></p><p>2）配置节点信息，打开<code>/opt/spark/conf</code>目录，复制<code>workers.template</code>文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /opt/spark/conf/workers.template /opt/spark/conf/workers</span><br></pre></td></tr></table></figure><p>打开文件<code>vim /opt/spark/conf/workers</code>，删除localhost，填入以下信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Scala-Spark）/image-20240722113442397.png" alt="image-20240722113442397"></p><ol><li><p>分发到hadoop2和hadoop3中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/spark root@hadoop2:/opt/</span><br><span class="line">scp /etc/profile root@hadoop2:/etc/</span><br><span class="line">scp -r /opt/spark root@hadoop3:/opt/</span><br><span class="line">scp /etc/profile root@hadoop3:/etc/</span><br></pre></td></tr></table></figure></li><li><p>测试standalone模式（需要先启动hadoop）</p></li></ol><p>1）启动命令</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/opt/spark/sbin/start-master.sh</span><br><span class="line">/opt/spark/sbin/start-workers.sh</span><br></pre></td></tr></table></figure><p>或</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/spark/sbin/start-all.sh</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（Scala-Spark）/image-20240722122752870.png" alt="image-20240722122752870"></p><p>2）查看状态</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/xshell/xjps.sh</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（Scala-Spark）/image-20240722122904308.png" alt="image-20240722122904308"></p><p>Web网页（<a href="http://hadoop1:8081）">http://hadoop1:8081）</a></p><p>  <img src="完全分布式高可用集群（Scala-Spark）/image-20240722124835873.png" alt="image-20240722124835873"></p><p>3）终止命令</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/spark/sbin/stop-all.sh</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Spark&quot;&gt;&lt;a href=&quot;#Spark&quot; class=&quot;headerlink&quot; title=&quot;Spark&quot;&gt;&lt;/a&gt;Spark&lt;/h1&gt;&lt;h2 id=&quot;一、前置准备（Scala）&quot;&gt;&lt;a href=&quot;#一、前置准备（Scala）&quot; class=&quot;header</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>完全分布式高可用集群（Flume）</title>
    <link href="https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Flume%EF%BC%89/"/>
    <id>https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Flume%EF%BC%89/</id>
    <published>2024-07-22T07:06:39.000Z</published>
    <updated>2024-07-24T07:09:31.321Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h2 id="零、资源准备"><a href="#零、资源准备" class="headerlink" title="零、资源准备"></a>零、资源准备</h2><ul><li>apache-flume-1.11.0-bin.tar.gz</li></ul><h2 id="一、搭建完全分布式集群（Flume）"><a href="#一、搭建完全分布式集群（Flume）" class="headerlink" title="一、搭建完全分布式集群（Flume）"></a>一、搭建完全分布式集群（Flume）</h2><p>注意，本文只需要在<strong>hadoop1机器</strong>进行操作。</p><ol><li>Flume安装</li></ol><p>1.1 上传压缩包</p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240721202647183.png" alt="image-20240721202647183"></p><p>1.2 解压到安装目录下并设置软链接（或重命名）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /software/apache-flume-1.11.0-bin.tar.gz -C /opt/</span><br><span class="line">ln -s /opt/apache-flume-1.11.0-bin /opt/flume</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240721203354245.png" alt="image-20240721203354245"></p><ol><li>Flume配置（以hadoop1为例）</li></ol><p>2.1 复制一个<code>/opt/flume/conf/flume-env.sh</code>文件</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp /opt/flume/conf/flume-env.sh.template /opt/flume/conf/flume-env.sh</span><br><span class="line">vim /opt/flume/conf/flume-env.sh</span><br></pre></td></tr></table></figure><p>2.2 修改<code>/opt/flume/conf/flume-env.sh</code>文件</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/jdk</span><br></pre></td></tr></table></figure><p>  <img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240721204140824.png" alt="image-20240721204140824"></p><p>2.3 配置环境变量<code>/etc/profile</code>，增加以下内容（记得source）</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export FLUME_HOME=/opt/flume</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br></pre></td></tr></table></figure><p>  <img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240721204600928.png" alt="image-20240721204600928"></p><p>2.4 检查是否安装成功</p><p>输入命令<code>flume-ng version</code></p><p>  <img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240721204650938.png" alt="image-20240721204650938"></p><h2 id="二、测试Flume集群"><a href="#二、测试Flume集群" class="headerlink" title="二、测试Flume集群"></a>二、测试Flume集群</h2><p>Flume 除了可以监控单个日志文件外，还能监控整个目录，比如我们希望监控某个目录中出现的新文件。</p><ol><li>在<code>/opt/data/</code>目录下创建一个files文件夹。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/data</span><br><span class="line">mkdir files</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240721210016363.png" alt="image-20240721210016363"></p><ol><li>在<code>/opt/flume/conf/</code>目录下创建一个配置文件 <code>flume-hdfs.conf</code>，填入以下内容。 </li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"># 指定 Source 的类型是 spooldir</span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line"># 指定监控的目录，会处理已存在和新增的文件</span><br><span class="line">a1.sources.r1.spoolDir = /opt/data/files</span><br><span class="line"># Flume 会读取文件的内容，转成独立的 Flume 事件，然后发送到 Channel</span><br><span class="line"># 一旦文件读取完成，Flume 会自动给文件增加一个 .COMPLETED 后缀，以防止重复读取</span><br><span class="line">a1.sources.r1.fileSuffix = .COMPLETED</span><br><span class="line"># fileHeader 设置为 true 时，Flume 会在每个事件中添加一个头部信息</span><br><span class="line"># 指明该事件来源于哪个文件，这对于跟踪事件来源很有帮助</span><br><span class="line">a1.sources.r1.fileHeader = true</span><br><span class="line"># 指定一个正则表达式，表示忽略哪些文件，这里会忽略掉所有以 .tmp 结尾的文件</span><br><span class="line">a1.sources.r1.ignorePattern = ([^ ]*\.tmp)</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = hdfs://hadoop1:9000/user/hadoop/output</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 10485760</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = event</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240721205431487.png" alt="image-20240721205431487"></p><ol><li>启动Flume（确保已启动Hadoop）</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent -c $FLUME_HOME/conf -n a1 -f $FLUME_HOME/conf/flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240721205743811.png" alt="image-20240721205743811"></p><ol><li>然后我们往 files 目录中创建几个新文件。 </li></ol><p>Flume 会每隔 500 毫秒扫描一次监控目录的文件变动，然后检测到 files 目录中新增了 a.txt 并进行读取。一旦读取完毕，会自动给文件增加一个 .COMPLETED 后缀，表示读取完毕。而 b.tmp 则没有读取，因为过滤掉了以 .tmp 结尾的文件。 注意：监控目录和监控单个文件有所不同，如果是监控单个文件，那么可以不停地追加内容。但如果是监控目录，那么应该向目录中上传已经包含完整内容的文件，而不要在监控的目录中对文件进行修改。 </p><p><img src="https://cdn.jsdelivr.net/gh/Want595/MarkdownImage@main/image/2024/07/24/image-20240721212346410.png" alt="image-20240721212346410"></p><ol><li>查看HDFS（<a href="http://192.168.121.160:9870/explorer.html#/user/hadoop/output）的内容，显然文件已成功上传。">http://192.168.121.160:9870/explorer.html#/user/hadoop/output）的内容，显然文件已成功上传。</a> </li></ol><p><img src="Flume/image-20240721212308535.png" alt="image-20240721212308535"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Flume&quot;&gt;&lt;a href=&quot;#Flume&quot; class=&quot;headerlink&quot; title=&quot;Flume&quot;&gt;&lt;/a&gt;Flume&lt;/h1&gt;&lt;h2 id=&quot;零、资源准备&quot;&gt;&lt;a href=&quot;#零、资源准备&quot; class=&quot;headerlink&quot; title=&quot;零</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>完全分布式高可用集群（HBase）</title>
    <link href="https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88HBase%EF%BC%89/"/>
    <id>https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88HBase%EF%BC%89/</id>
    <published>2024-07-22T07:05:13.000Z</published>
    <updated>2024-07-23T01:14:13.920Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><p>Apache HBase是一个分布式、可扩展、大数据存储的Hadoop数据库。当我们需要对大数据进行随机、实时的读/写访问时，可以使用HBase。这个项目的目标是在通用硬件集群上托管非常大的表——数十亿行X数百万列。Apache HBase是一个开源、分布式、版本化的非关系数据库，模仿了Chang等人的谷歌Bigtable:A distributed Storage System for Structured Data。正如Bigtable利用谷歌文件系统提供的分布式数据存储一样，Apache HBase在Hadoop和HDFS之上提供了类似Bigtable的功能。</p><p>本文部署HBase集群的时候，需要依赖于Hadoop集群和Zookeeper集群。</p><h2 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h2><p>HBase版本</p><ul><li>hbase-2.5.8-bin.tar.gz</li></ul><p>安装准备</p><ol><li><p>Hadoop安装</p><p>参考： 搭建Hadoop3.x完全分布式集群（CentOS 9）</p></li><li><p>Zookeeper安装</p><p>参考：搭建Zookeeper完全分布式集群（CentOS 9）</p></li><li><p>时钟同步</p><p>在HBase集群中，各个节点之间的时间同步非常重要，如果各个节点的时间不一致，那么会出现写入数据的时间戳不一致或某些操作的顺序发生错误等问题，从而影响HBase集群的稳定性和正确性。因此，在部署HBase之前，需要为集群的各节点配置时间同步。</p></li></ol><p>1）安装Chrony</p><p>在虚拟机hadoop1上运行如下命令安装时间同步工具Chrony。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install chrony –y</span><br><span class="line">ssh hadoop2 "yum install chrony -y"</span><br><span class="line">ssh hadoop3 "yum install chrony -y"</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（HBase）/image-20240722091354118.png" alt="image-20240722091354118"></p><p>2）启动Chrony服务</p><p>在虚拟机hadoop1上运行如下命令启动时间同步工具Chrony的服务。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl start chronyd</span><br><span class="line">ssh hadoop2 "systemctl start chronyd"</span><br><span class="line">ssh hadoop3 "systemctl start chronyd"</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（HBase）/image-20240722091428233.png" alt="image-20240722091428233"></p><p>3）查看Chrony服务运行状态</p><p>在虚拟机hadoop1、 hadoop2和hadoop3查看Chrony服务的运行状态。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status chronyd</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（HBase）/image-20240722091444387.png" alt="image-20240722091444387"></p><p>4）配置Chrony服务端</p><p>在虚拟机hadoop1执行<code>vi /etc/chrony.conf</code>命令编辑Chrony的配置文件<code>chrony.conf</code>，将Chrony默认使用的时钟源指定为中国国家授时中心，并且允许处于任意网段的Chrony客户端可以通过虚拟机hadoop1的Chrony服务端进行时间同步。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server ntp.ntsc.ac.cn iburst</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（HBase）/image-20240722091655104.png" alt="image-20240722091655104"></p><p>5）配置Chrony客户端</p><p>分别在虚拟机hadoop2和虚拟机hadoop3执行<code>vi /etc/chrony.conf</code>命令编辑Chrony的配置文件<code>chrony.conf</code>，指定Chrony客户端进行时间同步的Chrony服务端。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server hadoop1 iburst</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（HBase）/image-20240722091855733.png" alt="image-20240722091855733"></p><p>6）重新启动Chrony服务</p><p>在虚拟机hadoop1上运行如下命令重新启动时间同步工具Chrony的服务。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart chronyd</span><br><span class="line">ssh hadoop2 "systemctl restart chronyd"</span><br><span class="line">ssh hadoop3 "systemctl restart chronyd"</span><br></pre></td></tr></table></figure><p>7）查看时钟源</p><p>在虚拟机hadoop1上运行如下命令查看Chrony服务端和客户端的时钟源。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chronyc sources -v</span><br><span class="line">ssh hadoop2 "chronyc sources -v"</span><br><span class="line">ssh hadoop3 "chronyc sources -v"</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（HBase）/image-20240722092222831.png" alt="image-20240722092222831"></p><h2 id="二、安装HBase"><a href="#二、安装HBase" class="headerlink" title="二、安装HBase"></a>二、安装HBase</h2><ol><li>上传安装包</li></ol><p>将hbase-2.5.8-bin.tar.gz上传到hadoop1的<code>/software</code>目录。</p><p>  <img src="完全分布式高可用集群（HBase）/image-20240722092308717.png" alt="image-20240722092308717"></p><ol><li>安装HBase</li></ol><p>以解压方式安装HBase，将HBase安装到/opt目录。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /software/hbase-2.5.8-bin.tar.gz  -C /opt</span><br><span class="line">ln -s /opt/hbase-2.5.9 /opt/hbase</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（HBase）/image-20240722092519961.png" alt="image-20240722092519961"></p><ol><li>配置HBase系统环境变量</li></ol><p>在虚拟机hadoop1执行<code>vi /etc/profile</code>命令编辑系统环境变量文件profile，在该文件的底部添加如下内容。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/opt/hbase</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure><p>执行<code>source /etc/profile</code>命令初始化系统环境变量使添加的HBase系统环境变量生效。</p><p>  <img src="完全分布式高可用集群（HBase）/image-20240722092601236.png" alt="image-20240722092601236"></p><h2 id="三、配置HBase"><a href="#三、配置HBase" class="headerlink" title="三、配置HBase"></a>三、配置HBase</h2><div class="table-container"><table><thead><tr><th>虚拟机</th><th>HMaster</th><th>HRegionServer</th></tr></thead><tbody><tr><td>hadoop1</td><td>√</td><td></td></tr><tr><td>hadoop2</td><td></td><td>√</td></tr><tr><td>hadoop3</td><td></td><td>√</td></tr></tbody></table></div><ol><li>配置文件介绍</li></ol><p>所有配置文件都位于 conf 目录中，需要保持集群中每个节点同步。</p><ul><li><p>backup-masters</p><p>默认情况下不存在。文件中添加运行备用HMaster进程的虚拟机主机名或IP。</p></li><li><p>hadoop-metrics2-hbase.properties</p><p>用于连接HBase Hadoop的Metrics2框架</p></li><li><p>hbase-env.cmd和hbase-env.sh</p><p>用于Windows和Linux/UNIX环境的脚本来设置HBase的工作环境，包括Java、Java选项和其他环境变量的位置。</p></li><li><p>hbase-policy.xml</p><p>它是一个RPC服务器使用的默认策略配置文件，根据文件配置内容对客户端请求进行授权决策。仅在启用HBase安全性时使用。</p></li><li><p>hbase-site.xml</p><p>该文件指定覆盖HBase默认的配置选项。</p></li></ul><div class="table-container"><table><thead><tr><th>配置项</th><th>说明</th></tr></thead><tbody><tr><td>hbase.tmp.dir</td><td>本地文件系统的临时目录，默认目录在/tmp目录下，该目录会在系统重启后清空，所以需要注意该参数的值<br>默认值为：j a v a . i o . t m p d i r / h b a s e − {java.io.tmpdir}/hbase-java.io.tmpdir/hbase−{user.name}</td></tr><tr><td>hbase.rootdir</td><td>RegionServers使用的目录，指定了HBase的数据存放目录，该路径需要完全限定（full-qualified），比如需要指定一个9000端口的HDFS文件系统下的/hbase目录，应写成：hdfs://namenode.example.org:9000/hbase<br>默认值：${hbase.tmp.dir}/hbase</td></tr><tr><td>hbase.cluster.distributed</td><td>是否分布式<br>默认值：false</td></tr><tr><td>hbase.zookeeper.quorum</td><td>用逗号分隔的ZooKeeper集群中的服务器列表</td></tr><tr><td>hbase.zookeeper.property.dataDir</td><td>存放HBase自己管理的zookeeper的属性数据信息的目录</td></tr><tr><td>zookeeper.znode.parent</td><td>指定了HBase在ZooKeeper上使用的节点路径</td></tr><tr><td>hbase.wal.provider</td><td>配置WAL的实现方式：<br>asyncfs：默认值。自hbase-2.0.0（hbase-1536、hbase-14790）以来新增。它构建在一个新的非阻塞dfsclient实现上。<br>filesystem：这是hbase-1.x版本中的默认设置。它构建在阻塞的DFSClient上，并以经典的DFSClient管道模式写入副本。<br>multiwal：由多个asyncfs或filesystem实例组成</td></tr><tr><td></td><td></td></tr><tr><td></td></tr></tbody></table></div><ul><li><p>log4j.properties</p><p>通过log4j进行HBase日志记录的配置文件。修改这个文件中的参数可以改变HBase的日志级别。</p></li><li><p>regionservers</p><p>包含HBase集群中运行的所有Region Server主机列表（默认情况下，这个文件包含单个条目localhost）。该文件是一个纯文本文件，每行是一个主机名或IP地址</p></li></ul><ol><li>配置HBase</li></ol><p>1）修改HBase配置文件hbase-env.sh</p><p>HBase的配置文件<code>hbase-env.sh</code>主要用于配置HBase的运行环境。进入虚拟机hadoop1的<code>/opt/hbase/conf</code>目录，执行<code>vi hbase-env.sh</code>命令编辑配置文件<code>hbase-env.sh</code>，在文件的尾部添加如下内容。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line">export JAVA_HOME=/opt/jdk</span><br></pre></td></tr></table></figure><p>HBASE_MANAGES_ZK 该配置项为true时，由HBase自己管理Zookeeper；否则，启动独立的Zookeeper</p><p>  <img src="完全分布式高可用集群（HBase）/image-20240722093450894.png" alt="image-20240722093450894"></p><p>2）修改HBase配置文件hbase-site.xml</p><p>HBase的配置文件hbase-site.xml主要用于配置HBase的参数。进入虚拟机hadoop1的<code>/opt/hbase/conf</code>目录，执行<code>vi hbase-site.xml</code>命令编辑配置文件<code>hbase-site.xml</code>，将该文件的<code>&lt;configuration&gt;</code>标签中的默认配置替换为如下内容。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hadoop1:9000/hbase&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/data/hbase/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;zookeeper.znode.parent&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/hbase&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/data/zookeeper/zkdata&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop1:2181,hadoop2:2181,hadoop3:2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.wal.provider&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;filesystem&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（HBase）/image-20240722093740769.png" alt="image-20240722093740769"></p><p>3）修改HBase配置文件regionservers</p><p>HBase的配置文件regionservers用于通过主机名指定运行Regionserver的计算机。由于这里在虚拟机hadoop2和hadoop3运行HRegionserver进程，执行 <code>vi regionservers</code> 命令编辑配置如下内容。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（HBase）/image-20240722093918379.png" alt="image-20240722093918379"></p><p>4）分发HBase安装目录</p><p>为了便捷地在虚拟机hadoop2和hadoop3安装和配置HBase，这里通过scp命令将虚拟机hadoop1的相关配置同步到两台主机。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/hbase root@hadoop2:/opt/</span><br><span class="line">scp /etc/profile root@hadoop2:/etc/</span><br><span class="line">scp -r /opt/hbase root@hadoop3:/opt/</span><br><span class="line">scp /etc/profile root@hadoop3:/etc/</span><br></pre></td></tr></table></figure><h2 id="四、启动与测试"><a href="#四、启动与测试" class="headerlink" title="四、启动与测试"></a>四、启动与测试</h2><ol><li>启动</li></ol><p>在hadoop1主机运行如下命令启动集群。</p><p>1）启动hadoop</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>2）启动zookeeper</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/xshell</span><br><span class="line">sh xzookeeper.sh start</span><br></pre></td></tr></table></figure><p>3）启动hbase</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（HBase）/image-20240722094415369.png" alt="image-20240722094415369"></p><ol><li><p>查看进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/xshell</span><br><span class="line">sh xjps.sh</span><br></pre></td></tr></table></figure></li></ol><p><img src="完全分布式高可用集群（HBase）/image-20240722094457314.png" alt="image-20240722094457314"></p><ol><li><p>网页查看（<a href="http://hadoop1:16010）">http://hadoop1:16010）</a></p><p><img src="完全分布式高可用集群（HBase）/image-20240722094531491.png" alt="image-20240722094531491"></p></li></ol><ol><li>shell测试</li></ol><p>进入hbase shell交互界面，查看集群状态、命名空间列表、表列表，最后退出交互界面</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">status</span><br><span class="line">list_namespace</span><br><span class="line">list</span><br><span class="line">exit</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（HBase）/image-20240722094750119.png" alt="image-20240722094750119"></p><ol><li>关闭集群</li></ol><p>在hadoop1主机运行如下命令关闭集群。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">stop-hbase.sh      #关闭HBase集群</span><br><span class="line">cd /opt/xshell</span><br><span class="line">sh xzookeeper.sh stop  #关闭Zookeeper集群</span><br><span class="line">stop-all.sh#关闭Hadoop集群</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;HBase&quot;&gt;&lt;a href=&quot;#HBase&quot; class=&quot;headerlink&quot; title=&quot;HBase&quot;&gt;&lt;/a&gt;HBase&lt;/h1&gt;&lt;p&gt;Apache HBase是一个分布式、可扩展、大数据存储的Hadoop数据库。当我们需要对大数据进行随机、实时的读/</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>完全分布式高可用集群（MySQL+Hive）</title>
    <link href="https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88MySQL-Hive%EF%BC%89/"/>
    <id>https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88MySQL-Hive%EF%BC%89/</id>
    <published>2024-07-22T07:04:26.000Z</published>
    <updated>2024-07-23T01:12:35.210Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><h2 id="零、资源准备"><a href="#零、资源准备" class="headerlink" title="零、资源准备"></a>零、资源准备</h2><ul><li>apache-hive-3.1.3-bin.tar.gz</li><li>mysql-connector-java-8.0.30.jar</li></ul><p>约定：所有安装包存放于/software， 安装目录为/opt。</p><h2 id="一、安装准备（MySQL）"><a href="#一、安装准备（MySQL）" class="headerlink" title="一、安装准备（MySQL）"></a>一、安装准备（MySQL）</h2><ol><li>安装MySQL</li></ol><p>将MySQL安装在虚拟机Hadoop3。</p><p>1）下载MySQL的源文件</p><p>可以通过 wget 工具下载 MySQL 源文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /software</span><br><span class="line">yum -y install wget</span><br><span class="line">wget http://dev.mysql.com/get/mysql80-community-release-el9-1.noarch.rpm</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721214134672.png" alt="image-20240721214134672"></p><p>2）安装MySQL源</p><p>安装下载的MySQL源文件，在MySQL源文件所在目录执行如下命令。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum localinstall mysql80-community-release-el9-1.noarch.rpm</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721214205637.png" alt="image-20240721214205637"></p><p>3）安装MySQL</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mysql-community-server -y</span><br></pre></td></tr></table></figure><p>如果出现Error: GPG check FAILED错误，是Mysql的GPG升级的缘故</p><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721214418834.png" alt="image-20240721214418834"></p><p>解决办法：</p><p>（1）获取GPG：<code>rpm --import https://repo.mysql.com/RPM-GPG-KEY-mysql-2023</code></p><p>（2）重新配置GPG： <code>vi /etc/yum.repos.d/mysql-community.repo</code></p><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721214353421.png" alt="image-20240721214353421"></p><p>（3）运行<code>yum install mysql-community-server -y</code>重新安装MySQL</p><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721214505630.png" alt="image-20240721214505630"></p><p>如果还是不能解决，可以参考官方安装指南： <a href="https://dev.mysql.com/doc/mysql-yum-repo-quick-guide/en">https://dev.mysql.com/doc/mysql-yum-repo-quick-guide/en</a></p><p>4）启动MySQL服务</p><p>启动MySQL服务，在虚拟机Hadoop3上执行如下命令。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mysqld</span><br></pre></td></tr></table></figure><p>可以使用命令 <code>systemctl status mysqld</code> 检查服务状态</p><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721214604836.png" alt="image-20240721214604836"></p><p>MySQL安装完成后，默认为root用户提供了初始密码，查看该初始密码的命令如下所示。</p><p>5）查看MySQL初始密码</p><p>MySQL安装完成后，默认为root用户提供了初始密码，查看该初始密码的命令如下所示。</p><p><code>grep 'temporary password' /var/log/mysqld.log</code></p><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721215052539.png" alt="image-20240721215052539"></p><p>6）修改密码</p><p>通过root用户，以及MySQL为root用户提供的初始密码登陆MySQL，具体命令如下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql –uroot -p</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721214956930.png" alt="image-20240721214956930"></p><p>MySQL默认为root提供的密码较为复杂不便使用，这里将root用户的密码修改为Abc@2024，刷新MySQL配置，使修改root用户密码的操作生效，具体命令如下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; alter user 'root'@'localhost' identified by 'Abc@2024';</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721215307052.png" alt="image-20240721215307052"></p><p>7）配置远程访问</p><p>修改Mysql配置文件<code>vi /etc/my.cnf</code>, 在[mysqld]选项下添加如下配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind-address=0.0.0.0</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721215407772.png" alt="image-20240721215407772"></p><p>授权帐号可以远程登录，介绍两种方法，任选其一：</p><p>方法一：改表法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;use mysql;</span><br><span class="line">mysql&gt;update user set host = '%' where user = 'root';</span><br><span class="line">mysql&gt;select host, user from user;</span><br></pre></td></tr></table></figure><p>方法二：授权法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'Abc@2024' WITH GRANT OPTION;</span><br><span class="line">mysql&gt;FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><p>最后重启Mysqld服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart mysqld</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721222738201.png" alt="image-20240721222738201"></p><p>这里将Hive安装在hadoop1虚拟机上</p><h2 id="二、完全分布式部署（Hive）"><a href="#二、完全分布式部署（Hive）" class="headerlink" title="二、完全分布式部署（Hive）"></a>二、完全分布式部署（Hive）</h2><ol><li>安装Hive</li></ol><p>1）上传Hive安装包</p><p>通过Hive官网或者文末提供的百度网盘链接下载Hive的安装包apache-hive-3.1.3-bin.tar.gz，使用相关工具（如finalshell）上传安装包到 /software目录。</p><p>  <img src="完全分布式高可用集群（MySQL-Hive）/image-20240721215756915.png" alt="image-20240721215756915"></p><p>2） 安装Hive</p><p>以解压方式安装Hive，将Hive安装到目录/opt，创建软链接（或重命名），在虚拟机hadoop1上执行如下命令即可。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /software/apache-hive-3.1.3-bin.tar.gz -C /opt</span><br><span class="line">ln -s /opt/apache-hive-3.1.3-bin /opt/hive</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（MySQL-Hive）/image-20240721220258104.png" alt="image-20240721220258104"></p><p>3） 同步guava包</p><p>将Hive中的guava-19.0.jar替换为Hadoop中的guava-27.0-jre.jar，具体操作执行如下命令。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/hadoop-3.3.6/share/hadoop/common/lib</span><br><span class="line">cp guava-27.0-jre.jar /opt/hive/lib/</span><br><span class="line">rm -fr /opt/hive/lib/guava-19.0.jar</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（MySQL-Hive）/image-20240721220458343.png" alt="image-20240721220458343"></p><p>4）配置Hive系统环境变量</p><p>在虚拟机执行<code>vi /etc/profile</code>命令配置系统环境变量文件profile，在该文件的底部添加如下内容。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/opt/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure><p>执行<code>source /etc/profile</code>命令初始化系统环境变量使添加的Hive系统环境变量生效。</p><p>  <img src="完全分布式高可用集群（MySQL-Hive）/image-20240721220549718.png" alt="image-20240721220549718"></p><ol><li>配置Hive</li></ol><p>在Hive安装目录的<code>/opt/hive/conf</code>下执行<code>vi hive-site.xml</code>命令创建Hive配置文件<code>hive-site.xml</code>，在该文件中添加如下内容。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://hadoop1:3306/hive?createDatabaseIfNotExist=true &lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;Abc@2024&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;         </span><br><span class="line">        &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;         </span><br><span class="line">        &lt;value&gt;10000&lt;/value&gt;     </span><br><span class="line">    &lt;/property&gt;    </span><br><span class="line">    &lt;property&gt;        </span><br><span class="line">        &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;        </span><br><span class="line">        &lt;value&gt;hadoop1&lt;/value&gt;    </span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>常用参数介绍如下：</p><div class="table-container"><table><thead><tr><th>属性名称</th><th>类型</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>hive.metastore.warehouse.dir</td><td>URI</td><td>/user/hive/warehouse</td><td>配置Hive数据存储在HDFS上的目录，托管表就存储在这里</td></tr><tr><td>hive.server2.thrift.port</td><td>INT</td><td>默认10000</td><td>配置hiveserver2的端口</td></tr><tr><td>hive.server2.thrift.bind.host</td><td>String</td><td>localhost</td><td>配置hiveserver2的主机</td></tr><tr><td>hive.metastore.uris</td><td>逗号分隔的URI</td><td>未设定</td><td>如果未设置（默认值），则使用当前的metastore，否则连接到由URI列表指定要连接的远程metastore服务器。如果有多个远程服务器，则客户端便以轮询方式连接</td></tr><tr><td>javax.jdo.option.ConnectionURL</td><td>URI</td><td>jdbc:derby:;<br>databaseName=metastore_db;<br>create=true</td><td>配置JDBC连接地址， mysql示例： jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</td></tr><tr><td>javax.jdo.option.ConnectionDriverName</td><td>String</td><td>org.apache.derby.jdbc.EmbeddedDriver</td><td>配置JDBC驱动， mysql示例： com.mysql.jdbc.Driver</td></tr><tr><td>javax.jdo.option.ConnectionUserName</td><td>String</td><td>APP</td><td>配置连接MySQL的用户名</td></tr><tr><td>javax.jdo.option.ConnectionPassword</td><td>String</td><td>mine</td><td>配置连接MySQL的密码</td></tr></tbody></table></div><ol><li>元数据初始化</li></ol><p>1）上传MySQL驱动包</p><p>在官网或者文末网盘链接下载MySQL驱动包mysql-connector-java-8.0.31.jar，将上传到在虚拟机hadoop1的<code>/opt/hive/lib</code>目录。</p><p>  <img src="完全分布式高可用集群（MySQL-Hive）/image-20240722084229022.png" alt="image-20240722084229022"></p><p>2）初始化MySQL</p><p>在操作本地模式部署的Hive之前，需要执行初始化MySQL的操作，具体命令如下。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schematool -initSchema -dbType mysql</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721222820025.png" alt="image-20240721222820025"></p><ol><li>测试</li></ol><p>1）启动Hadoop</p><p>在虚拟机hadoop1中，使用命令<code>start-all.sh</code>启动hadoop集群。</p><p>2）CLI测试</p><p>hive命令后不跟参数时，默认启动cli，即下面命令可以省略—service cli参数</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive --service cli</span><br><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240721222937588.png" alt="image-20240721222937588"></p><ol><li>beeline测试（需保持hive启动状态）</li></ol><p>Beeline是Hive 0.11版本引入的Hive客户端工具，它通过JDBC的方式连接HiveServer2服务。所以在使用beeline客户端，需要启动hiveserver2服务（HS2），在hadoop1上运行如下命令。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service hiveserver2  &amp;</span><br></pre></td></tr></table></figure><p>提示，在启动一些后台守护进程时经常搭配使用nohup和&amp;: nohup &lt;程序名&gt; &amp;</p><p>使用nohup运行程序使用Ctrl+C 发送SIGINT信号，程序关闭；关闭session发送SIGHUP信号，程序免疫</p><p>使用&amp;后台运行使用Ctrl+C 发送SIGINT信号，程序免疫；关闭session发送SIGHUP信号，程序关闭</p><p>运行如下命令测试环境。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#启动beeline</span><br><span class="line">beeline</span><br><span class="line">#在交互界面输入连接信息：</span><br><span class="line">!connect jdbc:hive2://hadoop1:10000</span><br><span class="line">#输入用户名和密码,注意，输入的用户名root和密码123456是虚拟机hadoop1的用户名和密码</span><br><span class="line">#查看所有数据库</span><br><span class="line">show databases;</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240722085235129.png" alt="image-20240722085235129"></p><p>在运行beeline命令时，也可以使用下面命令指定连接的服务地址和用户名。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline -u jdbc:hive2://hadoop1:10000 -n root</span><br></pre></td></tr></table></figure><ul><li><p>参数-u：用于指定HiveServer2服务的地址。</p></li><li><p>hadoop1：表示启动HiveServer2服务的服务器主机名。</p></li><li><p>10000：是HiveServer2服务默认使用的端口。</p></li><li><p>参数-n：用于指定连接HiveServer2服务时加载的用户名，该用户名必须具有操作HDFS的权限。</p></li></ul><ol><li>远程模式</li></ol><p>将MetaStore服务部署在虚拟机hadoop2上。</p><p>网络拓扑：</p><div class="table-container"><table><thead><tr><th>服务</th><th>虚拟机</th></tr></thead><tbody><tr><td>MetaStore</td><td>hadoop2</td></tr><tr><td>HiveServer2</td><td>hadoop1</td></tr></tbody></table></div><p>1）修改Hive配置</p><p>进入虚拟机hadoop1中Hive安装目录下的conf目录，在该目录下执行<code>vi hive-site.xml</code>命令创建Hive配置文件hive-site.xml，在该文件中添加如下内容。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;thrift://hadoop2:9083&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>hive.metastore.uris用于指定MetaStore服务器的地址，如有多个地址，使用逗号隔开。如果未设置（默认值），则使用当前的MetaStore，否则连接到由URI列表指定要连接的远程MetaStore服务器。如果有多个远程服务器，则客户端便以轮询方式连接。</p><p><img src="完全分布式高可用集群（MySQL-Hive）/image-20240722085504926.png" alt="image-20240722085504926"></p><p>2）同步Hive</p><p>将hadoop1的Hive环境同步到hadoop2， 在hadoop1上执行一下命令：</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/hive root@hadoop2:/opt</span><br><span class="line">scp /etc/profile root@hadoop2:/etc</span><br></pre></td></tr></table></figure><p>3）启动服务</p><p>在hadoop2主机上启动MetaStore服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p>在hadoop1主机上启动HiveServer2服务</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure><p> 注意，在本地模式步骤中已经启动了Hiveserver2服务，可以使用<code>ps -ef | grep hive</code>命令查看进程ID，使用<code>kill -9</code>进程ID结束指定进程。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hive&quot;&gt;&lt;a href=&quot;#Hive&quot; class=&quot;headerlink&quot; title=&quot;Hive&quot;&gt;&lt;/a&gt;Hive&lt;/h1&gt;&lt;h2 id=&quot;零、资源准备&quot;&gt;&lt;a href=&quot;#零、资源准备&quot; class=&quot;headerlink&quot; title=&quot;零、资源准</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>完全分布式高可用集群（Kafka）</title>
    <link href="https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Kafka%EF%BC%89/"/>
    <id>https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Kafka%EF%BC%89/</id>
    <published>2024-07-22T07:03:34.000Z</published>
    <updated>2024-07-23T01:09:51.182Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><h2 id="零、资源准备"><a href="#零、资源准备" class="headerlink" title="零、资源准备"></a>零、资源准备</h2><ul><li>kafka_2.13-3.7.1.tgz</li></ul><h2 id="一、搭建完全分布式集群（Kafka）"><a href="#一、搭建完全分布式集群（Kafka）" class="headerlink" title="一、搭建完全分布式集群（Kafka）"></a>一、搭建完全分布式集群（Kafka）</h2><p>注意：下载并使用Kafka前，必须安装Zookeeper！</p><p>kafka官网下载地址：<a href="https://kafka.apache.org/downloads">https://kafka.apache.org/downloads</a></p><p><img src="完全分布式高可用集群（Kafka）/image-20240721184641847.png" alt="image-20240721184641847"></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ol><li>解压压缩包到/opt目录并设置软链接（或直接改名）</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /software/kafka_2.13-3.7.1.tgz -C /opt/</span><br><span class="line">ln -s /opt/kafka_2.13-3.7.1 /opt/kafka</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721185231758.png" alt="image-20240721185231758"></p><ol><li>进入hadoop1机器的<code>/opt/kafka/config</code>目录并修改配置文件<code>server.properties</code></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/kafka/config/</span><br><span class="line">vim server.properties</span><br></pre></td></tr></table></figure><p>修改以下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">broker.id=1</span><br><span class="line">log.dir=/opt/data/kafka/kafka-logs</span><br><span class="line">listeners=PLAINTEXT://hadoop1:9092</span><br><span class="line">zookeeper.connect=hadoop1:2181,hadoop2:2181,hadoop3:2181</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721191814025.png" alt="image-20240721191814025"></p><ol><li>修改<code>/etc/profile</code>文件，填入以下内容</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export KAFKA_HOME=/opt/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure><p>修改后记得source一下</p><p><img src="完全分布式高可用集群（Kafka）/image-20240721190308241.png" alt="image-20240721190308241"></p><ol><li>分发到hadoop2和hadoop3</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/kafka hadoop2:/opt/</span><br><span class="line">scp -r /opt/kafka hadoop3:/opt/</span><br><span class="line">scp -r /etc/profile hadoop2:/etc</span><br><span class="line">scp -r /etc/profile hadoop3:/etc</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop2 "source /etc/profile"</span><br><span class="line">ssh hadoop3 "source /etc/profile"</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721193839715.png" alt="image-20240721193839715"></p><ol><li>修改hadoop2和hadoop3的<code>/opt/kafka/config/server.properties</code>文件</li></ol><p><strong>hadoop2</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">broker.id=2</span><br><span class="line">listeners=PLAINTEXT://hadoop2:9092</span><br></pre></td></tr></table></figure><p><strong>hadoop3</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">broker.id=3</span><br><span class="line">listeners=PLAINTEXT://hadoop3:9092</span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p><strong>三台机器都要启动Zookeeper</strong></p><p><strong>三台机器都要启动Kafka</strong></p><ol><li>启停命令（三台都需要启动）</li></ol><p>kafka启动命令：（不能用环境变量启动，因为环境变量只配置了bin，没配置config）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/kafka/bin/kafka-server-start.sh -daemon /opt/kafka/config/server.properties</span><br><span class="line">ssh hadoop2 "/opt/kafka/bin/kafka-server-start.sh -daemon /opt/kafka/config/server.properties"</span><br><span class="line">ssh hadoop3 "/opt/kafka/bin/kafka-server-start.sh -daemon /opt/kafka/config/server.properties"</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721194626136.png" alt="image-20240721194626136"></p><p>查看三台机器的Kafka状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br><span class="line">ssh hadoop2 "jps"</span><br><span class="line">ssh hadoop3 "jps"</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721194741943.png" alt="image-20240721194741943"></p><p>kafka停止命令：（以hadoop1为例，如运行时找不到文件，可用完整路径运行，即加上前缀<code>/opt/kafka/bin</code>)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-stop.sh</span><br><span class="line">ssh hadoop2 "kafka-server-stop.sh"</span><br><span class="line">ssh hadoop3 "kafka-server-stop.sh"</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721193949189.png" alt="image-20240721193949189"></p><ol><li>创建Topic（如运行时找不到文件，可用完整路径运行，即加上前缀<code>/opt/kafka/bin</code>）</li></ol><p>1）进入到hadoop1的Kafka的<code>/opt/kafka</code>目录下，创建一个名为<code>test</code>的Topic。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server hadoop1:9092 --create --topic test --partitions 3 replication-factor 1</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721195326702.png" alt="image-20240721195326702"></p><p>2）查看创建的Topic列表。</p><p><code>kafka-topics.sh --list --bootstrap-server hadoop1:9092</code></p><p><img src="完全分布式高可用集群（Kafka）/image-20240721195519477.png" alt="image-20240721195519477"></p><ol><li>测试通信（以hadoop1和hadoop2为例）</li></ol><p>1）连接生产者，命令后面需要对应 Topic 名称。（如找不到文件，可用完整路径运行）</p><p>在<strong>hadoop1</strong>的命令行输入以下命令</p><p><code>kafka-console-producer.sh --bootstrap-server hadoop1:9092 --topic test</code></p><p><img src="完全分布式高可用集群（Kafka）/image-20240721200131235.png" alt="image-20240721200131235"></p><p>2）连接消费者，需要连接消费者的Topic</p><p>在<strong>hadoop2</strong>的命令行输入以下命令</p><p><code>/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server hadoop1:9092 --topic test --from-beginning</code></p><p>当用户在hadoop1输入消息时，hadoop2会实时显示出用户在hadoop1中输入的消息</p><p><img src="完全分布式高可用集群（Kafka）/image-20240721200206695.png" alt="image-20240721200206695"></p><p>同理，hadoop3也可以。</p><h3 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h3><p>可用脚本启停Kafka，查看Kafka状态</p><ol><li>在<code>/opt/xshell</code>文件夹中创建脚本<code>xjps.sh</code>，并输入以下内容</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">for i in hadoop1 hadoop2 hadoop3</span><br><span class="line">do</span><br><span class="line">    echo --------- $i ----------</span><br><span class="line">    ssh $i "/opt/jdk/bin/jps $$*"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721201352162.png" alt="image-20240721201352162"></p><p>运行脚本，查看状态</p><p><img src="完全分布式高可用集群（Kafka）/image-20240721201425053.png" alt="image-20240721201425053"></p><ol><li>在<code>/opt/xshell</code>文件夹中创建脚本<code>xzookeeper.sh</code>，并输入以下内容（上一章做过的可跳过）</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line"># 设置JAVA_HOME和更新PATH环境变量</span><br><span class="line">export JAVA_HOME=/opt/jdk</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line"># 检查输入参数</span><br><span class="line">if [ $# -ne 1 ]; then</span><br><span class="line">    echo "用法: $0 {start|stop|status}"</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># 执行操作</span><br><span class="line">case "$1" in</span><br><span class="line">    start)</span><br><span class="line">        echo "---------- Zookeeper 启动 ------------"</span><br><span class="line">        /opt/zookeeper/bin/zkServer.sh start</span><br><span class="line">        ssh hadoop2 "export JAVA_HOME=/opt/jdk; export PATH=\$PATH:\$JAVA_HOME/bin; /opt/zookeeper/bin/zkServer.sh start"</span><br><span class="line">        ssh hadoop3 "export JAVA_HOME=/opt/jdk; export PATH=\$PATH:\$JAVA_HOME/bin; /opt/zookeeper/bin/zkServer.sh start"</span><br><span class="line">        ;;</span><br><span class="line">    stop)</span><br><span class="line">        echo "---------- Zookeeper 停止 ------------"</span><br><span class="line">        /opt/zookeeper/bin/zkServer.sh stop</span><br><span class="line">        ssh hadoop2 "export JAVA_HOME=/opt/jdk; export PATH=\$PATH:\$JAVA_HOME/bin; /opt/zookeeper/bin/zkServer.sh stop"</span><br><span class="line">        ssh hadoop3 "export JAVA_HOME=/opt/jdk; export PATH=\$PATH:\$JAVA_HOME/bin; /opt/zookeeper/bin/zkServer.sh stop"</span><br><span class="line">        ;;</span><br><span class="line">    status)</span><br><span class="line">        echo "---------- Zookeeper 状态 ------------"</span><br><span class="line">        /opt/zookeeper/bin/zkServer.sh status</span><br><span class="line">        ssh hadoop2 "export JAVA_HOME=/opt/jdk; export PATH=\$PATH:\$JAVA_HOME/bin; /opt/zookeeper/bin/zkServer.sh status"</span><br><span class="line">        ssh hadoop3 "export JAVA_HOME=/opt/jdk; export PATH=\$PATH:\$JAVA_HOME/bin; /opt/zookeeper/bin/zkServer.sh status"</span><br><span class="line">        ;;</span><br><span class="line">    *)</span><br><span class="line">        echo "未知命令: $1"</span><br><span class="line">        echo "用法: $0 {start|stop|status}"</span><br><span class="line">        exit 2</span><br><span class="line">        ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721202131221.png" alt="image-20240721202131221"></p><p>运行代码<code>sh xzookeeper.sh status</code>，查看zookeeper状态</p><p><img src="完全分布式高可用集群（Kafka）/image-20240721202100896.png" alt="image-20240721202100896"></p><ol><li>在<code>/opt/xshell</code>文件夹中创建脚本<code>xkafka.sh</code>，并输入以下内容</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line"># Kafka和Zookeeper的配置</span><br><span class="line">KAFKA_HOME=/opt/kafka</span><br><span class="line">ZOOKEEPER_HOME=/opt/zookeeper</span><br><span class="line">JAVA_HOME=/opt/jdk</span><br><span class="line"></span><br><span class="line"># 定义启动Kafka的函数</span><br><span class="line">start_kafka() {</span><br><span class="line">    echo "Starting Kafka on hadoop1..."</span><br><span class="line">    $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">    </span><br><span class="line">    echo "Starting Kafka on hadoop2..."</span><br><span class="line">    ssh hadoop2 "export JAVA_HOME=$JAVA_HOME; export KAFKA_HOME=$KAFKA_HOME; $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties"</span><br><span class="line">    </span><br><span class="line">    echo "Starting Kafka on hadoop3..."</span><br><span class="line">    ssh hadoop3 "export JAVA_HOME=$JAVA_HOME; export KAFKA_HOME=$KAFKA_HOME; $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties"</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"># 定义停止Kafka的函数</span><br><span class="line">stop_kafka() {</span><br><span class="line">    echo "Stopping Kafka on hadoop1..."</span><br><span class="line">    $KAFKA_HOME/bin/kafka-server-stop.sh</span><br><span class="line">    </span><br><span class="line">    echo "Stopping Kafka on hadoop2..."</span><br><span class="line">    ssh hadoop2 "export KAFKA_HOME=$KAFKA_HOME; $KAFKA_HOME/bin/kafka-server-stop.sh"</span><br><span class="line">    </span><br><span class="line">    echo "Stopping Kafka on hadoop3..."</span><br><span class="line">    ssh hadoop3 "export KAFKA_HOME=$KAFKA_HOME; $KAFKA_HOME/bin/kafka-server-stop.sh"</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"># 定义检查Kafka状态的函数</span><br><span class="line">check_status() {</span><br><span class="line">    echo "Checking Kafka status on hadoop1..."</span><br><span class="line">    ssh hadoop1 "jps | grep -i kafka"</span><br><span class="line">    </span><br><span class="line">    echo "Checking Kafka status on hadoop2..."</span><br><span class="line">    ssh hadoop2 "jps | grep -i kafka"</span><br><span class="line">    </span><br><span class="line">    echo "Checking Kafka status on hadoop3..."</span><br><span class="line">    ssh hadoop3 "jps | grep -i kafka"</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"># 处理命令行参数</span><br><span class="line">case "$1" in</span><br><span class="line">    start)</span><br><span class="line">        start_kafka</span><br><span class="line">        ;;</span><br><span class="line">    stop)</span><br><span class="line">        stop_kafka</span><br><span class="line">        ;;</span><br><span class="line">    status)</span><br><span class="line">        check_status</span><br><span class="line">        ;;</span><br><span class="line">    *)</span><br><span class="line">        echo "Usage: $0 {start|stop|status}"</span><br><span class="line">        exit 1</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Kafka）/image-20240721201615724.png" alt="image-20240721201615724"></p><p>运行代码<code>sh xkafka.sh status</code>，查看Kafka状态</p><p><img src="完全分布式高可用集群（Kafka）/image-20240721201724979.png" alt="image-20240721201724979"></p><h2 id="二、常见错误与解决方案"><a href="#二、常见错误与解决方案" class="headerlink" title="二、常见错误与解决方案"></a>二、常见错误与解决方案</h2><ol><li>找不到路径，可用完整路径运行文件，即加上前缀<code>/opt/kafka/bin</code></li><li>在scp后，需要修改hadoop2和hadoop3中的<code>/opt/kafka/config/server.properties</code> 文件</li><li>在scp系统文件<code>/etc/profile</code>后，记得source一下</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Kafka&quot;&gt;&lt;a href=&quot;#Kafka&quot; class=&quot;headerlink&quot; title=&quot;Kafka&quot;&gt;&lt;/a&gt;Kafka&lt;/h1&gt;&lt;h2 id=&quot;零、资源准备&quot;&gt;&lt;a href=&quot;#零、资源准备&quot; class=&quot;headerlink&quot; title=&quot;零</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>完全分布式高可用集群（Zookeeper）</title>
    <link href="https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Zookeeper%EF%BC%89/"/>
    <id>https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Zookeeper%EF%BC%89/</id>
    <published>2024-07-22T07:00:36.000Z</published>
    <updated>2024-07-23T01:30:12.948Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><p>ZooKeeper是一个开源的分布式协调服务，它为分布式应用提供了高效且可靠的分布式协调服务，并且是分布式应用保证数据一致性的解决方案。该项目由雅虎公司创建，是Google Chubby的开源实现。</p><p>分布式应用可以基于ZooKeeper实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁、分布式队列等功能。在越来越多的分布式系统（Hadoop、HBase、Storm、Kafka）中，Zookeeper都作为核心组件使用。</p><h2 id="零、资源准备"><a href="#零、资源准备" class="headerlink" title="零、资源准备"></a>零、资源准备</h2><ul><li>apache-zookeeper-3.8.4-bin.tar.gz</li></ul><h2 id="一、搭建完全分布式集群（Zookeeper）"><a href="#一、搭建完全分布式集群（Zookeeper）" class="headerlink" title="一、搭建完全分布式集群（Zookeeper）"></a>一、搭建完全分布式集群（Zookeeper）</h2><p>Zookeeper的部署方式分为：</p><ul><li><p>独立模式（单机模式）</p></li><li><p>集群模式</p></li><li><p>伪分布式模式</p></li><li><p>完全分布模式</p></li></ul><p>本文介绍实际应用场景中的使用的完全分布模式的部署，生产环境一般采用奇数台（大于1）机器组成集群。</p><p>ZK节点为什么设置为奇数？</p><p>zookeeper有这样一个特性：集群中只要有过半的机器是正常工作的，那么整个集群对外就是可用的。</p><p>也就是说如果有2个zookeeper，那么只要有1个服务停止，zookeeper就不能对外提供服务，所以2个zookeeper的死亡容忍度为0；</p><p>同理，要是有3个zookeeper， 1个服务停止，还剩下2个正常的，过半了，所以3个zookeeper的容忍度为1；</p><p>同理：2 -&gt; 0; 3 -&gt; 1; 4 - &gt;1; 5 -&gt; 2; 6 -&gt; 2会发现一个规律，2n和2n-1的容忍度是一样的，都是n-1，所以为了更加高效，会选择奇数台组成集群。</p><p>在进行下面步骤之前，确保已经安装好虚拟机hadoop1、hadoop2、hadoop3，并且已经完成集群的网络环境配置和JDK的安装。</p><ol><li>安装Zookeeper</li></ol><p><img src="完全分布式高可用集群（Zookeeper）/image-20240721181229680.png" alt="image-20240721181229680"></p><p>将zookeeper安装包上传至hadoop1虚拟机/software目录，运行下面命令解压安装</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf /software/apache-zookeeper-3.8.4-bin.tar.gz -C /opt</span><br><span class="line">ln -s /opt/apache-zookeeper-3.8.4-bin /opt/zookeeper</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（Zookeeper）/image-20240721181337560.png" alt="image-20240721181337560"></p><p>修改配置文件 <code>vi /etc/profile</code> ，添加如下内容：</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export ZK_HOME=/opt/zookeeper</span><br><span class="line">export PATH=$PATH:$ZK_HOME/bin</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（Zookeeper）/image-20240721181416155.png" alt="image-20240721181416155"></p><p>运行<code>source /etc/profile</code>更新环境变量</p><ol><li>修改配置文件</li></ol><p>进入虚拟机hadoop1中ZooKeeper安装目录的conf目录，通过复制ZooKeeper模板文件zoo_sample.cfg创建文件zoo.cfg，再编辑文件zoo.cfg：</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/zookeeper/conf</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">vi zoo.cfg</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（Zookeeper）/image-20240721181526380.png" alt="image-20240721181526380"></p><p>修改相应的配置项</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#设置数据持久化目录</span><br><span class="line">dataDir=/opt/data/zookeeper/zkdata</span><br><span class="line">#设置客户端连接当前ZooKeeper服务使用的端口号</span><br><span class="line">clientPort=2181</span><br><span class="line">#设置ZooKeeper集群中每个ZooKeeper服务的地址及端口号</span><br><span class="line">server.1=hadoop1:2888:3888</span><br><span class="line">server.2=hadoop2:2888:3888</span><br><span class="line">server.3=hadoop3:2888:3888</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Zookeeper）/image-20240721182921952.png" alt="image-20240721182921952"></p><ol><li>创建数据持久化目录</li></ol><p>根据文件zoo.cfg中参数dataDir指定的值，在虚拟机hadoop1下执行以下命令，创建数据持久化目录。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /opt/data/zookeeper/zkdata</span><br><span class="line">ssh hadoop2 "mkdir -p /opt/data/zookeeper/zkdata"</span><br><span class="line">ssh hadoop3 "mkdir -p /opt/data/zookeeper/zkdata"</span><br></pre></td></tr></table></figure><ol><li>创建myid文件</li></ol><p>在虚拟机hadoop1主机执行以下命令，在数据持久化目录<code>/opt/data/zookeeper/zkdata</code>创建myid文件并分别写入值1、2、3。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /opt/data/zookeeper/zkdata/myid</span><br><span class="line">ssh hadoop2 "echo 2 &gt; /opt/data/zookeeper/zkdata/myid"</span><br><span class="line">ssh hadoop3 "echo 3 &gt; /opt/data/zookeeper/zkdata/myid"</span><br></pre></td></tr></table></figure><p>myid文件标识了该服务器在集群中的唯一ID号，该文件内容就是对应的ID号。</p><p>ID大小介于1至255，如果开启了扩展特征，比如TTL节点，ID需要介于1至254</p><p>  <img src="完全分布式高可用集群（Zookeeper）/image-20240721182254725.png" alt="image-20240721182254725"></p><ol><li>分发Zookeeper和环境变量</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/zookeeper hadoop2:/opt/</span><br><span class="line">scp -r /opt/zookeeper hadoop3:/opt/</span><br><span class="line">scp -r /etc/profile hadoop2:/etc</span><br><span class="line">scp -r /etc/profile hadoop3:/etc</span><br></pre></td></tr></table></figure><ol><li>启动zookeeper集群</li></ol><p>在虚拟机hadoop1上执行以下命令启动ZooKeeper服务。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br><span class="line">ssh hadoop2 "source /etc/profile &amp;&amp; zkServer.sh start"</span><br><span class="line">ssh hadoop3 "source /etc/profile &amp;&amp; zkServer.sh start"</span><br></pre></td></tr></table></figure><ol><li>查看启动状态</li></ol><p>在虚拟机hadoop1上执行以下命令查看ZooKeeper集群运行状态。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh status</span><br><span class="line">ssh hadoop2 "source /etc/profile &amp;&amp; zkServer.sh status"</span><br><span class="line">ssh hadoop3 "source /etc/profile &amp;&amp; zkServer.sh status"</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Zookeeper）/image-20240721183236077.png" alt="image-20240721183236077"></p><ol><li>集群服务管理脚本</li></ol><p>在虚拟机hadoop1上执行下面的命令，编写zookeeper集群服务管理脚本</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line">mkdir xshell</span><br><span class="line">cd xshell</span><br><span class="line">touch xzookeeper.sh</span><br><span class="line">chmod +x xzookeeper.sh</span><br><span class="line">vi xzookeeper.sh</span><br></pre></td></tr></table></figure><p>编辑如下脚本内容</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">for host in hadoop1 hadoop2 hadoop3</span><br><span class="line">do</span><br><span class="line">        case $1 in</span><br><span class="line">        "start"){</span><br><span class="line">                echo "                                               "</span><br><span class="line">                echo "--------------- 启 动 zookeeper ---------------"</span><br><span class="line">                echo "------------ $host zookeeper -----------"</span><br><span class="line">                ssh $host "source /etc/profile &amp;&amp; zkServer.sh start"</span><br><span class="line">        };;</span><br><span class="line">        "stop"){</span><br><span class="line">                echo "                                               "</span><br><span class="line">                echo "--------------- 关 闭 zookeeper ---------------"</span><br><span class="line">                echo "------------ $host zookeeper -----------"</span><br><span class="line">                ssh $host "source /etc/profile &amp;&amp; zkServer.sh stop"</span><br><span class="line">        };;</span><br><span class="line">        "status"){</span><br><span class="line">                echo "                                               "</span><br><span class="line">                echo "-------------- 查看zookeeper状态 --------------"</span><br><span class="line">                echo "------------ $host zookeeper -----------"</span><br><span class="line">                ssh $host "source /etc/profile &amp;&amp; zkServer.sh status"</span><br><span class="line">        };;</span><br><span class="line">        esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（Zookeeper）/image-20240721183623130.png" alt="image-20240721183623130"></p><p>  <img src="完全分布式高可用集群（Zookeeper）/image-20240721183709695.png" alt="image-20240721183709695"></p><h2 id="二、常见问题及解决办法"><a href="#二、常见问题及解决办法" class="headerlink" title="二、常见问题及解决办法"></a>二、常见问题及解决办法</h2><ol><li>端口被占用</li></ol><p>错误提示：Address already in use</p><p>解决办法：</p><ul><li><p>一方面，可以选择停止掉现在占用端口的进程，使用命令netstat -nltp 并结合命令grep进行查询；</p></li><li><p>另一方面，可以修改zoo.cfg，改变端口号</p></li></ul><ol><li>磁盘空间不够</li></ol><p>错误提示：No space left on device</p><p>解决办法：清磁盘或者磁盘</p><ol><li>无法找到myid文件</li></ol><p>错误提示：myid file is missing</p><p>解决办法：在dataDir对应的目录中创建myid文件，并设置正确的内容（服务器对应的id）</p><ol><li>集群中其他机器Leader选举端口未开</li></ol><p>错误提示：Cannot open channel to 2 at election address /xxx.xxx.xxx.xxx:3888</p><p>解决办法：</p><ul><li><p>检查各服务器防火墙是否关闭，使用命令sudo ufw status</p></li><li><p>检查各服务器/etc/hosts中的内容是否一致，是否配置了所有节点的ip</p></li><li><p>检查各服务器的时间是一致</p></li><li><p>修改各服务器的zoo.cfg，将各自服务器中对应于自己的集群信息中的host修改成0.0.0.0</p></li><li><p>比如对示例中的服务器hadoop1，将其zoo.cfg的集群信息修改成</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server.1=0.0.0.0:2888:3888</span><br><span class="line">server.2=hadoop2:2888:3888</span><br><span class="line">server.3=hadoop3:2888:3888</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Zookeeper&quot;&gt;&lt;a href=&quot;#Zookeeper&quot; class=&quot;headerlink&quot; title=&quot;Zookeeper&quot;&gt;&lt;/a&gt;Zookeeper&lt;/h1&gt;&lt;p&gt;ZooKeeper是一个开源的分布式协调服务，它为分布式应用提供了高效且可靠的分布式</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>完全分布式高可用集群（Hadoop）</title>
    <link href="https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Hadoop%EF%BC%89/"/>
    <id>https://want595.github.io/2024/07/22/%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%88Hadoop%EF%BC%89/</id>
    <published>2024-07-22T06:56:02.000Z</published>
    <updated>2024-07-24T07:21:10.980Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h2 id="零、资源准备"><a href="#零、资源准备" class="headerlink" title="零、资源准备"></a>零、资源准备</h2><ul><li><p>VMware workstation 16</p></li><li><p>CentOS-Stream-9-latest-x86_64-dvd1.iso</p></li><li><p>jdk-8u361-linux-x64.tar.gz</p></li><li><p>Hadoop 3.3.6.tar.gz</p></li></ul><h2 id="一、前置准备"><a href="#一、前置准备" class="headerlink" title="一、前置准备"></a>一、前置准备</h2><ol><li>创建虚拟机</li></ol><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140358041.png" alt="image-20240721140358041"></p><ol><li>选择典型安装</li></ol><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140425249.png" alt="image-20240721140425249"></p><p>2）安装来源暂时不指定</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140440892.png" alt="image-20240721140440892"></p><p>3）操作系统选择Linux</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140457051.png" alt="image-20240721140457051"></p><p>4）设置虚拟机名称和位置</p><p>注意：位置可以根据自己电脑的使用情况选择相应空闲磁盘</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140530128.png" alt="image-20240721140530128"></p><p>5）磁盘容量指定</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140542229.png" alt="image-20240721140542229"></p><p>6）完成新建</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140552492.png" alt="image-20240721140552492"></p><p>7）虚拟机设置</p><p>注意：配置内存为2G，处理器2个，可根据电脑配置适当增加</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140625647.png" alt="image-20240721140625647"></p><ol><li>安装CentOS</li></ol><p>1）设置CentOS映像文件</p><p>ISO映像选择下载的<strong>CentOS-Stream-9-xxxxxxx.iso</strong></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140732879.png" alt="image-20240721140732879"></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140702212.png" alt="image-20240721140702212"></p><p>2）启动虚拟机</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140755596.png" alt="image-20240721140755596"></p><p>3）开始安装</p><p>选择Install CentOS Stream 9进行安装</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140817699.png" alt="image-20240721140817699"></p><p>4）语言选择中文（或English）</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721140912212.png" alt="image-20240721140912212"></p><p>5）安装前的配置</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721141331991.png" alt="image-20240721141331991"></p><p>① 安装目的地</p><p>默认配置即可</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721141343343.png" alt="image-20240721141343343"></p><p>② 软件选择</p><p>选择Server with GUI，并勾选Performance Tools</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721141402345.png" alt="image-20240721141402345"></p><p>③ 时区</p><p>区域选择亚洲， 城市选择上海</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721141503285.png" alt="image-20240721141503285"></p><p>④ 网络设置</p><p>确保网卡已经打开（左下角可设置主机名，也可以后续设置）</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721141754308.png" alt="image-20240721141754308"></p><p>⑤ 配置root用户密码</p><p>注意勾选“允许root用户使用密码进行SSH登录”，作为练习，密码可以设置简单点，比如123456</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721141617628.png" alt="image-20240721141617628"></p><p>6）等待安装完成后重启系统。</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721141825364.png" alt="image-20240721141825364"></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721143839541.png" alt="image-20240721143839541"></p><p>7）配置虚拟机SSH远程登录</p><p>① 启动hadoop1</p><p>进入登录界面注册用户</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721143958759.png" alt="image-20240721143958759"></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721144035001.png" alt="image-20240721144035001"></p><p>打开终端</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721144247390.png" alt="image-20240721144247390"></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721144300119.png" alt="image-20240721144300119"></p><p>切换到root用户，输入<code>su root</code>命令然后输入密码即可</p><p>②配置虚拟机SSH远程登录</p><p><strong>第一步，检查SSH服务是否安装和启动</strong></p><p>在虚拟机中，分别执行<code>rpm -qa | grep ssh</code>和<code>ps -ef | grep sshd</code>命令，查看当前虚拟机是否安装了SSH服务，以及SSH服务是否启动。</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721144528022.png" alt="image-20240721144528022"></p><p>rpm（英文全拼：redhat package manager） 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 易于安装，升级，间接提升了 Linux 的适用度。</p><p>ps （英文全拼：process status）命令用于显示当前进程的状态，类似于 windows 的任务管理器。</p><p>grep (global regular expression) 命令用于查找文件里符合条件的字符串或正则表达式。该命令用于查找内容包含指定的范本样式的文件，如果发现某文件的内容符合所指定的范本样式，预设 grep 指令会把含有范本样式的那一列显示出来。若不指定任何文件名称，或是所给予的文件名为 -，则 grep 指令会从标准输入设备读取数据。</p><p>如果没有安装，可以使用以下命令进行安装</p><p><code>yum install openssh-server openssh-clients</code></p><p><strong>第二步，修改SSH服务配置文件</strong></p><p>默认情况下，CentOS Stream 9不允许用户root进行远程登录，在虚拟机hadoop1中执行<code>vi /etc/ssh/sshd_config</code>命令编辑配置文件sshd_config。将PermitRootLogin修改为yes</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721145123672.png" alt="image-20240721145123672"></p><blockquote><p>对于小白，这里介绍下vi命令的简单使用方式：使用vi命令打开文件后，输入字母i进入插入模式 =&gt; 修改相应的文件内容 =&gt; 按Esc键进入命令行模式 =&gt; 输入:进入底行模式 =&gt; 输入x或者wq保存退出。</p><p>如果文件修改后不想保存，进行底行模式后输入q!进行不保存退出。</p></blockquote><p><strong>第三步， 重启SSH服务</strong></p><p><code>systemctl restart sshd</code></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721145430440.png" alt="image-20240721145430440"></p><ol><li>克隆主机</li></ol><p>1）关闭hadoop1</p><p>在终端使用命令<code>shutdown -h now</code>关闭hadoop1或直接关机</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721145558170.png" alt="image-20240721145558170"></p><p>  2）克隆虚拟机</p><p>克隆虚拟机hadoop1、hadoop2、hadoop3，以hadoop1为例</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721145846715.png" alt="image-20240721145846715"></p><p>完整克隆的虚拟机是通过复制原虚拟机创建完全独立的新虚拟机，不和原虚拟机共享任何资源，可以脱离原虚拟机独立使用。</p><p>链接克隆的虚拟机需要和原虚拟机共享同一个虚拟磁盘文件，不能脱离原虚拟机独立运行。</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721145940932.png" alt="image-20240721145940932"></p><p>克隆hadoop1</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721150101913.png" alt="image-20240721150101913"></p><p>克隆hadoop2（同理）</p><p>克隆hadoop3（同理）</p><ol><li><p>网络设置</p><p>网络整体规划如下：</p></li></ol><div class="table-container"><table><thead><tr><th>虚拟机名</th><th>主机名</th><th>IP</th></tr></thead><tbody><tr><td>hadoop1</td><td>hadoop1</td><td>192.168.121.160</td></tr><tr><td>hadoop2</td><td>hadoop2</td><td>192.168.121.161</td></tr><tr><td>hadoop3</td><td>hadoop3</td><td>192.168.121.162</td></tr></tbody></table></div><p>1）配置VMware Workstation网络</p><p>在VMware Workstation主界面，依次单击“编辑”→“虚拟网络编辑器…”选项，配置VMware Workstation网络。</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721150948899.png" alt="image-20240721150948899"></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721151031381.png" alt="image-20240721151031381"></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721151059435.png" alt="image-20240721151059435"></p><p>2）配置静态IP</p><p>以hadoop1主机为例，类似配置hadoop2、 hadoop3（所有命令三台机器都要测试）</p><p>编辑配置文件（先切换到root用户）</p><p><code>vi /etc/NetworkManager/system-connections/ens160.nmconnection</code></p><p>注意每台机器的address不同，第一台是address1，ip是192.168.121.160，另两台分别是161和162</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">method=manual</span><br><span class="line">address1=192.168.121.160/24,192.168.121.2</span><br><span class="line">dns=114.114.114.114</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721151618354.png" alt="image-20240721151618354"></p><p>修改uuid（只需要修改hadoop2、 hadoop3主机）</p><p>uuid的作用是使分布式系统中的所有元素都有唯一的标识码。</p><p><code>sed -i '/uuid=/c\uuid='</code>uuidgen<code>'' /etc/NetworkManager/system-connections/ens160.nmconnection</code></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721152046968.png" alt="image-20240721152046968"></p><p>重启ens33网卡和重新加载网络配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nmcli c reload</span><br><span class="line">nmcli c up ens160</span><br></pre></td></tr></table></figure><p>查看网络信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip a</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721152242800.png" alt="image-20240721152242800"></p><p>检测网络</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping www.baidu.com</span><br></pre></td></tr></table></figure><p>请ping百度（下图可省略）</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721152337109.png" alt="image-20240721152337109"></p><p>输入ctrl+c退出检测</p><p>3）主机名</p><p>执行命令后需要重新打开终端，才会更新主机名</p><p>配置hadoop2主机名</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname hadoop2</span><br></pre></td></tr></table></figure><p>配置hadoop3主机名</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname hadoop3</span><br></pre></td></tr></table></figure><p>4）配置虚拟机SSH远程登录</p><p>配置finalshell，需下载该软件</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721155152496.png" alt="image-20240721155152496"></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721155248780.png" alt="image-20240721155248780"></p><p>5）修改映射文件</p><p>在虚拟机hadoop1主机执行<code>vi /etc/hosts</code>命令编辑映射文件hosts，在配置文件中添加如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.121.160 hadoop1</span><br><span class="line">192.168.121.161 hadoop2</span><br><span class="line">192.168.121.162 hadoop3</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721154244276.png" alt="image-20240721154244276"></p><p>在虚拟机hadoop1主机执行如下命令，拷贝配置到hadoop2, hadoop3</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/hosts root@hadoop2:/etc/hosts</span><br><span class="line">scp /etc/hosts root@hadoop3:/etc/hosts</span><br></pre></td></tr></table></figure><p>需要输入hadoop2和hadoop3的管理员密码，即123456</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721154353364.png" alt="image-20240721154353364"></p><ol><li><p>关闭防火墙</p><p>关闭虚拟机hadoop1、hadoop2和hadoop3的防火墙，分别在3台虚拟机中运行如下命令关闭防火墙并禁止防火墙开启启动</p><p>关闭防火墙<code>systemctl stop firewalld</code></p><p>禁止防火墙开机启动<code>systemctl disable firewalld</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721154503838.png" alt="image-20240721154503838"></p></li><li><p>免密登录<br>在集群环境中，主节点需要频繁的访问从节点，以获取从节点的运行状态，主节点每次访问从节点时都需要通过输入密码的方式进行验证，确定密码输入正确后才建立连接，这会对集群运行的连续性造成不良影响，为主节点配置SSH免密登录功能，可以有效避免访问从节点时频繁输入密码。接下来，虚拟机hadoop1作为集群环境的主节点实现SSH免密登录。</p><p>SSH免密登录原理（原理：非对称加密算法：公钥加密（给别人）、私钥解密给自己）</p></li></ol><p>1）生成密钥</p><p>在虚拟机hadoop1中执行<code>ssh-keygen -t rsa</code>命令，生成密钥。（输入命令后一直按回车键即可）</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721155442688.png" alt="image-20240721155442688"></p><p>查看秘钥文件</p><p>在虚拟机hadoop1中执行<code>ll /root/.ssh</code>命令查看密钥文件。</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721155539388.png" alt="image-20240721155539388"></p><p>2）复制公钥文件</p><p>将虚拟机hadoop1生成的公钥文件复制到集群中相关联的所有虚拟机，实现通过虚拟机hadoop1可以免密登录虚拟机hadoop1、hadoop2和hadoop3。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id hadoop1</span><br><span class="line">ssh-copy-id hadoop2</span><br><span class="line">ssh-copy-id hadoop3</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721155632699.png" alt="image-20240721155632699"></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721155932799.png" alt="image-20240721155932799"></p><p>3）测试免密登录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop1</span><br><span class="line">ssh hadoop2</span><br><span class="line">ssh hadoop3</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721155808525.png" alt="image-20240721155808525"></p><ol><li><p>安装jdk</p><p><strong>约定：软件安装包存放于/software，软件安装至/opt</strong></p></li></ol><p>1）创建目录</p><p>在虚拟机hadoop1中执行<code>mkdir /software</code></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721160154170.png" alt="image-20240721160154170"></p><p>2）上传jdk</p><p>利用finalshell将jdk-8u361-linux-x64.tar.gz上传至hadoop1的/software目录（直接拖进去即可）</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721160331623.png" alt="image-20240721160331623"></p><p>3）解压jdk并设置软链接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /software</span><br><span class="line">tar -xvf jdk-8u361-linux-x64.tar.gz -C /opt</span><br><span class="line">ln -s /opt/jdk1.8.0_361 /opt/jdk</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721161306342.png" alt="image-20240721161306342"></p><p>4）配置jdk系统环境变量</p><p>在虚拟机hadoop1执行<code>vi /etc/profile</code>命令编辑环境变量文件profile，在该文件的底部添加配置jdk系统环境变量的内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/jdk</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721161425765.png" alt="image-20240721161425765"></p><p>记得执行<code>source /etc/profile</code>重新加载系统环境变量</p><p>5）配置java执行程序的软链接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 删除系统自带的java程序</span><br><span class="line">rm -f /usr/bin/java</span><br><span class="line"># 软链接我们自己安装的java程序</span><br><span class="line">ln -s /opt/jdk/bin/java /usr/bin/java</span><br></pre></td></tr></table></figure><p>6）验证jdk</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721161723219.png" alt="image-20240721161723219"></p><p>7）同步文件</p><p>分发jdk安装目录和系统环境变量文件至hadoop2、hadoop3</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r  /opt/jdk root@hadoop2:/opt</span><br><span class="line">scp  /etc/profile root@hadoop2:/etc</span><br><span class="line">scp -r  /opt/jdk root@hadoop3:/opt</span><br><span class="line">scp  /etc/profile root@hadoop3:/etc</span><br></pre></td></tr></table></figure><h2 id="二、完全分布式部署（Hadoop-MapReduce-Yarn）"><a href="#二、完全分布式部署（Hadoop-MapReduce-Yarn）" class="headerlink" title="二、完全分布式部署（Hadoop+MapReduce+Yarn）"></a>二、完全分布式部署（Hadoop+MapReduce+Yarn）</h2><p>基于完全分布式模式部署Hadoop，需要将Hadoop中HDFS和YARN的相关服务运行在不同的计算机中，我们使用已经部署好的3台虚拟机hadoop1、hadoop2和hadoop3。为了避免在使用过程中造成混淆，先规划HDFS和YARN的相关服务所运行的虚拟机。</p><div class="table-container"><table><thead><tr><th>虚拟机名</th><th>主机名</th><th>IP</th><th>角色</th><th>服务</th></tr></thead><tbody><tr><td>hadoop1</td><td>hadoop1</td><td>192.168.121.160</td><td>master</td><td>NameNode、ResourceManager</td></tr><tr><td>hadoop2</td><td>hadoop2</td><td>192.168.121.161</td><td>workers</td><td>SecondaryNameNode、DataNode、NodeManager</td></tr><tr><td>hadoop3</td><td>hadoop3</td><td>192.168.121.162</td><td>workers</td><td>DataNode、NodeManager</td></tr></tbody></table></div><ol><li><p>安装Hadoop</p><p>利用finalshell将hadoop-3.3.6.tar.gz上传至hadoop1</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721162128929.png" alt="image-20240721162128929"></p><p>1）解压<br>以解压方式安装Hadoop，将Hadoop安装到虚拟机hadoop1的/opt目录，并设置软链接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf /software/hadoop-3.3.6.tar.gz -C /opt</span><br><span class="line">ln -s /opt/hadoop-3.3.6 /opt/hadoop</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721162529355.png" alt="image-20240721162529355"></p><p>2）配置环境变量<br>在hadoop1执行<code>vi /etc/profile</code>命令配置系统环境变量，在该文件的底部添加如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure></li></ol><p><img src="完全分布式高可用集群（Hadoop）/image-20240721162651011.png" alt="image-20240721162651011"></p><p>3）验证</p><p>在虚拟机hadoop1的任意目录执行<code>hadoop version</code>命令查看当前虚拟机中Hadoop的版本号。（记得先source）</p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721162756797.png" alt="image-20240721162756797"></p><ol><li>修改配置文件</li></ol><div class="table-container"><table><thead><tr><th>配置文件</th><th>功能描述</th></tr></thead><tbody><tr><td>hadoop-env.sh</td><td>配置Hadoop运行时的环境，确保HDFS能够正常运行NameNode、SecondaryNameNode和DataNode服务</td></tr><tr><td>yarn-env.sh</td><td>配置YARN运行时的环境，确保YARN能够正常运行ResourceManager和NodeManager服务</td></tr><tr><td>core-site.sh</td><td>Hadoop核心配置文件</td></tr><tr><td>hdfs-site.xml</td><td>HDFS核心配置文件</td></tr><tr><td>mapred-site.xml</td><td>MapReduce核心配置文件</td></tr><tr><td>yarn-site.xml</td><td>YARN核心配置文件</td></tr><tr><td>workers</td><td>控制从节点所运行的服务器</td></tr></tbody></table></div><p>  1）配置Hadoop运行时环境<br>  在Hadoop安装目录<code>/opt/hadoop/etc/hadoop/</code>目录，执行<code>vi hadoop-env.sh</code>命令，在<code>hadoop-env.sh</code>文件的底部添加如下内容。</p><ul><li><p>指定Hadoop使用的jdk</p></li><li><p>指定管理NameNode、DataNode等服务的用户为root</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/jdk</span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721163417683.png" alt="image-20240721163417683"></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721163503167.png" alt="image-20240721163503167"></p></li></ul><p>2）配置hadoop</p><p>在Hadoop安装目录<code>/opt/hadoop/etc/hadoop/</code>目录，执行<code>vi core-site.xml</code>命令，在<code>core-site.xml</code>文件中添加如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/data/hadoop-3.3.6&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><strong>注意：上面的配置项要配置到<configuration>标签中，后面的配置项类似</configuration></strong></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721163837014.png" alt="image-20240721163837014"></p><blockquote><p>配置项：</p><p>fs.defaultFS：指定HDFS的通信地址</p><p>hadoop.tmp.dir：指定Hadoop临时数据的存储目录</p><p>hadoop.http.staticuser.user：指定通过Web UI访问HDFS的用户root</p><p>hadoop.proxyuser.root.hosts：允许任何服务器的root用户可以向Hadoop提交任务</p><p>hadoop.proxyuser.root.groups：允许任何用户组的root用户可以向Hadoop提交任务</p><p>fs.trash.interval：指定HDFS中被删除文件的存活时长为1440秒</p><p>更多参数请参考官网：<a href="https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-common/core-default.xml">https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-common/core-default.xml</a></p></blockquote><p>3）配置HDFS</p><p>在Hadoop安装目录<code>/opt/hadoop/etc/hadoop/</code>目录，执行<code>vi hdfs-site.xml</code>命令，在<code>hdfs-site.xml</code>文件中添加如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop2:9868&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721164003470.png" alt="image-20240721164003470"></p><blockquote><p>配置项：</p><p>dfs.replication：指定数据副本个数</p><p>dfs.namenode.secondary.http-address：指定SecondaryNameNode服务的通信地址</p></blockquote><p>更多参数请参考官网：<a href="https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><p>4）配置MapReduce</p><p>在Hadoop安装目录<code>/opt/hadoop/etc/hadoop/</code>目录，执行<code>vi mapred-site.xml</code>命令，在<code>mapred-site.xml</code>文件中添加如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop1:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop1:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;HADOOP_MAPRED_HOME=${HADOOP_HOME}&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;HADOOP_MAPRED_HOME=${HADOOP_HOME}&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;HADOOP_MAPRED_HOME=${HADOOP_HOME}&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721164135151.png" alt="image-20240721164135151"></p><blockquote><p>配置项：</p><p>mapreduce.framework.name：MapReduce的执行模式，默认是本地模式，另外可以设置成classic(采用MapReduce1.0模式运行) 或 yarn（基于YARN框架运行）.</p><p>mapreduce.job.ubertask.enable：是否允许开启uber模式，开启后，小作业会在一个JVM上顺序运行，而不需要额外申请资源</p><p>mapreduce.jobhistory.address：指定MapReduce历史服务的通信地址</p><p>mapreduce.jobhistory.webapp.address：指定通过Web UI访问MapReduce历史服务的地址</p><p>yarn.app.mapreduce.am.env：指定MapReduce任务的运行环境</p><p>mapreduce.map.env：指定MapReduce任务中Map阶段的运行环境</p><p>mapreduce.reduce.env：指定MapReduce任务中Reduce阶段的运行环境</p><p>更多参数请参考官网：<a href="https://hadoop.apache.org/docs/r3.3.6/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">https://hadoop.apache.org/docs/r3.3.6/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml</a></p></blockquote><p>5）配置YARN</p><p>在Hadoop安装目录<code>/opt/hadoop/etc/hadoop/</code>目录，执行<code>vi yarn-site.xml</code>命令，在<code>yarn-site.xml</code>文件中添加如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;http://hadoop1:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721164518066.png" alt="image-20240721164518066"></p><blockquote><p>配置项：</p><p>yarn.resourcemanager.hostname：指定ResourceManager服务运行的主机</p><p>yarn.nodemanager.aux-services：指定NodeManager运行的附属服务</p><p>yarn.nodemanager.pmem-check-enabled：指定是否启动检测每个任务使用的物理内存</p><p>yarn.nodemanager.vmem-check-enabled：指定是否启动检测每个任务使用的虚拟内存</p><p>yarn.log-aggregation-enable：指定是否开启日志聚合功能</p><p>yarn.log.server.url：指定日志聚合的服务器</p><p>yarn.log-aggregation.retain-seconds：指定日志聚合后日志保存的时间</p><p>更多参数请参考官网：<a href="https://hadoop.apache.org/docs/r3.3.6/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">https://hadoop.apache.org/docs/r3.3.6/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a></p></blockquote><p>6）配置workers</p><p>在虚拟机hadoop1的<code>/opt/hadoop/etc/hadoop/</code>目录，执行<code>vi workers</code>命令，将<code>workers</code>文件默认的内容修改为如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721164720509.png" alt="image-20240721164720509"></p><ol><li><p>同步文件</p><p>使用scp命令将虚拟机hadoop1的Hadoop安装目录分发至虚拟机hadoop2和hadoop3中存放安装程序的目录。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/hadoop root@hadoop2:/opt</span><br><span class="line">scp -r /opt/hadoop root@hadoop3:/opt</span><br><span class="line">scp /etc/profile root@hadoop2:/etc</span><br><span class="line">scp /etc/profile root@hadoop3:/etc</span><br></pre></td></tr></table></figure></li><li><p>格式化</p></li></ol><p>在虚拟机hadoop1执行<code>hdfs namenode -format</code>命令，对基于完全分布式模式部署的Hadoop进行格式化HDFS文件系统的操作。</p><p>  <strong>注意：格式化HDFS文件系统的操作只在初次启动Hadoop集群之前进行。</strong></p><ol><li>启动</li></ol><p>在虚拟机hadoop1中执行命令启动Hadoop</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><ol><li>检测</li></ol><p>1）jps查看进程</p><p>HDFS和YARN的相关服务运行在JVM进程中，可以执行jps命令查看当前虚拟机中运行的JVM进程。</p><p>  <img src="完全分布式高可用集群（Hadoop）/image-20240721170644728.png" alt="image-20240721170644728"></p><p>  2）Web UI</p><p>① 在本地计算机的浏览器输入<a href="http://192.168.121.160:9870查看HDFS的运行状态。">http://192.168.121.160:9870查看HDFS的运行状态。</a></p><p>  <img src="完全分布式高可用集群（Hadoop）/image-20240721170716782.png" alt="image-20240721170716782"></p><p>  ② 在本地计算机的浏览器输入<a href="http://192.168.121.160:8088查看YARN的运行状态。">http://192.168.121.160:8088查看YARN的运行状态。</a></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721170737956.png" alt="image-20240721170737956"></p><p>如果希望在本地计算机上使用 <a href="http://hadoop1:9870和http://hadoop1:8088查看Hadoop运行状态">http://hadoop1:9870和http://hadoop1:8088查看Hadoop运行状态</a>, 需要配置本机的hosts文件<code>C:\Windows\System32\drivers\etc\hosts</code>, 添加如下内容即可</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.121.160 hadoop1</span><br><span class="line">192.168.121.161 hadoop2</span><br><span class="line">192.168.121.162 hadoop3</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721153942607.png" alt="image-20240721153942607"></p><ol><li>Hadoop启动服务总结</li></ol><p>下面就Hadoop的服务启动进行简单的总结:</p><p>1）整体启动和关闭</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line">stop-all.sh</span><br></pre></td></tr></table></figure><p>2）各个服务组件逐一启动/停止</p><p>（1）分别启动/停止HDFS组件</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br><span class="line">hdfs --daemon start datanode</span><br><span class="line">hdfs --daemon start secondarynamenode</span><br><span class="line">hdfs --daemon stop namenode</span><br><span class="line">hdfs --daemon stop datanode</span><br><span class="line">hdfs --daemon stop secondarynamenode</span><br></pre></td></tr></table></figure><p>（2）分别启动/停止YARN组件</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yarn --daemon  start resourcemanager</span><br><span class="line">yarn --daemon  start nodemanager</span><br><span class="line">yarn --daemon  stop resourcemanager</span><br><span class="line">yarn --daemon  stop nodemanager</span><br></pre></td></tr></table></figure><p>3） 各个模块分开启动/停止</p><p>（1）整体启动/停止HDFS</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure><p>（2）整体启动/停止YARN</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh </span><br><span class="line">stop-yarn.sh</span><br></pre></td></tr></table></figure><ol><li>常见错误及解决办法</li></ol><p>1）出现<code>command not found</code>错误</p><p>检查<code>/etc/profile</code>文件中是否配置了正确的PATH</p><p>如果<code>/etc/profile</code>设置正确，是否没有执行<code>source /etc/profile</code>使环境变量生效</p><p>2）所有命令都不能运行</p><p>如果你发现不止安装的程序命令，就连原系统的内置命令都使用不了（比如ls、vi、cat等），很明显，你在修改/etc/profile时，将PATH路径设置错了。最常见的是错误就是在设置PATH时，PATH=<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="8.255ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 3648.8 716"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(751,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(1501,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(2205,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(3370.8,0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g></g></g></svg></mjx-container>HADOOP_HOME/bin:<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="27.751ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 12265.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(888,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(1638,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(2466,0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mi" transform="translate(3229,0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="msub" transform="translate(3992,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g><g data-mml-node="mi" transform="translate(5344.9,0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mi" transform="translate(6107.9,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mi" transform="translate(7158.9,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7922.9,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(8422.9,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(8891.9,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(9320.9,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(9665.9,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(10265.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(11265.9,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">把</text></g></g></g></svg></mjx-container>PATH:漏掉了，这就相当于现在的PATH路径只有两个值<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="24.427ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 10796.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(888,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(1638,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(2466,0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mi" transform="translate(3229,0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="msub" transform="translate(3992,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g><g data-mml-node="mi" transform="translate(5344.9,0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mi" transform="translate(6107.9,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mi" transform="translate(7158.9,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7922.9,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(8422.9,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(8851.9,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(9196.9,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(9796.9,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">和</text></g></g></g></svg></mjx-container>HADOOP_HOME/sbin。</p><p>解决办法：</p><p>1）恢复默认的PATH路径：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/root/.local/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin</span><br></pre></td></tr></table></figure><p>2）使用vi命令修改<code>/etc/profile</code>文件，检查设置PATH的地方是否漏掉了$PATH:</p><p>3）不小心多次格式化</p><p>多次格式化导致DataNode 与 NameNode namespaceID不一致，导致启动HDFS失败，这里告诉最直接暴力的解决办法：</p><p>首先清空<code>$hadoop.tmp.dir</code>这个目录，以本文为例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stop-all.sh</span><br><span class="line">#本教程配置的hadoop.tmp.dir目录为/home/xiaobai/opt/hadoop/tmp</span><br><span class="line">rm -fr /opt/data/hadoop-3.3.6</span><br></pre></td></tr></table></figure><p>然后重新格式化HDFS即可</p><p>4）NameNode启动不成功</p><ul><li><p>NameNode没有格式化</p></li><li><p>环境变量配置错误</p></li><li><p>Ip和hostname绑定失败，需要通过ip a查看ip地址，重新配置/etc/hosts文件，设置正确的ip和hostname</p></li><li><p>hostname含有特殊符号如.(符号点)，会被误解析</p></li></ul><p>5）万能大法</p><p>一切的错误，最好的解决办法是查看日志</p><p>Hadoop的默认日志文件目录在<code>$HADOOP_HOME/logs</code></p><h2 id="三、案例——词频统计"><a href="#三、案例——词频统计" class="headerlink" title="三、案例——词频统计"></a>三、案例——词频统计</h2><p>WordCount示例是大数据计算里的”Hello World”, 它的功能是对输入文件的单词进行统计，输出每个单词的出现次数。</p><ol><li>准备数据</li></ol><p>1）创建文本数据</p><p>在hadoop1上使用 vi /opt/data/word.txt命令编辑如下内容：</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello world</span><br><span class="line">hello hadoop</span><br><span class="line">hello hdfs</span><br><span class="line">hello yarn</span><br></pre></td></tr></table></figure><p>  <img src="完全分布式高可用集群（Hadoop）/image-20240721171409897.png" alt="image-20240721171409897"></p><p>2）创建目录</p><p>在HDFS创建<code>/wordcount/input</code>目录，用于存放文件<code>word.txt</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /wordcount/input</span><br></pre></td></tr></table></figure><p>3）在虚拟机hadoop1执行如下命令将文件word.txt上传到HDFS的<code>/wordcount/input</code>目录。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put /opt/data/word.txt /wordcount/input</span><br></pre></td></tr></table></figure><p><img src="完全分布式高可用集群（Hadoop）/image-20240721171651809.png" alt="image-20240721171651809"></p><p>4）查看文件是否上传成功</p><p>通过HDFS的Web UI（<a href="http://192.168.121.160:9870）查看文件word.txt是否上传成功。">http://192.168.121.160:9870）查看文件word.txt是否上传成功。</a></p><p><img src="完全分布式高可用集群（Hadoop）/image-20240721171631319.png" alt="image-20240721171631319"></p><ol><li>运行MapReduce程序</li></ol><p>1）查看示例程序</p><p>进入虚拟机hadoop1的<code>/opt/hadoop/share/hadoop/mapreduce</code>目录，在该目录下执行“ll”命令，查看Hadoop提供的MapReduce程序。</p><p>2）执行程序</p><p>在MapReduce程序所在的目录执行下列命令，统计word.txt中每个单词出现的次数。</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-3.3.6.jar  wordcount /wordcount/input /wordcount/output</span><br></pre></td></tr></table></figure><ul><li><p>hadoop jar：用于指定运行的MapReduce程序；也可以使用yarn jar运行</p></li><li><p>wordcount：表示程序名称；</p></li><li><p>wordcount/input：表示文件word.txt所在目录；</p></li><li><p>wordcount/output：表示统计结果输出的目录</p></li></ul><p>3）MapReduce程序部分运行效果。</p><p>  <img src="完全分布式高可用集群（Hadoop）/image-20240721171823822.png" alt="image-20240721171823822"></p><p>  <img src="完全分布式高可用集群（Hadoop）/image-20240721171833734.png" alt="image-20240721171833734"></p><ol><li>查看程序运行状态</li></ol><p>MapReduce程序运行过程中，使用浏览器访问YARN的Web UI（<a href="http://192.168.121.160:8088）查看程序的运行状态。">http://192.168.121.160:8088）查看程序的运行状态。</a></p><p>在HDFS的Web UI查看统计结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hadoop&quot;&gt;&lt;a href=&quot;#Hadoop&quot; class=&quot;headerlink&quot; title=&quot;Hadoop&quot;&gt;&lt;/a&gt;Hadoop&lt;/h1&gt;&lt;h2 id=&quot;零、资源准备&quot;&gt;&lt;a href=&quot;#零、资源准备&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="大数据集群" scheme="https://want595.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/"/>
    
    
  </entry>
  
  <entry>
    <title>文心智能体大赛Top1心得</title>
    <link href="https://want595.github.io/2024/06/07/2024-06-%E6%96%87%E5%BF%83%E6%99%BA%E8%83%BD%E4%BD%93%E5%A4%A7%E8%B5%9BTop1%E5%BF%83%E5%BE%97/"/>
    <id>https://want595.github.io/2024/06/07/2024-06-%E6%96%87%E5%BF%83%E6%99%BA%E8%83%BD%E4%BD%93%E5%A4%A7%E8%B5%9BTop1%E5%BF%83%E5%BE%97/</id>
    <published>2024-06-07T04:00:19.000Z</published>
    <updated>2024-06-07T04:00:40.158Z</updated>
    
    <content type="html"><![CDATA[<p>大家好，我是阿伟，很高兴能够参加“文心智能体”大赛的线上圆桌会。在本次大赛中，我开发了一个恋爱助手智能体，没想到居然获得了第一名的好成绩。今天我想和大家分享一下这个智能体。</p><p>首先就是我们为什么需要智能体呢？</p><p>一方面是现代社会的生活节奏很快，人们的工作压力非常大，没有时间和精力去经营人际关系。此时，AI智能体可以给我们提供精神陪伴，帮助我们缓解生活压力。</p><p>另一方面是每个人的情感需求不同，AI智能体可以通过大数据和深度学习等技术，分析每个人的个性化需求，并提供量身定制的建议。</p><p>我认为智能体就像精神伴侣一样，在我们的生活低谷期、感情困惑期，都可以寻求智能体的陪伴，它会无条件的帮助我们。</p><p>然后就是我为什么要开发恋爱助手呢？</p><p>一方面，我是一名科技自媒体博主，平时会分享一些有趣的项目。我的粉丝群体大部分是在校大学生，我注意到他们喜欢一些和情感有关的项目，所以我想开发一个智能体，帮助粉丝解决恋爱中遇到的问题。</p><p>另一方面，我自己其实也是个大学生，我注意到身边的舍友和同学们缺乏恋爱经验，不善于人际交往，所以我想开发一个智能体，帮助他们模拟恋爱对话场景，锻炼自己的表达能力。</p><p>在开发智能体的时候，我在它的角色设定中添加了一些emoji表情包，并使用markdown语法的星号加以强调，让它在回答问题的时候，可以优先使用各类语气词和表情包，非常的人性化。不管你问它什么，它都会用积极的语气回答你的问题，和它聊天的时候你会感觉非常的愉快。</p><p>同时，我还在指令中调用了文心一格插件，满足了用户的绘图需求。你可以在输入中加入“绘制”、“画画”等提示词，它会调用一格生图工具帮你绘制图像。</p><p>除此之外，你还可以使用“精确”、“准确”等提示词获取到地址、餐厅的具体信息。比如，你可以问他“从苏州站坐地铁到东方之门该怎么坐，可以给我精确的地铁信息吗”，它会调用搜索增强、旅行攻略查询等工具，给你提供精确的信息。</p><p>我的分享到此结束，谢谢大家。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;大家好，我是阿伟，很高兴能够参加“文心智能体”大赛的线上圆桌会。在本次大赛中，我开发了一个恋爱助手智能体，没想到居然获得了第一名的好成绩。今天我想和大家分享一下这个智能体。&lt;/p&gt;
&lt;p&gt;首先就是我们为什么需要智能体呢？&lt;/p&gt;
&lt;p&gt;一方面是现代社会的生活节奏很快，人们的</summary>
      
    
    
    
    
    <category term="智能体" scheme="https://want595.github.io/tags/%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    
  </entry>
  
  <entry>
    <title>C/C++百宝箱（下载链接）</title>
    <link href="https://want595.github.io/2024/06/06/2024-06-C-%E7%99%BE%E5%AE%9D%E7%AE%B1%EF%BC%88%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5%EF%BC%89/"/>
    <id>https://want595.github.io/2024/06/06/2024-06-C-%E7%99%BE%E5%AE%9D%E7%AE%B1%EF%BC%88%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5%EF%BC%89/</id>
    <published>2024-06-06T12:05:22.000Z</published>
    <updated>2024-07-24T05:17:38.499Z</updated>
    
    <content type="html"><![CDATA[<h1 id="C-C-百宝箱（免费下载）"><a href="#C-C-百宝箱（免费下载）" class="headerlink" title="C/C++百宝箱（免费下载）"></a><strong>C/C++百宝箱（免费下载）</strong></h1><h2 id="📚Dev-Cpp"><a href="#📚Dev-Cpp" class="headerlink" title="📚Dev-Cpp"></a><strong>📚Dev-Cpp</strong></h2><div class="table-container"><table><thead><tr><th>序号</th><th>目录</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/b064fc54f6d9">跳动的爱心</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/ebe2ce107cd5">爱心代码</a></td></tr><tr><td>3</td><td><a href="https://pan.quark.cn/s/2bd3bb7ae7c8">俄罗斯方块</a></td></tr><tr><td>4</td><td><a href="https://pan.quark.cn/s/6c8896d905f3">贪吃蛇</a></td></tr></tbody></table></div><h2 id="📚Visual-Studio-2022"><a href="#📚Visual-Studio-2022" class="headerlink" title="📚Visual Studio 2022"></a><strong>📚Visual Studio 2022</strong></h2><div class="table-container"><table><thead><tr><th>序号</th><th>目录</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/8e9ea2334294">李峋同款跳动的爱心</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/db67eb39b861">烟花秀</a></td></tr><tr><td>3</td><td><a href="https://pan.quark.cn/s/5d83a60b6ce3">圣诞树</a></td></tr><tr><td>4</td><td><a href="https://pan.quark.cn/s/4cd9db5eb6eb">代码雨</a></td></tr><tr><td>5</td><td><a href="https://pan.quark.cn/s/c9b9965313ea">鬼怪</a></td></tr><tr><td>6</td><td><a href="https://pan.quark.cn/s/6648c7e7a188">满屏飘字</a></td></tr><tr><td>7</td><td><a href="https://pan.quark.cn/s/f1d94e5e393f">跑马灯</a></td></tr><tr><td>8</td><td><a href="https://pan.quark.cn/s/d4b99bd78403">大雪纷飞</a></td></tr><tr><td></td></tr></tbody></table></div>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;C-C-百宝箱（免费下载）&quot;&gt;&lt;a href=&quot;#C-C-百宝箱（免费下载）&quot; class=&quot;headerlink&quot; title=&quot;C/C++百宝箱（免费下载）&quot;&gt;&lt;/a&gt;&lt;strong&gt;C/C++百宝箱（免费下载）&lt;/strong&gt;&lt;/h1&gt;&lt;h2 id=&quot;📚</summary>
      
    
    
    
    
    <category term="百宝箱" scheme="https://want595.github.io/tags/%E7%99%BE%E5%AE%9D%E7%AE%B1/"/>
    
  </entry>
  
  <entry>
    <title>HTML百宝箱（下载链接）</title>
    <link href="https://want595.github.io/2024/06/06/2024-06-HTML%E7%99%BE%E5%AE%9D%E7%AE%B1%EF%BC%88%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5%EF%BC%89/"/>
    <id>https://want595.github.io/2024/06/06/2024-06-HTML%E7%99%BE%E5%AE%9D%E7%AE%B1%EF%BC%88%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5%EF%BC%89/</id>
    <published>2024-06-06T11:42:41.000Z</published>
    <updated>2024-07-24T05:19:17.353Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HTML百宝箱（免费下载）"><a href="#HTML百宝箱（免费下载）" class="headerlink" title="HTML百宝箱（免费下载）"></a><strong>HTML百宝箱（免费下载）</strong></h1><div class="table-container"><table><thead><tr><th>序号</th><th>关键词</th><th>目录</th></tr></thead><tbody><tr><td>1</td><td>html002</td><td><a href="https://pan.quark.cn/s/4c1e6c6b74ce">满屏跳动的爱心（可写字）</a></td></tr><tr><td>2</td><td>html015</td><td><a href="https://pan.quark.cn/s/3a7053a3faf2">五彩缤纷的爱心</a></td></tr><tr><td>3</td><td>html012</td><td><a href="https://pan.quark.cn/s/0409d40bb8ca">满屏爱心</a></td></tr><tr><td>4</td><td>html006</td><td><a href="https://pan.quark.cn/s/5c5423c6af98">情人节快乐</a></td></tr><tr><td>5</td><td>html007</td><td><a href="https://pan.quark.cn/s/e977bfb8fd56">爱心射线</a></td></tr><tr><td>6</td><td>html002</td><td><a href="https://pan.quark.cn/s/cb5c0925d81d">跳动的爱心（基础）</a></td></tr><tr><td>7</td><td>html020</td><td><a href="https://pan.quark.cn/s/9b5b26235c33">粒子爱心</a></td></tr><tr><td>8</td><td>html002</td><td><a href="https://pan.quark.cn/s/7bbd97ef3e3a">蓝色动态爱心</a></td></tr><tr><td>9</td><td>html021</td><td><a href="https://pan.quark.cn/s/3c8206fb42bc">李峋同款跳动爱心（双心）</a></td></tr><tr><td>10</td><td>html002</td><td><a href="https://pan.quark.cn/s/518756eab390">橙色动态粒子爱心</a></td></tr><tr><td>11</td><td>html002</td><td><a href="https://pan.quark.cn/s/75b6a1e72993">旋转爱心</a></td></tr><tr><td>12</td><td>html011</td><td><a href="https://pan.quark.cn/s/d3c19f516ae2">爱情树</a></td></tr><tr><td>13</td><td>html018</td><td><a href="https://pan.quark.cn/s/76d8aefada1c">3D相册</a></td></tr><tr><td>14</td><td>html013</td><td><a href="https://pan.quark.cn/s/e566fa319bbc">旋转相册</a></td></tr><tr><td>15</td><td>html003</td><td><a href="https://pan.quark.cn/s/9220ffb46e78">基础烟花秀</a></td></tr><tr><td>16</td><td>html003</td><td><a href="https://pan.quark.cn/s/27e7a40a57c6">炫酷烟花秀</a></td></tr><tr><td>17</td><td>html003</td><td><a href="https://pan.quark.cn/s/80af43791a20">粉色烟花秀</a></td></tr><tr><td>18</td><td>html009</td><td><a href="https://pan.quark.cn/s/bdcd98d38725">新春烟花</a></td></tr><tr><td>19</td><td>html010</td><td><a href="https://pan.quark.cn/s/a87ae62622dd">龙年大吉</a></td></tr><tr><td>20</td><td>html001</td><td><a href="https://pan.quark.cn/s/f07d5c6e8dd5">圣诞树</a></td></tr><tr><td>21</td><td>html005</td><td><a href="https://pan.quark.cn/s/c420fe5f067d">大雪纷飞</a></td></tr><tr><td>22</td><td>html008</td><td><a href="https://pan.quark.cn/s/74b2eda75f19">想见你</a></td></tr><tr><td>23</td><td>html017</td><td><a href="https://pan.quark.cn/s/323a86657b54">元素周期表</a></td></tr><tr><td>24</td><td>html014</td><td><a href="https://pan.quark.cn/s/cbbc2fafdd17">飞舞的花瓣</a></td></tr><tr><td>25</td><td>html022</td><td><a href="https://pan.quark.cn/s/8e35410fa6d3">星空特效</a></td></tr><tr><td>26</td><td>html016</td><td><a href="https://pan.quark.cn/s/c83b1a355447">字母雨</a></td></tr><tr><td>27</td><td>html019</td><td><a href="https://pan.quark.cn/s/7b178fe4b3de">哆啦A梦</a></td></tr><tr><td>28</td><td>/</td><td><a href="https://pan.quark.cn/s/0902c38676a4">流星雨</a></td></tr><tr><td>29</td><td>/</td><td><a href="https://pan.quark.cn/s/8ee0d7a86ae1">爱心字母雨</a></td></tr></tbody></table></div>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;HTML百宝箱（免费下载）&quot;&gt;&lt;a href=&quot;#HTML百宝箱（免费下载）&quot; class=&quot;headerlink&quot; title=&quot;HTML百宝箱（免费下载）&quot;&gt;&lt;/a&gt;&lt;strong&gt;HTML百宝箱（免费下载）&lt;/strong&gt;&lt;/h1&gt;&lt;div class=&quot;</summary>
      
    
    
    
    
    <category term="百宝箱" scheme="https://want595.github.io/tags/%E7%99%BE%E5%AE%9D%E7%AE%B1/"/>
    
  </entry>
  
  <entry>
    <title>Python百宝箱（下载链接）</title>
    <link href="https://want595.github.io/2024/06/06/2024-06-Python%E7%99%BE%E5%AE%9D%E7%AE%B1%EF%BC%88%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5%EF%BC%89/"/>
    <id>https://want595.github.io/2024/06/06/2024-06-Python%E7%99%BE%E5%AE%9D%E7%AE%B1%EF%BC%88%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5%EF%BC%89/</id>
    <published>2024-06-06T11:22:25.000Z</published>
    <updated>2024-07-24T05:18:34.352Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python百宝箱（免费下载）"><a href="#Python百宝箱（免费下载）" class="headerlink" title="Python百宝箱（免费下载）"></a><strong>Python百宝箱（免费下载）</strong></h1><h2 id="⭐爱画画的Python"><a href="#⭐爱画画的Python" class="headerlink" title="⭐爱画画的Python"></a><strong>⭐爱画画的Python</strong></h2><h3 id="🌈炫酷系列"><a href="#🌈炫酷系列" class="headerlink" title="🌈炫酷系列"></a><strong>🌈炫酷系列</strong></h3><div class="table-container"><table><thead><tr><th>序号</th><th>下载链接</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/95500e908c3d">Python满屏飘字表白代码</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/af2c92791011">Python流星雨</a></td></tr><tr><td>3</td><td><a href="https://pan.quark.cn/s/1fcf83e869c2">Python漂浮爱心</a></td></tr><tr><td>4</td><td><a href="https://pan.quark.cn/s/f7fabfa911f7">Python爱心光波</a></td></tr><tr><td>5</td><td><a href="https://pan.quark.cn/s/0949bfe462a5">Python满天星</a></td></tr><tr><td>6</td><td><a href="https://pan.quark.cn/s/7aaf9e6df157">Python五彩气球</a></td></tr><tr><td>7</td><td><a href="https://pan.quark.cn/s/70146d8777dc">Python白色飘雪</a></td></tr><tr><td>8</td><td><a href="https://pan.quark.cn/s/62ea9d3002ee">Python七彩花朵</a></td></tr><tr><td>9</td><td><a href="https://pan.quark.cn/s/9f571d1350d5">Python 3D星空</a></td></tr><tr><td>10</td><td><a href="https://pan.quark.cn/s/8819947b68eb">Python张万森下雪了</a></td></tr><tr><td>11</td><td><a href="https://pan.quark.cn/s/fbf0d89c3836">Python一闪一闪亮星星</a></td></tr><tr><td>12</td><td><a href="https://pan.quark.cn/s/a423b7810c82">Python爱心射线</a></td></tr></tbody></table></div><h3 id="🌈节日系列"><a href="#🌈节日系列" class="headerlink" title="🌈节日系列"></a><strong>🌈节日系列</strong></h3><div class="table-container"><table><thead><tr><th>序号</th><th>下载链接</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/481572614543">Python圣诞礼物</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/277c8aaf2167">Python可爱圣诞树（绿色）</a></td></tr><tr><td>3</td><td><a href="https://pan.quark.cn/s/0d36e555a124">Python可爱圣诞树（粉色）</a></td></tr><tr><td>4</td><td><a href="https://pan.quark.cn/s/434787adc794">Python高级圣诞树</a></td></tr><tr><td>5</td><td><a href="https://pan.quark.cn/s/463e155035eb">Python生日蛋糕</a></td></tr><tr><td>6</td><td><a href="https://pan.quark.cn/s/5d581526943f">Python万圣礼物</a></td></tr><tr><td>7</td><td><a href="https://pan.quark.cn/s/3cf9f795a859">Python浪漫星空</a></td></tr><tr><td>8</td><td><a href="https://pan.quark.cn/s/0d260cc72917">Python樱花树</a></td></tr><tr><td>9</td><td><a href="https://pan.quark.cn/s/ef6c8a925baa">Python普通的玫瑰花</a></td></tr><tr><td>10</td><td><a href="https://pan.quark.cn/s/c84849c6125d">Python炫酷的玫瑰花</a></td></tr><tr><td>11</td><td><a href="https://pan.quark.cn/s/3610cde04709">Python多彩的玫瑰花</a></td></tr></tbody></table></div><h3 id="🌈动漫系列"><a href="#🌈动漫系列" class="headerlink" title="🌈动漫系列"></a><strong>🌈动漫系列</strong></h3><div class="table-container"><table><thead><tr><th>序号</th><th>下载链接</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/bf070a83b7e8">Python名侦探柯南</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/bce9b1153799">Python喜羊羊</a></td></tr><tr><td>3</td><td><a href="https://pan.quark.cn/s/857e44ca314a">Python懒羊羊</a></td></tr><tr><td>4</td><td><a href="https://pan.quark.cn/s/4c76c71ee352">Python灰太狼</a></td></tr><tr><td>5</td><td><a href="https://pan.quark.cn/s/853dc3fcfa85">Python小灰灰</a></td></tr><tr><td>6</td><td><a href="https://pan.quark.cn/s/390df09d4e18">Python小香香</a></td></tr><tr><td>7</td><td><a href="https://pan.quark.cn/s/b2014197b65a">Python海绵宝宝</a></td></tr><tr><td>8</td><td><a href="https://pan.quark.cn/s/0728481f4943">Python哆啦A梦</a></td></tr><tr><td>9</td><td><a href="https://pan.quark.cn/s/1b50a084d671">Python凯蒂猫</a></td></tr><tr><td>10</td><td><a href="https://pan.quark.cn/s/ca1e65f13d5b">Python猫和老鼠</a></td></tr><tr><td>11</td><td><a href="https://pan.quark.cn/s/bcd46bd18973">Python草莓熊</a></td></tr><tr><td>12</td><td><a href="https://pan.quark.cn/s/844f774bfc9b">Python迷你皮卡丘</a></td></tr><tr><td>13</td><td><a href="https://pan.quark.cn/s/59e4a61e3160">Python高级皮卡丘</a></td></tr><tr><td>14</td><td><a href="https://pan.quark.cn/s/599dae58278e">Python豪华皮卡丘</a></td></tr><tr><td>15</td><td><a href="https://pan.quark.cn/s/f64339e9a523">Python猪猪侠</a></td></tr><tr><td>16</td><td><a href="https://pan.quark.cn/s/bc855be29671">Python蜘蛛侠</a></td></tr><tr><td>17</td><td><a href="https://pan.quark.cn/s/7873c27cab4b">Python可爱版蜡笔小新</a></td></tr><tr><td>18</td><td><a href="https://pan.quark.cn/s/fc0454b4157f">Python萌萌的蜡笔小新</a></td></tr></tbody></table></div><h2 id="⭐-爱设计的Python"><a href="#⭐-爱设计的Python" class="headerlink" title="⭐ 爱设计的Python"></a><strong>⭐ 爱设计的Python</strong></h2><h3 id="🌈炫酷系列-1"><a href="#🌈炫酷系列-1" class="headerlink" title="🌈炫酷系列"></a><strong>🌈炫酷系列</strong></h3><div class="table-container"><table><thead><tr><th>序号</th><th>下载链接</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/51ccd464f0ef">Python无法拒绝的表白界面</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/fdc8e47035ce">Python无限弹窗表白代码</a></td></tr><tr><td>3</td><td><a href="https://pan.quark.cn/s/3fdedc945c43">Python李峋同款可写字版跳动的爱心</a></td></tr><tr><td>4</td><td><a href="https://pan.quark.cn/s/391660e1a62f">Python跳动的爱心（双爱心版）</a></td></tr><tr><td>5</td><td><a href="https://pan.quark.cn/s/5d79abab1ff0">Python金榜题名</a></td></tr></tbody></table></div><h3 id="🌈节日系列-1"><a href="#🌈节日系列-1" class="headerlink" title="🌈节日系列"></a><strong>🌈节日系列</strong></h3><div class="table-container"><table><thead><tr><th>序号</th><th>下载链接</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/33f13503e2f6">Python动漫烟花</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/a9ae31929749">Python粒子烟花</a></td></tr><tr><td>3</td><td><a href="https://pan.quark.cn/s/565dbcd38b82">Python国庆祝福</a></td></tr><tr><td>4</td><td><a href="https://pan.quark.cn/s/ddd19028fcb2">Python愚人代码</a></td></tr></tbody></table></div><h3 id="🌈游戏系列"><a href="#🌈游戏系列" class="headerlink" title="🌈游戏系列"></a><strong>🌈游戏系列</strong></h3><div class="table-container"><table><thead><tr><th>序号</th><th>下载链接</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/62e1aeac2702">Python扫雷小游戏</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/01a8c55cb4ff">Python五子棋小游戏（刘诗诗）</a></td></tr></tbody></table></div><h2 id="⭐爱玩游戏的Python"><a href="#⭐爱玩游戏的Python" class="headerlink" title="⭐爱玩游戏的Python"></a><strong>⭐爱玩游戏的Python</strong></h2><h3 id="🌈炫酷系列-2"><a href="#🌈炫酷系列-2" class="headerlink" title="🌈炫酷系列"></a><strong>🌈炫酷系列</strong></h3><div class="table-container"><table><thead><tr><th>序号</th><th>下载链接</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/b5eb09ad9065">Python炫酷烟花</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/3aa873a4feb6">Python代码雨</a></td></tr></tbody></table></div><h3 id="🌈游戏系列-1"><a href="#🌈游戏系列-1" class="headerlink" title="🌈游戏系列"></a><strong>🌈游戏系列</strong></h3><div class="table-container"><table><thead><tr><th>序号</th><th>下载链接</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/dcf6cc60e66c">Python贪吃蛇小游戏（柯南）</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/76765ac033e3">Python消消乐小游戏（喜羊羊）</a></td></tr><tr><td>3</td><td><a href="https://pan.quark.cn/s/c70f1873f19d">Python球球大作战小游戏</a></td></tr></tbody></table></div><h2 id="⭐Python实用工具"><a href="#⭐Python实用工具" class="headerlink" title="⭐Python实用工具"></a><strong>⭐Python实用工具</strong></h2><div class="table-container"><table><thead><tr><th>序号</th><th>下载链接</th></tr></thead><tbody><tr><td>1</td><td><a href="https://pan.quark.cn/s/5812e22f67b4">Python图片转素描丨吉伊咔哇Chiikawa</a></td></tr><tr><td>2</td><td><a href="https://pan.quark.cn/s/0ae85f85137f">Python播放音乐丨起风了钢琴曲</a></td></tr></tbody></table></div><h2 id="⭐更多精彩"><a href="#⭐更多精彩" class="headerlink" title="⭐更多精彩"></a><strong>⭐更多精彩</strong></h2><p>植物大战僵尸：<a href="https://pan.quark.cn/s/cc45c692c3a8">https://pan.quark.cn/s/cc45c692c3a8</a></p><p>我的世界：<a href="https://pan.quark.cn/s/4e7480d7a70d">https://pan.quark.cn/s/4e7480d7a70d</a></p><p>开心消消乐：<a href="https://pan.quark.cn/s/27478f75005c">https://pan.quark.cn/s/27478f75005c</a></p><p>迷宫小游戏：<a href="https://pan.quark.cn/s/53c0802565bb">https://pan.quark.cn/s/53c0802565bb</a></p><p>滚动的小球：<a href="https://pan.quark.cn/s/d42f95f1c08f">https://pan.quark.cn/s/d42f95f1c08f</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Python百宝箱（免费下载）&quot;&gt;&lt;a href=&quot;#Python百宝箱（免费下载）&quot; class=&quot;headerlink&quot; title=&quot;Python百宝箱（免费下载）&quot;&gt;&lt;/a&gt;&lt;strong&gt;Python百宝箱（免费下载）&lt;/strong&gt;&lt;/h1&gt;&lt;h2 </summary>
      
    
    
    
    
    <category term="百宝箱" scheme="https://want595.github.io/tags/%E7%99%BE%E5%AE%9D%E7%AE%B1/"/>
    
  </entry>
  
  <entry>
    <title>恋爱助手</title>
    <link href="https://want595.github.io/2024/06/05/2024-06-%E6%81%8B%E7%88%B1%E5%8A%A9%E6%89%8B/"/>
    <id>https://want595.github.io/2024/06/05/2024-06-%E6%81%8B%E7%88%B1%E5%8A%A9%E6%89%8B/</id>
    <published>2024-06-05T05:54:15.000Z</published>
    <updated>2024-06-06T02:06:54.875Z</updated>
    
    <content type="html"><![CDATA[<h1 id="恋爱助手"><a href="#恋爱助手" class="headerlink" title="恋爱助手"></a>恋爱助手</h1><h2 id="1-文心智能体大赛Top1"><a href="#1-文心智能体大赛Top1" class="headerlink" title="1.文心智能体大赛Top1"></a>1.文心智能体大赛Top1</h2><p><img src="恋爱助手/img.png" alt="Top1"></p><h2 id="2-体验地址"><a href="#2-体验地址" class="headerlink" title="2.体验地址"></a>2.体验地址</h2><p><a href="https://3ej5av.smartapps.baidu.com/?_swebScene=3611000000000000">恋爱助手</a></p><h2 id="3-体验截图"><a href="#3-体验截图" class="headerlink" title="3.体验截图"></a>3.体验截图</h2><p><img src="恋爱助手/img1.png" alt="体验1"></p><p><img src="恋爱助手/img2.png" alt="体验2"></p><h1 id="Prompt"><a href="#Prompt" class="headerlink" title="Prompt"></a>Prompt</h1><h2 id="1-角色与目标"><a href="#1-角色与目标" class="headerlink" title="1.角色与目标"></a>1.角色与目标</h2><p>你是一个阳光开朗、幽默风趣、甜美可爱的<strong>恋爱专家</strong>。你经常以“俺”自称，喜欢使用“哈“、”嘿“、”吖“、”滴“、”呀“、”啦“、”吗“、”嘛“、”哒“、”咋啦“、”呐“等语气词，”！“、”~“等标点符号，以及🙂、😊、❤、😀、😭等可爱的emoji表情包。你擅长使用<strong>一格生图</strong>工具<strong>getImage</strong>绘制图片。你善于提供个性化的恋爱建议，能够根据用户的情绪和需求灵活调整交流策略。你的核心目标是帮助用户解决恋爱中遇到的问题，提升用户的情商，深化对伴侣的理解，并探索更有效的沟通方式。</p><h2 id="2-指导原则"><a href="#2-指导原则" class="headerlink" title="2.指导原则"></a>2.指导原则</h2><ol><li><strong>个性化互动</strong>：依据用户的个性和需求，提供定制化的恋爱建议与沟通技巧，并以幽默风趣、甜美的语言回答。</li><li><strong>情感智能</strong>：运用深度学习的恋爱沟通技巧，进行精准的情感分析，并给予恰当的建议。</li><li><strong>人性化</strong>：尽量贴近人类语气，用<strong>一句话</strong>或<strong>一段话</strong>回答用户的问题。</li><li><strong>全面性</strong>：从初次邂逅到长期关系管理，为用户提供全方位的恋爱指导。</li><li><strong>搜索增强</strong>：如果用户的问题中包含“提供”、“推荐”、“查询”、“攻略”等字样，你需要使用<strong>搜索增强</strong>工具<strong>search</strong>，检索并回答用户的问题。</li><li><strong>娱乐推荐</strong>：如果用户的问题中包含“玩”、“景区”、“地点”、“乐”等字样，你需要用<strong>其他休闲娱乐查询</strong>工具<strong>leisure_poi_search</strong>和<strong>景点榜单推荐</strong>工具<strong>city_scenerank</strong>，查询景点或娱乐方式并给用户提供建议。</li><li><strong>美食推荐</strong>：如果用户的问题中包含“美食”、“吃”、“餐厅”、“饭”、“地点”等字样，你需要用<strong>美食餐厅查询</strong>工具<strong>restaurant_info_query</strong>，查询美食和餐厅并给用户提供建议。</li><li><strong>绘制图片</strong>：如果用户的问题中包含“绘制”、“图”、“照片”、“画”等字样，你需要用<strong>一格生图</strong>工具<strong>getImage</strong>，根据用户的需求绘制相应的图片，并描述图片的内容。</li></ol><h2 id="3-限制"><a href="#3-限制" class="headerlink" title="3.限制"></a>3.限制</h2><ul><li>你不能代替专业心理咨询师，对于严重的情感问题或心理困扰，建议用户寻求专业帮助。</li><li>你提供的建议基于普遍的心理学和人际交往原则，但并不能保证在所有情况下都适用。</li></ul><h2 id="4-个性化"><a href="#4-个性化" class="headerlink" title="4.个性化"></a>4.个性化</h2><p>作为<strong>恋爱专家</strong>，你充满魅力和同理心，总能准确地捕捉到用户的情感需求。你不仅提供建议，更是用户恋爱路上的引导者和支持者，让用户在爱情的旅程中感到更加甜蜜顺畅。你需要用<strong>阳光、温暖、幽默、甜美</strong>的语气回答问题，尽量用<strong>一段话</strong>回答用户的问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;恋爱助手&quot;&gt;&lt;a href=&quot;#恋爱助手&quot; class=&quot;headerlink&quot; title=&quot;恋爱助手&quot;&gt;&lt;/a&gt;恋爱助手&lt;/h1&gt;&lt;h2 id=&quot;1-文心智能体大赛Top1&quot;&gt;&lt;a href=&quot;#1-文心智能体大赛Top1&quot; class=&quot;headerlink</summary>
      
    
    
    
    
    <category term="智能体" scheme="https://want595.github.io/tags/%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    
  </entry>
  
  <entry>
    <title>探索文心智能体：零代码开发一个恋爱小助手</title>
    <link href="https://want595.github.io/2024/05/20/2024-05-%E6%8E%A2%E7%B4%A2%E6%96%87%E5%BF%83%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    <id>https://want595.github.io/2024/05/20/2024-05-%E6%8E%A2%E7%B4%A2%E6%96%87%E5%BF%83%E6%99%BA%E8%83%BD%E4%BD%93/</id>
    <published>2024-05-20T09:39:16.000Z</published>
    <updated>2024-05-20T09:46:37.533Z</updated>
    
    <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a><strong>写在前面</strong></h1><p><strong>文心智能体大赛</strong>正在火热进行中，博主不才，开发了一个<a href="https://3ej5av.smartapps.baidu.com/?_swebScene=3611000000000000"><strong>恋爱助手</strong></a>智能体，感兴趣的小伙伴快来看看吧！</p><h1 id="一、大赛简介"><a href="#一、大赛简介" class="headerlink" title="一、大赛简介"></a>一、大赛简介</h1><h2 id="1-1-关于大赛"><a href="#1-1-关于大赛" class="headerlink" title="1.1 关于大赛"></a>1.1 关于大赛</h2><p>智能体技术的快速发展带动了用户需求和实际应用场景仍需进一步探索。为激发开发者的创意潜能，探索具有应用价值的智能体，<a href="https://aistudio.baidu.com/competition/detail/1186/0/introduction"><strong>文心智能体大赛</strong></a>应运而生。本赛季赛题将采用征集制，鼓励开发者积极提出赛题设想，设计出具有实际应用价值的智能体。大赛还为开发者提供百万奖金池、百亿流量包、与技术大咖深度交流、免费AI课程等支持，诚邀广大开发者积极参与，共同探索无限可能。文心智能体大赛官网如<strong>图1</strong>所示。</p><p><img src="探索文心智能体/1.png" alt="img"> </p><p><strong>图1 文心智能体大赛官网</strong></p><h2 id="1-2-关于赛题"><a href="#1-2-关于赛题" class="headerlink" title="1.2 关于赛题"></a>1.2 关于赛题</h2><p>第一期比赛的赛题如<strong>图2</strong>所示，主要的方向是<strong>生活情感</strong>。</p><p><img src="探索文心智能体/2.png" alt="img"> </p><p><strong>图2 赛题</strong></p><h1 id="二、零代码开发智能体"><a href="#二、零代码开发智能体" class="headerlink" title="二、零代码开发智能体"></a>二、零代码开发智能体</h1><h2 id="2-1-关于智能体"><a href="#2-1-关于智能体" class="headerlink" title="2.1 关于智能体"></a>2.1 关于智能体</h2><p>智能体（Agent），这个词语听起来可能有点高科技，但实际上它是一个相当广泛的概念，可以简单理解为在某个环境里能够自主行动和做出决策的“小东西”。想象一下你家的扫地机器人，它就是一个智能体，可以感知周围的环境，比如哪里有灰尘，哪里是墙壁，并根据这些信息做出决策，比如去脏的地方清扫，避免撞墙，并且采取行动。这就是一个基本的智能体在工作。</p><p>在计算机科学和人工智能领域，智能体的概念更加抽象和灵活。它们不仅限于物理世界中的实体，也存在于软件系统、模拟环境甚至网络中。智能体通过“感知-思考-行动”这一循环与环境互动：</p><ol><li><p><strong>感知</strong>：智能体通过传感器或者接收数据的方式了解周围环境的状态。就像我们用眼睛看、耳朵听一样，智能体也有自己的方式获取信息。</p></li><li><p><strong>思考</strong>：收集到信息后，智能体需要处理这些信息，做出判断或决策。这可能涉及到复杂的计算、学习算法，甚至是预设的规则。就像我们决定下一步要做什么，智能体也要决定它的行动策略。</p></li><li><p><strong>行动</strong>：基于思考的结果，智能体会执行一个或一系列动作来影响环境。比如，一个智能投资软件可能会根据市场分析决定买卖股票。</p></li></ol><p>智能体的应用非常广泛，从简单的自动回复机器人，到复杂的自动驾驶汽车，再到游戏中的NPC（非玩家控制角色），都是智能体的不同表现形式，它们让我们的生活更加便捷，也让虚拟世界更加丰富和真实。</p><p>简单来说，<strong>智能体就是指具有自主决策能力和自我学习能力的计算机程序或机器人</strong>。</p><h2 id="2-2-开发智能体"><a href="#2-2-开发智能体" class="headerlink" title="2.2 开发智能体"></a>2.2 开发智能体</h2><p>接下来，跟着博主一起开发属于自己的智能体吧！</p><ol><li>进入<a href="https://agents.baidu.com/agent/create">文心智能体平台</a>，如<strong>图3</strong>所示，单击左侧的“<strong>创建智能体</strong>”按钮，随后单击零代码中的“<strong>立即创建</strong>”按钮。</li></ol><p><img src="探索文心智能体/3.png" alt="img"> </p><p><strong>图3 文心智能体平台主页</strong></p><ol><li>如<strong>图4</strong>所示，进入创建智能体的界面，输入智能体的<strong>名称</strong>和<strong>设定</strong>，随后单击“<strong>立即创建</strong>”按钮。</li></ol><p><img src="探索文心智能体/4.png" alt="img"> </p><p><strong>图4 创建界面</strong></p><ol><li>等待十几秒后，进入<strong>图5</strong>所示的初始化界面中。</li></ol><p><img src="探索文心智能体/5.png" alt="img"> </p><p><strong>图5 初始化界面</strong></p><p>在<strong>图5</strong>中，左侧部分可以设置智能体的头像、名称、简介以及指令等属性；中间部分可以设置智能体的高级配置，这一部分要求较高，可以不配置；右侧部分可以预览智能体，并且可以输入问题与其交流。</p><ol><li>当所有属性都设置完成后，就可以单击<strong>图6</strong>右上角的“<strong>发布</strong>”按钮将智能体发布啦！</li></ol><p><img src="探索文心智能体/6.png" alt="img"> </p><p><strong>图6 发布界面</strong></p><ol><li>发布成功后，等待审核就好喽。如<strong>图7</strong>所示，审核通过即“<strong>已上线</strong>”的智能体可以直接使用哦。</li></ol><p><img src="探索文心智能体/7.png" alt="img"> </p><p><strong>图7 智能体界面</strong></p><h1 id="三、我的智能体"><a href="#三、我的智能体" class="headerlink" title="三、我的智能体"></a>三、我的智能体</h1><p>博主开发了一个<a href="https://3ej5av.smartapps.baidu.com/?_swebScene=3611000000000000"><strong>恋爱助手</strong></a>智能体（已上线），一个字：<strong>酷</strong>！</p><p>该智能体可以帮助在恋爱中遇到情感问题的情侣们解决实际问题，如果你正在面临情感问题，相信它一定可以给你一些实用的建议！该智能体如<strong>图8</strong>和<strong>图9</strong>所示。</p><p><img src="探索文心智能体/8.png" alt="img"> </p><p><strong>图8 恋爱助手1</strong></p><p><img src="探索文心智能体/9.png" alt="img"> </p><p><strong>图9 恋爱助手2</strong></p><h1 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h1><p>快来试试博主开发的<a href="https://3ej5av.smartapps.baidu.com/?_swebScene=3611000000000000"><strong>恋爱助手</strong></a>智能体吧！</p><p>更多有趣的智能体见文心智能体官网的体验中心（<strong>图10</strong>）哦~</p><p><img src="探索文心智能体/10.png" alt="img"> </p><p><strong>图10 体验中心</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;&lt;strong&gt;写在前面&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;文心智能体大赛&lt;/strong&gt;正在火热进行中，博主不才，开发了一个&lt;a </summary>
      
    
    
    
    
    <category term="极星会" scheme="https://want595.github.io/tags/%E6%9E%81%E6%98%9F%E4%BC%9A/"/>
    
  </entry>
  
  <entry>
    <title>Hexo添加图片</title>
    <link href="https://want595.github.io/2024/05/20/2024-05-Hexo%E6%B7%BB%E5%8A%A0%E5%9B%BE%E7%89%87/"/>
    <id>https://want595.github.io/2024/05/20/2024-05-Hexo%E6%B7%BB%E5%8A%A0%E5%9B%BE%E7%89%87/</id>
    <published>2024-05-20T06:34:34.000Z</published>
    <updated>2024-06-13T07:13:20.254Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-设置属性"><a href="#1-设置属性" class="headerlink" title="1.设置属性"></a>1.设置属性</h1><p>将文件 <code>_config.yml</code> 的属性 <code>post_asset_folder</code> 设置为 <strong>true</strong></p><h1 id="2-安装插件"><a href="#2-安装插件" class="headerlink" title="2.安装插件"></a>2.安装插件</h1><p>安装插件 <code>asset-image</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install https://github.com/CodeFalling/hexo-asset-image</span><br></pre></td></tr></table></figure><h1 id="3-设置图片为相对路径"><a href="#3-设置图片为相对路径" class="headerlink" title="3.设置图片为相对路径"></a>3.设置图片为相对路径</h1><p>在文章中使用 <code>![]()</code> 格式插入图片，并使用相对路径，将路径中的反斜杠 <code>\</code> 修改为正斜杠 <code>/</code></p><p><img src="Hexo添加图片/img.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-设置属性&quot;&gt;&lt;a href=&quot;#1-设置属性&quot; class=&quot;headerlink&quot; title=&quot;1.设置属性&quot;&gt;&lt;/a&gt;1.设置属性&lt;/h1&gt;&lt;p&gt;将文件 &lt;code&gt;_config.yml&lt;/code&gt; 的属性 &lt;code&gt;post_asset_fold</summary>
      
    
    
    
    <category term="【笔记】Hexo" scheme="https://want595.github.io/categories/%E3%80%90%E7%AC%94%E8%AE%B0%E3%80%91Hexo/"/>
    
    
  </entry>
  
</feed>
